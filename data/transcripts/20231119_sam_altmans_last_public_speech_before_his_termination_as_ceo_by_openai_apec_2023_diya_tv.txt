Unknown: rapidly in a short amount of time. It's an astonishing time to be alive and it feels different from other disruptive technologies. This year alone, discussions have ranged from possibilities of apocalyptic doom to the exhilarating promise of unprecedented advancements. However, many questions remain around how we can best utilize AI as humans for the good of humanity and to craft a future that's equitable, responsible, and aligned with our core values. To discuss this critical topic, as the mayor said, we're fortunate to be joined today by three of the world's leading thinkers and developers of AI, Chris Cox, Chief Product Officer of Meta James Manica, Google's SVP of Research, Technology and Society and Sam Altman, CEO of OpenAI. Let's give them a warm welcome. Researchers are using AI to create new proteins and discover new drugs. AI tutors may transform the way that children are educated. Their potential financial workforce Chris, James, and Sam, why are you devoting your life to this work?
Chris Cox: I mean it's funny, I started studying AI back in 2001, back when AI was a lot more arcane of science than I think it is today. Yeah. it felt to me like our ability to understand learning would help understand ourselves, would help us understand how we learn in our own consciousness. And part of what's been so interesting about today is that the technology that's allowing AI to start to be really good, is modeled after the way our minds work. I think part of why this is such an exciting period for AI is we're starting to see that the technologies we're building are starting to become a little bit closer to the way we learn, which is through exposure to one another. the promise of making the technology really humane and really modeled after the way that we interact with the world and our own judgments about what's right and wrong and our own judgments about what feels good and what doesn't feel good. sort of the excitement, but also at the same time the importance of discipline and seriousness about making sure that the way we ushered into the world is responsible. Yeah, of course.
Unknown: for you, is this because it's been a twenty year period for you focused on this, is this to you your life's work?
Chris Cox: I mean I started at Facebook back in 2005. I was one of our, I think thirteenth engineers or fourteenth engineers and most of my work there has been in trying to design software that gives people the content they care about, connects them to their friends and family. And if you go back at least to our company history, I think similar to Google, the fundamental innovation was really about getting good at recommending content for each person, personalizing content, understanding vast amounts of data, helping each person get uniquely the stuff they care about. that was AI that was sort of behind the scenes that was really important and part of what's starting to happen now people are becoming or I think having contact with it, And I think that embodiment is part of what sort of like taking this tech that's been around for a little while and suddenly giving it a mode of interacting with each other.
Unknown: And James, what about you? And you actually came you were studying AI and went to McKinsey
James Manyika: I'm looking forward to the conversation. For me, Lorene, the the very first thing I ever published in my whole life was in 1992. I was an undergraduate. It was a paper on training and modeling neural networks. My advisers actually advised me not to put the word AI in my dissertation because no one would take me seriously. So we called it something else. But when I look back from that time to where we are, the progress has been extraordinary. It's been extraordinary. You know, in the intervening time, I was at McKinsey looking at these big problems in society, climate change and so forth. So part of what was realizing that AI actually has a possibility of helping us to tackle all of these things. So what I get very excited now when I think about the work we're doing at Google, for example, you know, there are several areas that kind of motivate me, excite us. The possibility of actually helping people in very assistive ways do some of the most imaginative creative endeavors, learn languages, speak languages, get past access barriers, linguistic difficulty, AI will help with that. Yeah. The possibility that in fact we could actually transform economies, AI will help with that. Then I think about science, the possibility of having these extraordinary breakthrough innovations to advance science. You mentioned proteins. I think it's quite stunning that, you know, my colleagues at DeepMind, AlphaFold, was able to predict the protein structure of all 200,000,000 proteins known to science Think about access to maternal health in low income countries and communities. You know, the mayor can talk a lot about what we see in California. So all of these things, AI gives us the possibility of actually addressing and enhancing how we tackle all of this. This is what motivates me and excites me.
Unknown: you've been working on this for decades. And so but now we find ourselves in a particular moment of inflection. So I wonder if you can help ground the audience in understanding the AI landscape. So how does each of you think of where we are with the technology
James Manyika: And in fact, many people today, you know, even before, you know, Sam's extraordinary moment last year, were already using AI. that that, that has brought us here. In 2017, my colleagues at Google Research published a paper called Attention's All You Need. and a lot of things then rapidly accelerated, from that moment. speech suddenly be able to do very general things and things just accelerated. So I think that was a pivotal moment that has brought us to where we are. They're very general, everything from writing poetry, composing music and all these things. They've also become what's now termed multimodal. So it's not just language, but also images and video and all these things and coding. It's a very exciting time. We're starting to see them do very well on benchmark tests on how well do they do on kind of a range of cognitive tasks and capabilities. There's something called BigBench, which has something like two zero four kind of metrics you can evaluate. So they're starting to be very, very, very good. But I think it's worth pointing out that they still have some serious limitations, actually. to have a deeper understanding for where we are now about what these systems are good at, what they're not good at, how we solve and augment those capabilities, link them to other systems. But I'm actually pretty excited because the what I now call the scaling laws, and I'm sure Sam will also get into this, is as you scale these systems, they seem to get more capable, more powerful,
Unknown: I agree, they're very exciting. They also can be very concerning and for some people really frightening. So I want to read to you, Sam, a quote from the wise public intellectual Yuval Harari who said, AI is the first tool in history that can create new ideas by itself. There's a danger that we will spend all our effort on developing an AI at the time we don't understand ourselves, which is right now, and then letting AI take over and that would lead to human AI So we obviously don't want that. So that brings up this question of proper regulation and proper guardrails. And I think having the industry come together as it has and take some steps forward around how do we think about this collectively. So the industry has taken a very healthy step forward by launching the Frontier Model Forum. And just in the last two weeks, we've had a lot of regulatory bodies come forward. We had the White House executive order. We had the Bletchley declaration. We have the advisory body on AI that the UN has convened. So I'd like each of you to talk a little bit about how you think about some of the existential threats like Yuval has articulated and others as well as the state of regulation. What's proper? What's too much? How do we get it right now and then be open to evolving as the technology evolves? what about the executive order? How close does that get to getting it right?
James Manyika: that seem to come up that then drive the need for thinking through regulation. I think it's worth teasing that out a bit. So for example, what you typically have as concerns are in a few categories. Even if the systems work very well, how do we wanna think about what uses are appropriate, what uses are not appropriate? Mhmm. that's all about kinda use and misuse. We should think through that or what the rules should be for that. Then you've got all these other concerns about, when this technology works its way through society, implications for labor markets, for things like intellectual property, copyright and so forth. Then of course you've got the kinds of safety questions that Sam was talking about. As we get more capable systems, how do we wanna think about the approach to safety and those So I think it's worth because what happens in these regulatory conversations, often people are coming from any one of these Yes, different If you look at some of the, what were the White House commitments that came out of many of our companies, they're mostly again oriented around these safety questions.
Unknown: the advisory council or the advisory board of the UN that you're co chairing, are you breaking down these four different categories and trying to articulate appropriate regulations around each of these?
James Manyika: Well, we're thinking through, first of all, this is the UN high level advisory body. So we've kind of organized ourselves to think about three kinds of areas. One is to think about the opportunities and enablers of the opportunities. Keep in mind, as we all have said, there's so much that's exciting that could benefit the world. So we're thinking about those opportunities and enablers. Then we're also, second, thinking about these complexities and risks of all these different kinds that I just described. What's the right way to think about coordination, governance and so forth in ways that benefit the whole world? Because one of the risks that we have is we're likely could end up with a patchwork of
Chris Cox: silver lining of all of the attention on AI is there's a lot of attention on AI, and that means that you have the scrutiny of not just folks outside of the company, but folks inside of the company. I think one of the things a lot of people don't understand about our companies is like you have a whole lot of very serious people And what does bias mean? What does an unbiased dataset look like? one thing that I think a lot of outsiders to tech don't see that I'm inspired by is the amount of people inside of the companies who are incredibly serious and dedicated to each of the sort of nuanced is helping to do that we need as an industry is not sort of like 32 different versions And I agree with everything Sam said that my general perspective and my company's perspective and my team's perspective is the current models are pretty good if you look at the vast majority of the good use cases. And like we have not really been And I think that's something that the industry generally shares as a perspective right now that is very frequently lost in translation
Unknown: so returning to the open source question, do you at Meta balance the benefits of open science and open research and open data sets or algorithms with the fact that bad actors could actually cause a lot of trouble and harm a lot of people? And who decides what's open and what's not open? And how is that decision made? Yes, sure. So just as history,
Chris Cox: Meta built, designed and deployed LAMA, which was the first large language model that we open sourced back in February as well as LAMA two, which was the most recent open source large language model we open sourced in June. Our company was built on open source technologies. And I think it's worth remembering when we look back on the early days of the Internet, It was technologies that allowed entrepreneurs to not have to go pay huge licensing fees in order to get access to tools that allowed them to build companies that So became amazing to open source and the individuals who, whether for technologies or whether for services like Wikipedia, sort of volunteered to contribute to technologies for the benefit of technologists and for the benefit of ultimately science and medicine and education and all these other things. Equitable information. Now how did we make the decision to open source LAMA and LAMA First from scientists, from chemists, from folks who were at serious institutions working on the hardest problems, from folks researching cancer, were seeing that there was an enormous appetite to have access to a model that was close to the frontier, that was safe, fine tuning and adjusting system cards, red teaming, which is where you get various categories of sophisticated people to pretend they are bad actors in order to test the veracity of the system. We also spent a long time talking to We spent a bunch of time with government, with the White House, with elected officials in other countries to sort of just like bounce off them the idea that we were contemplating this. written by the AI safety researchers the model, you can't just say, Trust us. Hard question. I mean every week I hear an incredible story of LAMA two being used to this week it was like Stanford students writing creating some glasses that allowed a blind person to understand what was being said in front of them.
Unknown: dollar company, it was a group of students. Yes. No, these tools, when put in the hands in a democratic way, will create marvelous things. Yeah. It's true. Sam, how do you respond to
James Manyika: while in addition to encouraging innovation, entrepreneurship, a chance to understand what we're doing. Yeah. Which Sam implied, but he can jump in, is as these models become more capable two, three generations from now, how do you want to think through that question? Not with the concern about innovators and entrepreneurs and researchers,
Unknown: what I find a lot of a lot of comfort in is the fact that that you all are actually having these conversations and you know each other and there's there's open discourse, there's some disagreement, but but it's healthy. It's healthy to have these conversations. What's not healthy is not talking about it and going off and doing our own and then endangering the rest of us. But and in the next several generations of products, I'm sure that the conversations will continue to be interrogated. In 2024, which is this coming year, however, we have right in front of us not only the U. S. Election, but elections in 40 other countries around the world. And we've already seen disinformation over the last eight years, and AI has the potential to supercharge disinformation. So how are you all thinking about that? about assuring that if AI is used in political advertising, it's disclosed like Meta is positing. But also how do we make sure that we can understand if there are deep fake videos, convincing advertising, how do we navigate through this in this coming year?
Chris Cox: thing that we've learned over the past ten years, I would say, of focusing quite seriously on our role in elections is, And what that means for us is so much of your ability to scale. to operate at scale because that allows you to take for us, for example, if we have something that's reported, starts going viral, fact checkers can look at it, they can decide if it's misleading, they can label it. And then AI can help us quickly detect everything that looks like it. And those sorts of systems, whether or not an image was generated by AI, whether it was generated with Photoshop or whether it was an actual image or whether it was a piece of text, agnostic to how the piece of content was was created, a lot of the systems that we built over the years can be used and be deployed against this sort of behavior.
Unknown: But also I remember, and this may be a sore spot, but I remember there was a deepfake with Nancy Pelosi and Facebook didn't take it down. So I'm not blaming you, but that happened and I know that that was a source of a lot of tension. So how do we as consumers trust what we're seeing? And how do public personas trust that their likenesses are not going to be manipulated?
Chris Cox: Yes. So I think the first thing is, Sam and I were just talking about this, there is a fair amount of public awareness on people should be skeptical about deepfakes. Like what's interesting about this chapter of where we are with the Internet is there already is a broadly held understanding that like deepfakes are something you should keep an eye out for. I think that's good. The second thing is we've gotten a lot more sophisticated at understanding and detecting misinformation. The way we do that is working with 90 different fact checkers, certified fact checking institutions across 60 languages. In order to understand that for content that goes viral, let's have somebody let's have the sort of professionals take a look at it and then make sure we label that content. So those are the kinds of things that we've learned over the years make a huge difference
James Manyika: it's worth remembering that in fact these tools and technologies can actually be part of the solution. I'm sure it's true for you Chris, but in our case, like YouTube and other platforms, the volume that content is being uploaded, it far outstrips any human's ability to review every single thing. So in many cases, AI is actually assisting in that task to be able to do it at scale. We also find that even in our AI systems when we're doing adversarial testing, tasks we all have and we're starting to make progress on it is how do we develop new techniques to improve our confidence in information. So in our case at Google, we've been spending a lot of time investing in research on watermarking know, earlier this year we actually, for example, introduced something called Synth ID, which is a watermarking technique we're building into all our generative systems. It's still early days,
Unknown: suppose the problem is that often the damage is done and then we notice and then we correct. And I also understand about broad antibodies at the societal level because we've now been swimming in a sea of propaganda and misinformation. However, we still have a lot of people in this country and elsewhere who believe in conspiracy theories that are easily debunked, but nevertheless, they believe in them. And that has to do with human nature and the way that the brain latches on to information. And that's something that and a lot of the three of you and others speak often with elected officials and political leaders, and these are all U. S.-based companies. And so how are you thinking about obviously wanting to be inclusive and have equitable distribution to benefits across the world, but also balancing that with U. S. National security. And I'm sure that comes up in a lot of private conversations, but what can you share with this audience about that?
James Manyika: in mind, especially in a setting like this at the Asia Pacific kind of summit, is that a lot of these questions are really about us, not about the technology, about us as societies. Because a lot of these questions about whether it's bias or and all these questions often are very different from place to place, country to country. We found, for example, as we've rolled out these technologies, having to have very different conversations in different countries about what counts as bias, what's not in different communities. But these are questions about society Yeah. Of these questions. And as Sam said, society has faced these things for centuries, but maybe this technology amplifies us. I think we all have to think about these normative difference questions in all our different countries. I think that's important to keep in mind. But at least hopefully, there's a baseline we all agree with, which is hopefully we all adhere to things like basic human rights, human freedoms, those kinds of things, safety, violence and so forth. I think we can agree on, but that's just the floor, not the ceiling. The rest I think is up to us collectively. I think in conversations to your question with various governments, I think there's clearly a serious concern around safety, around misinformation. And I think we all have the responsibility to do more technical work on that front to safeguard these systems and do as much as we can. One of the things many of us have encouraged Because we're all experiencing,
Chris Cox: it's funny. I would say the biggest difference between operating in The U. S. And operating in a country like Brazil or India or Indonesia is WhatsApp. And I think a lot of Americans still don't understand the primacy of WhatsApp as an experience that matters a lot for folks in huge swaths of the world. It's the world's largest messaging platform. We spend a lot of time with governments and also with NGOs in each country where we're big during important events. So during COVID, we spent a bunch of time in Brazil and India and Indonesia and Mexico figuring out how we could play a role in helping them get up to date accurate information from the public health ministry. Turns out the answer on WhatsApp was like, just put all over the country, like text this number on WhatsApp and we'll send you up to date information on social distancing, on vaccines, on masks, on whatever. We do similar things during elections basically access to some institution they trust in the tool that they're using to get a lot of their information. So we do that. We've learned that it's important not just for the people in those countries, but also for like the ongoing relationship of trust. For us to be brokers of those services, like we do really need to do a good job of showing up during a public health emergency, during a natural disaster, during an election, during a war or a crisis. Put a lot of time and energy into how are we helping people get accurate information. And I think that's something I know Google does quite seriously as well. And it's something I think we, as American companies operating global services, have tried to really improve upon every year.
James Manyika: where the compositions are somewhat different actually. Often there's a lot of excitement about the possibilities to transform, leapfrog, and get access to the world's knowledge and information. how do we make sure that they are included and participate and have the tools and the capabilities to capitalize on these technologies and the possibilities for their own economies. We get, for example, we're spending an enormous amount of time on things like these language moonshots. So for example, Google Translate works with roughly right now 133 languages, because that opens up inclusivity in ways that are extraordinary to people in many, many different languages in Latin America and Africa and other places that I think are going to open. So I think part of the exciting opportunity is to how do we enable economies, entrepreneurs everywhere, quite frankly, to take advantage of this and pursue their own ambitions and do amazing things for their countries and for their communities.
Unknown: Well, you teed us up for the perfect last question, James. So let's assume that the four of us are sitting on the stage here again in 2024 because the mayor did such a brilliant job at hosting everyone at APAC. They're all coming back. So we'll come back too, and we'll have this conversation and I will ask each of you what is the most remarkable surprise that happened in AI in your field, in your company in 2024? And what is it that you'll be telling us about? And why is that a remarkable thing? Why is it brilliant?
James Manyika: three or four big societal breakthroughs we'll have done on this technology, ideally in science, ideally in something that really, really matters for society. I hope we can show you three or four such things. I mean proteins and biology are one thing, but hopefully there'll be more in education, more in healthcare.
Chris Cox: that is positive. That comes from somebody who previously would not have been able to build, somebody or some institution that would not have been able to accomplish something until
Unknown: yes. I think in healthcare alone and in Disease Quest alone,
James Manyika: figuring out and struck the right balance between making sure we have mechanisms to limit and address the harms and things we're concerned about, but at at the same time have some extraordinary kind of pro innovation things that enable the opportunities that we all want. I think we somehow