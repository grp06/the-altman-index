Sam Altman: But people under I I think people understand much worse than what we'll have this year to say nothing of what we'll have next year, People, lots of people have found ways to get value out of them and also to understand their limitations. So,
Unknown: You know, how the neural network operates, what weights it assigns to various things. Do you think that we will get there, or is it getting so inherently complicated
Sam Altman: that's kind of okay. I think, you know, in some sense, the hardest part is when it's right 99.999% I actually can't look in your brain and look at the 100,000,000,000,000 synapses, and try to understand what's happening in each one and say, okay, I really understand
Unknown: what do you think is left for a human being to do if the AI can outanalyze a human being, can outcalculate a human being? A lot of people then say, well, that will you know, that means what we will be left with, our core innate humanness, will be our emotional intelligence, our empathy, our ability to care for others. But do you think AI could do that better than us as well? And if so, what what's the core competence of human beings?
Sam Altman: really care about what other humans think. That seems very deeply wired into us. So chess but we're still, like, very focused on each other, and I think we will do things with better tools. And I admit, it does feel different this time. General purpose cognition feels so close to what we all treasure about humanity that it does feel different. So, of course, you know, there'll be kind of the human roles where you want another human, but even without that, I think Like, when I think about my job, I'm certainly not a great AI researcher. My my my role is to, like, you know, figure out what we're gonna do, think about that, and then, like, work with other people to coordinate and make it happen. And I think everyone's job will look a little bit more like that. We will all operate at a little bit higher of a level of abstraction. We will all have access to a lot more capability, and we'll still, like, make decisions. They may trend more towards curation over time, but we'll make decisions about what should happen in the world.
Unknown: minister from every country. It was amazing actually. I've never really seen anything like it. But everyone's there because we realize we are at this threshold moment, but we're not totally there yet. and other products and going, wow, we're having this incredible experience with an AI. We really have not quite had this kind of interactivity before, but we don't trust it quite yet. pretty bad. We don't want that in our AI industry. We want to have a good healthy partnership with these moderators and and with these regulators. I think that that begins the power of kind of where we're going. And when I talk to our customers about what they want, And today the AI is really not at a point where we're replacing human beings. It's really at a point where we're augmenting them. So I would probably not be surprised
Unknown: improve productivity in exactly, you know, the question Mark was posing? So you run a vast organization. I mean, you just told me you employed 330,000 people in India alone.
Unknown: The tech revolution is transforming right now what we do. What is our job is to make breakthroughs that change patients' lives. With AI, I can do it faster and I can do it better. And this is not only because of the advancements in biology as we spoke, but also the advancements in technology and the collision between the two of them. But they are creating tremendous synergistic effects that will allow us to do things that we were not able to do until now. I truly believe that we are about to enter a scientific renaissance in life sciences because of this coexistence of advancements in technology and biology. Generative AI is something that we were all impressed, but we saw it now, let's say, basically last year. Right? But AI in different forms exists for many, many years and we are using it very, very intensively in our labs. The best example that I think people will resonate, it is the oral pill for COVID, it's called Paxlovid, was developed in the chemist part of it. It was developed in four months. Usually, it takes four years. This is because the typical process is what we call drug discovery. You really synthesize millions of molecules and and then you try to discover within them which one works. With AI now we are moving to drug design instead of drug discovery. So instead of making 3,000,000 molecules, we make 600. And we made by using tremendous computational power and algorithms that help us to design the most likely molecules to be successful. And then we look to find the best among them. For years to four months,
Unknown: The issue that Sam raised about trust, that that Mark Benioff raised about trust, does seem central. How do you get people to trust AI? Should they trust AI? And should government regulate AI so that it is trustworthy?
Unknown: because this is at such an emerging stage. You can, often not justified. But I think, look, we can all do tremendously well out of AI. The UK is already doing well. London is the second largest hub for AI after San Francisco and The UK has just become the world's third largest tech economy, trillion dollar tech economy after The United States and China. But
Unknown: regulation is likely to be most people are gonna be most worried about the issue of trust, which is the combination of AI and medical. And is this, you know, is it safe for me to listen to this doctor, to take this drug? This has all been developed in a computer somewhere.
Unknown: Is there a way to alleviate that? I think there is. First of all, we need to understand that AI is a very powerful tool. So in the hands of bad people, it can do bad things for the world. But in the hands of good people, it can do great things for the world. And I'm certain right now that the benefits clearly outweighs the risks. But I think we need regulations. Right now, there is a lot of debate how those regulations will set guardrails. And there are some countries that they are more focused on how to protect against the bad players. There are some countries that they are more focused on how to enable the scientists to do all the great things with this tool that we want the world to have as in the next pandemic. the right balance that will protect, at the same time,
Unknown: But there are many people who fear this much larger issue of the technology ruling over us. Right? You've always taken a benign view of AI or relatively benign view, but people like Elon Musk and sometimes Bill Gates and other very
Sam Altman: we've made massive progress there. Now, there's a harder question than the technical one, which is who gets to decide what those values are And what the defaults are, what the bounds are, how does it work in this country versus that country, what am I allowed to do with it versus not? I think it's good that people are afraid of the downsides of this technology. I think it's good that we're talking about it. I think it's good that we and others are being held to a high standard. And, technology has been made to be safe and also how the different stakeholders in society have handled their negotiations about what safe means and what safe enough is. But I I have a lot of empathy for the the general and discomfort of the world towards companies like us and, you know, our our the other people doing similar things, which is like, why is our future in their hands? figure out a way to get the input from society about how we're going to make these decisions, not only about, you know, what what the values of the system are, but what the safety thresholds are, and what kind of global coordination we need to ensure that stuff that happens in one country does not super negatively impact another, to to show that picture. So I think not having caution, not feeling the gravity of what the potential stakes are would be very bad. So, I like that people are nervous about it. We have our own nervousness, but we believe that we can manage through it. And the only way to do that is to put the technology in the hands of people, let society and the technology co evolve, and sort of step by step with a very tight feedback loop and course correction,
Unknown: And they got 300 call center operators down there and they're like using our service cloud and they want to use Einstein which is our, you know, AI platform. It'll do a trillion predictive and generative transactions this week. We're partnered with Sam and it's very exciting. It has a trust layer that lets our customers feel comfortable using their product. And I said to them, what do you really want? And I'm not sure what they want. Are they looking to replace people? Are they looking to And something incredible happened. Revenues are up 30%? Yeah. How did that happen? Well, these were all just service professionals, but they now have this generative AI and predictive as well. They've all been augmented. adding value to the customers and it's a miraculous thing. Their morale went way up. They can't believe what they've been able to achieve. They didn't even know what they didn't know about the products. It all was kind of being tutored and mentored and inspired to them by the AI. And that idea that Einstein could augment them and that then, yes, they got their revenue, they got their margin that they so badly wanted. Even the WAF, you know, this app that you're all using is running on Einstein. So those predictions that you're getting, hey, because you like this AI panel, you should try that AI panel and you may look over here. That is our Einstein platform giving you those ideas. This is really the power. And that's why I think these conversations and this governance and getting clear about what our core values are is so important. And, yes, our customers are gonna get more margin.
Sam Altman: link out, show brands of places like the New York Times or the Wall Street Journal or any other great publication But it's displaying that information when the user queries, not using it to train the model. Now, is these models will be able to take smaller amounts of higher quality data during their training process and think harder about it and learn more. we'd like to find a way for you to get paid for that. If you teach our models, if you help provide the human feedback,
Unknown: global AI standards, it's very important that they reflect liberal democratic values. But I think it is really important that we talk to countries like China because about this morning's discussion is Sam has a sign saying no one knows the future. But we do have agency over the future and that is the tension between the two. I think that we are incredibly lucky that people like Sam are helping to transform humanity's prospects for the future. I don't think anyone in this room thinks the world will be a better place if there wasn't AI. But we have choices now whilst being humble about not being able to predict the future, remember that we do have control over the laws, the regulations. We have the ability to shape this journey. And I think we should also look at history and say look at the industrial revolution, the computer revolution. Where those revolutions succeeded was where the benefits were spread evenly throughout society and not concentrated in small groups.
Unknown: I don't know what can happen with AI. As Sam said, nobody knows. And I don't know how China eventually will think about those things. What I know it is that in life sciences, China is making tremendous progress. Right now, I think there are even more biotechs in China than exist in The U. S. Or in The U. K. Or in Europe. I think the Chinese government is committed to develop basic science in life science. I think in a few years we will start seeing the first new molecular entities coming from China and not from The US.
Sam Altman: And so I think that as I think one lesson, is as we get we, the whole world, get closer to very powerful AI, I expect