Sam Altman: I think the prevailing wisdom back then was that AI was going to and then AI than people thought. You can see DALL three generate these amazing images or was not perfect and I certainly would not have predicted GPT-four nine years ago quite how it turned out, But a lot of the other parts about people still really want a human doctor, that's definitely very true. decade or few decades to improving more of what we want, are abundant and inexpensive intelligence. The more powerful, the more general, the smarter, the better. I think that is AGI and abundant and cheap energy. And if we can get these two things done in the world, then it's almost like the best tool humanity has yet created. kind of this unfolding human story. And, you know, it's new and anything new comes with change and change is not always all easy, but I think this will be just absolutely tremendous upside and going to know, we're going to
Mira Murati: And, you know, they're not quite right because they define our own intelligence, and we're building something slightly different. You can kind of see how the definition of intelligence evolves from machines that were really great at chess and off ago and now the GPT series and then what's next. They continue to evolve and it pushes how we define intelligence.
Sam Altman: But think there are experts in areas that are going to be better than AI systems for a long period of time. And so, like, you know, you could come to some area where I'm really an expert at some task and I'll be like, all right, GPT-four is doing a horrible job there, GPT-five, six, whatever, is doing a horrible job there. But you can come to other tasks where I'm okay but certainly not an expert. I'm kind of like maybe like an average of what different people in the world could do at something. And for that, what we mean by that is that in any given area, expert humans may like, experts in any area can just do extraordinary things and that may take us a while to be able to do with these systems. But for kind of the more average case performance, so, you know, me doing something that I'm, like, not very good at anyway, maybe our future versions can help me with that a lot.
Mira Murati: you know, before we had the product, we were sort of looking at academic benchmarks and how well these models were doing on academic benchmarks. And, you know, OpenAI is known for betting on scaling, you know, throwing a ton of compute and data on these neural networks and seeing how they get better and better at predicting the next token. But it's not that we really care about the prediction of the next token. We care about the tasks in the real world to which this correlates to. And so that's actually what we started seeing once we put out research in the real world, and we build out products through the API, eventually through ChatGPT as well. And so now we actually have real world examples. We can see how our customers do in specific domains, how it moves the needle for specific businesses, and of course, with GPT-four, we saw that it did really well in exams like SAT and LSAT and so on. So it kind of goes to our earlier point that we're continually evolving our definition of what it means for these models to be more capable. But what we really look for is reliability and safety. These are very interweaved, and it's very important to make systems that, of course, are increasingly capable, but that you can truly rely on. And they are robust and that you can trust the output of the system. So we're kind of pushing in And, you know, as we build the next model, the next set of technologies, we're both betting, continuing to bet on scaling, but we're also looking at, you know, this other element of multimodality because we want these models to kind of perceive the world in a similar way to how we do. We perceive the world not just in text, but images and sounds and so on. So we want to have robust representations we're not where we need to be. But we're sort of on the right track, and it's unknown. It's research. It could be that continuing in this path of reinforcement learning with human feedback, we can get all the way to really reliable outputs, and we're also adding other elements like retrieval and search so you have the ability to provide more factual answers or to get more factual outputs from the model. So there's a combination of technologies that we're putting together to kind of reduce the hallucination issue.
Sam Altman: a few thoughts in different directions here. One, we obviously data ownership and how economic flows work, but we want to get to something that everybody feels really excited about. But one of the challenges has been people, different kinds of data owners have very different pictures. So we're just experimenting with a lot of things. We're doing partnerships of different shapes, And we think, like with any new field, we'll find something that sort of just becomes a new standard. Also, I think as these models get smarter and more capable, word humanity has ever produced or whatever. what really will matter in the future is particularly valuable data. People trust the conversation about data and the shape of all of this, because of the technological progress we're making, it's shift. About thing that people really like about a GPT model is not fundamentally that it knows particular knowledge. There's better ways to find that. It's that it has this And then there's other places like we offer a version of the API. They offer a version of the API, but that's like a very work it out so that we all benefit and we're all happy and we jointly want as much usage of our models
Mira Murati: and maybe, you know, when you're having fun. And so that's why it's actually so important to get it right, and we have to be so careful about how we design this interaction so that ultimately it's, you know, elevating and it's fun and it makes productivity better and it enhances creativity. And this is ultimately where we're trying to go, and as we increase the capabilities of the technology, we also want to make sure that on the product side, we feel in control of these these systems in the sense that we can steer them to do the things that we want them to do, and the output is reliable. That's very important. And, of course, we want it to be personalized. Right? And as as it has more information about your preferences, the things you like, the things you do, and the capabilities of the models increase and other features like memory and so on, it has of course, it will become more personalized.
Sam Altman: and we've got some opinions. But a, we really think that the decisions belong to humanity, that any new technological I personally have deep misgivings about this vision of the future where everyone is super close to AI friends and not more so than human friends or whatever. I personally don't want that. I personally think that personalization is great, personality is great, but it's important that it's not like personness and at least that you know when you're talking to AI and when you're not. We named it ChatGPT and not there's a long story behind that, but we named it ChatGPT and not a person's name very intentionally. And we do a bunch of subtle things in the way you use it to make it clear that you're not talking to a person. And I think what's going to happen is that in the same way that people have a lot of relationships with people, they're going keep doing that, and then there'll also be these, like, AIs in the world, but you kind of know they're just a different thing. bad about what we can do with this new technology in terms of a new computing platform. And I do think every sufficiently big new technology are great. Like, I have no interest in trying to go compete with a smartphone. I think the way What AI enables is so fundamentally new maybe just like for a bunch of reasons, it doesn't happen. But I think it's well worth the effort of talking about or thinking about what can we make now that before we had computers that could think or at the scale that we think the world will demand and at the model scale that we think the research can support. we have wonderful partnerships right now with people who are doing amazing work. So the default path would certainly be not to, but I would Nvidia certainly has something amazing, amazing. But, you know, I think, like, There has got to be something more interesting to talk about in our limited time here together than it shouldn't come as a surprise because we have said this all the way through. Like, it's just a tremendously expensive endeavor.
Mira Murati: small teams that innovate quickly. The product side, you know, we're doing a lot of things. We're trying to push great uses of AI out there both on platform side and first party and work with customers.
Sam Altman: of the GPTs but new techniques that we're interested in that could help generate new knowledge and someone with access to a system like this can say, like, help me hack into this computer system or help me design a new biological pathogen that's much worse than COVID or any number of other things, it seems to us like it doesn't take much imagination to think about scenarios that deserve great caution. And again, we all come and do this because we're so excited about the tremendous upside and the incredibly positive impact, I think it would be like a moral failing not to go pursue that for humanity. But we've got to address and this happens with many other technologies we've got to address the downsides that come along with this. And it doesn't mean you don't do it. It doesn't mean you just say, like, this AI thing, we're going to go full Dune and not have computers or whatever. But it means that you are thoughtful about the risks, you try to measure what the capabilities are, and you try to build your own technology in a way that
Mira Murati: I mean, like Sim said, you've got the capabilities, and then there is always a downside. Whenever you have such immense and great capability, there's always a downside. So we've got a fierce task ahead of us to figure out what are these downsides, discover, understand them, build the tools to mitigate them, and it's not, you know, like, a single fix. You usually have to intervene everywhere from the data to the model to the tools in the product and, of course, policy and then thinking about the entire regulatory and societal infrastructure that can kind of keep up with these technologies that we're building. Because, ultimately, what we want is to slowly roll out these capabilities in a way that makes sense and allow society to adapt because, you know, the the progress is incredibly rapid, and we want to allow for adaptation and for the whole infrastructure that's needed for these technologies to actually be absorbed productively, to exist and be there. So when you think about what are sort of the concrete safety measures along the way, I'd say number one is actually rolling out the technology and slowly making contact with reality, understanding how it affects certain use cases and industries and actually dealing with the implications of that, whether it's regulatory copyrights, you know, whatever the impact is, actually absorbing that and dealing with that and moving on to more and more capabilities. I don't think that building the technology in a lab in a vacuum without contact with the real world and with the friction that you see with reality is a good way to actually deploy it safely.
Sam Altman: the point Mira was making, I think, is really important, it's very difficult to make a technology safe in the lab. Society decide, Hey, here's a thing that is not an acceptable risk tolerance and this other thing that people are worried about, that's totally okay. And, you know, like, we see this with many other technologies. Airplanes have gotten unbelievably safe, even though they didn't start that way, and it was like careful, thoughtful engineering and understanding why when something went wrong, it went wrong and how to address it and the shared best practices there. I think we're going to see in all sorts of ways that the things that we worry about with AI and theory don't quite play out in practice. There's a ton of talk right now about deepfakes and the I think that's an example of where we were thinking about the last generation too much. And AI will disrupt society in all of these ways, but, you know, we all kind of are like, more capable than any humans, I think it's very reasonable to say we need to treat that with caution and a coordinated approach. But we think what's happening with open source is great. We think startups need to be able to train their own models and deploy them into the world. And a regulatory response on that would be a disastrous mistake for this country or others. let's look forward at where
Mira Murati: We're definitely responsible for the technologies that we develop and put out there and, you know, misinformation. And that's that's clearly a big issue as we create more and more capable models, and we've been developing technologies to deal with the provenance of an image or text and detect output. you want to give the user sort of flexibility and you also don't want them to feel monitored, and so you have to consider the user and you also have to consider people that are impacted by the system that are not users. And so these are quite nuanced issues that require a lot of interaction and input from not just your users of the product, but also society more broadly. And figuring out, you know, also with partners really good, almost, you know, 99% reliable, but we're still testing it. It's early, and we want to be sure that it's going to work. And even then, it's not just a technology problem. Misinformation is such a nuanced and broad problem, so you still have to be careful about how you roll it out, where you integrate it. But we're certainly working on the research side, and for for image, least, we have a very reliable tool in in the early stages. For images, it's a bit more straightforward problem. But in either case, we'll definitely test it out because we don't have all the answers. Right? Like, we're building these technologies first.
Sam Altman: the
Mira Murati: right, and what's what's real and what's not real. Actually, I think in the world that we're going towards, marching towards, the the bigger risk is really this individualized persuasion and and how to deal that, and that's going to be a very tricky problem to deal with.
Sam Altman: So I'm super excited to see it all play out. I think things can get so much better for people than Again, we're clearly dealing with something very powerful that's going to impact all of us in ways we can't perfectly foresee yet. But what a time to be alive and get to witness this.
Mira Murati: And the fears we talked a lot about the fears, but, you know, we've got this opportunity right now, and we've got summers and winters in AI and so on. But, you know, when we look back ten years from now, I hope that we get this right. And I think there are many ways to to mess it up, And, you know, I I think it's certainly going to lead to a lot of disruption in the workforce, and we don't know exactly the scale of that or or the trajectory along the way, but that's that's for sure. And one of the things that I, in retrospect, it's not that we specifically planned it, but in retrospect, I'm happy about is that with the release of Charge Beauty, we sort of brought AI into the, you know, collective consciousness, and people are kind of paying attention because they're not reading about it in the press. People are not just telling them about it, but they can play with it, they can interact with it, and get a sense for the capabilities. And so I think it's actually really important to bring these technologies into the world and make them as widely accessible as possible. Know, Sam mentioned earlier, like, we're working really hard to make these models cheaper and faster so they're accessible very broadly, but I think that's key for people themselves to actually interact with the technology and experience it and sort of visualize
Sam Altman: Every technological revolution affects the job market. and we'll find new and better jobs. The thing that I think we do need to confront as a society is the speed at which this is going to happen. It seems like over two, maximum three, probably two generations, we can adapt, society can adapt to almost any amount of job market change. But not cool. Like, that's not an easy message to get across. And although I tremendously great satisfaction in expressing yourselves in being useful and sort of contributing back to society. That's not going away. That is such an innate human desire. Like, evolution doesn't work that fast. Also, the sort of ability to creatively express yourself and to sort of leave something, to add something back to the trajectory of the species is That's like a wonderful part of the human experience. So we're going to keep finding things to do. And the people in the future will probably think some of the things will think some of the things those people do are very silly and not real work in a way that, like, a hunter gatherer probably wouldn't think this is real work either. You know, we're just trying to, like, entertain ourselves with some silly status game. That's fine with me. That's how it goes. It is not enough to just give people a universal basic income. People need to have agency, the ability to influence this. We need to sort of jointly be architects of the future.