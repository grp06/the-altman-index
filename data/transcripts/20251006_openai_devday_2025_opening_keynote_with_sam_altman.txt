Sam Altman: Good morning and welcome to Dev Day. Thanks for being here in San Francisco, the city where we started and where we are committed to building the future of AI. Back in 2023, we had 2,000,000 weekly developers and a 100,000,000 weekly ChatGPT users. 4,000,000 developers have built with OpenAI. More than 800 people use ChatGPT every week, and we process over 6,000,000,000 per minute on the API. Thanks to all of you. AI has gone from something people build play with to something people build with every day. Before we get started with all of today's announcements, we wanna do something fun. On the screen behind me are the names of the developers in the room who have built apps on our platforms that have crossed some big milestones. 10,000,000,000 tokens processed, a 100,000,000,000, even 1,000,000,000,000. Let's give them a round of applause. While it's exciting to celebrate how far you all have come, we are still so early on this journey. So today, we're gonna focus on what matters most to you all, is making it easier to build with AI. We've been listening to developers, hearing where you get stuck, what you want us to build next so that you can build more things. We've got four things for you today. We're gonna show you how we're making it possible to build apps inside of ChatGPT and how we can help you get a lot of distribution. We're gonna show you how building agents is going to be much faster and better. and underneath all of this, we'll give you updates to models and APIs to support everything you want to build. We want ChatGPT to be a great way for people to make progress, to be more productive, more inventive, to learn faster, to do whatever they're trying to do in their lives better. We have been continuously amazed by the creative ways that people use it. Since our first dev day, we've been working to try to figure out how to open up ChatGPT to developers and we've tried things like GPTs, we've adopted standards like MCPs, but we've learned a lot along the way and today we're gonna open up ChatGPT for developers to build real apps inside of ChatGPT. This will enable a new generation of apps that are interactive, adaptive, and personalized that you can chat with. So to build them, today we're launching the apps SDK which will be available in preview starting today. With the apps SDK, you get the full stack. You can connect your data, trigger actions, render a fully interactive UI and more. The apps SDK is built on MCP. You get full control over your back end logic and front end UI. We've published the standard so that anyone can integrate the apps SDK. When you build with the apps SDK, your apps can reach hundreds of millions of ChatGPT users. When someone's using ChatGPT, you'll be able to find an app by asking for it by name. For example, you could sketch out a product flow for chat gbt and then say Figma, turn the sketch into a workable diagram. The Figma app will take over, respond and complete the action. You can also then launch FigJam from ChatGPT if you want to iterate further. So when a user asks for something, we can surface a relevant app as a recommendation. Maybe a user says they need a playlist for their party this weekend. Chatty Boutique would then recommend building it in Spotify. They'll be available in ChatGPT today and this is just the beginning. We're gonna roll out more apps from partners in the weeks ahead. For developers, the apps SDK is available in preview to start building with today. Our goal is to get this in your hands early, hear your feedback, build it together with you. And then later this year, developers will be able to submit apps for review and publication. We'll also release a directory that users can browse in addition to discovery and conversation. Any apps that meet the standards provided in our developer guidelines will be eligible to be listed. Apps that meet higher standards for design and functionality will get featured more prominently, including in the directory and suggested as apps and conversations. We've published a draft of the developer guidelines along with the apps SDK so you'll know what to expect and we'll share more in monetization from apps soon. We'd also love your feedback about what you'd want. This should be an exciting new chapter for developers and for ChatGPT users. So that was apps and we hope everyone loves it. Thank you. Software that can take on tasks with context, tools and trust but for all the excitement around agents and all the potential, very few are actually making it into production and into major use. So, we've talked to thousands of teams, many of them in this room who are building agents to reimagine how work gets done and we've asked what we can do to make agents much easier to build. So today we're gonna launch something to help with that. The goal here is something for every builder that wants to go from idea to agent faster and easier. So we're excited to introduce a new thing called agent kit. Agent kit is a complete set of building blocks available in the OpenAI platform designed to help you take agents from prototype to production. It is everything you need to build, deploy, and optimize agentic workflows with way less friction. Our hope is that everyone from individual developers to large enterprises So the first one is agent builder. This is a canvas to build agents. It's a fast visual way to design the logic steps, test the flows, and ship ideas. We've heard this one loud and clear and we're making it easy to bring great chat experiences right into your own apps. You can see in the video here how chat can work across each agent node and call on tools to form the best response. And then finally, evals for agents. We're shipping new features dedicated to measuring the performance of agents. You get trace grading to help you understand agent decisions step by step. You get datasets so you can assess individual agent nodes. You get automated prompt optimization and you can even now run evals on external models directly from the OpenAI platform. This is all the stuff that we wished we had when we were trying to build our first agents. And of course, agents need access to data. So with OpenAI's connector registry, you can securely connect agents to your internal tools and third party systems through an admin control panel while keeping everything safe and under your control. So let's look at a couple of examples. Albertsons runs over 2,000 grocery stores across The US. More than 37,000,000 people shop there each week and each store is like its own little economy. Managers have to make all these constant decisions, tweaking this promotion or that product mix, resetting the displays, working with a bunch of vendors. It's like a lot of stuff. So, Albertsons built an agent using Agent Kit. So, now imagine a situation where sales are unexpectedly down for ice cream, down 32%. Now an associate can just ask the agent what's going on. The agent will look at the full context of everything it can discover, seasonality, historical trends, external factors and it'll give a recommendation. Maybe it's time to adjust the display or to run a local ad. So, let's take a look at another agent. HubSpot is a customer platform used by hundreds of thousands of organizations around the world and they used Agent Kit to improve the responses of Breeze, their AI tool using the custom responses widget. It then uses the Breeze assistant to search its own knowledge base, look up local treatments for the state's low humidity, pulls in policy details and puts everything together. It then offers multiple ideas and a recommendation. We have a bunch of great agent launch partners that have already scaled agents using AgentKit and it's available to everyone starting today. So, let's do a live demo and I will pass it off to Christina.
Unknown: So you may have already seen our dev day website. It's the site here that all of you have access to and has everything about today's schedule. But right now, it's just a static page. And to make this interesting, I'm gonna give myself eight minutes to build and ship an agent right here in front of you. Agent builder helps you model really complex workflows in an easy and visual way using the common patterns that we've learned from building agents ourselves. human in the loop and other logical nodes. Today, I'm planning on building a workflow that uses two specialized agents. The first will be a sessions agent which will return information about the schedule, and categorize the type of message coming in whether it's asking about a specific session or something more generic. And then I've added in an if else node to route behavior based on that classifier. Next, I'll create the session agent. Here I'll drag and drop an agent node, I'll call this session agent. I'll give it the context about kind of grabbing information about a session and then I can add in various tools here. Today, I already have a doc with all the information about sessions, so I'll simply drop that in. But showing the schedule should also be fun and visually interesting, not just plain text. So I'll also create a widget for them. I'll head over to our widget builder. Here I could create a widget from scratch. I can browse the gallery to learn about other widgets and reuse them. But for today, I've actually already designed a widget for this use case. In this case, it's an onboarding session widget for Froge, one of our dev day friends that you'll see around the venue who's holding a one on one onboarding session in Golden Gate Park. So we can simply download this Drop that in, we can preview it to make sure we added in the right widget and everything looks ready to go. So this session agent is now done. So one of the most important things when building agents is being able to trust them and guardrails help you have that confidence, protecting against hallucinations, adding moderation, blocking PII. In this case, already have a couple pre built guardrails. I'll turn one on for PII and then I'll just include I'll attach this in to the beginning of the workflow to make sure Froge is really protected against PII and then I'll add in an additional agent to handle cases when this information is passed in. So again, I'll make it speak in the style of Froge to stay consistent and I'll remind it that it cannot help with questions that contain sensitive information and remove kind of the context. Great. So I think this workflow is ready to go. I can also configure the output to determine what shows up to the end user. In this case, I can also turn off file search sources if that is kind of more internal. And I think that's it. Let's test it out. I can preview this directly from to learn about building agents. And I can see this message moving its way through that workflow we just created, checking guardrail, categorizing intent, pulling information from the file of sessions that I just added in, finding the right session, using the widget that I added, and determining, you know, orchestrating agents at scale at eleven fifteen with James and Rohan is like the best session for me to go to to learn more about this. And then I see a couple of rivets because this is this is actually Froze talking to me and and riveting at me. So okay. I think this agent looks good. Need to need to watch the time. So we just built a few specialized agents using tools. We added in guardrails. We customized them using some widgets, and then we also tested out the workflow in preview. The one thing we haven't yet done is a full set of evals and we can also do that directly from the agent builder to make sure that everything behaves exactly as expected before going live. But right now, I've got a giant clock chasing me and dev day is waiting, so let's publish this. the right on the right, we also have code export in case I wanna run this in my own environment, in my own servers. But you can see this is this is quite a bit of code to write, and so I'm just gonna stick with using the workflow ID that we just created and then head over to my site. So here in my DevDay site, I'm first gonna create a chat kit session using the the workflow that we just created. I'll simply drop in that workflow ID. I'll add in the chat kit react component using that client secret that we just created in our own server, and it'll have some froze specific colors and starter prompts. categorizing the message, pulling from tools from file search, using the widget that we designed, and then again deciding orchestrating agents at scale is the right And I can keep iterating on this agent directly in the in the visual builder and also deploy these changes directly to my site without making any code changes at all. This includes adding new tools, adding new widgets for other use cases, adding new guardrails, So in just a few minutes, we've designed an agent workflow visually. We added in some tools and widgets. We previewed it. We deployed it. We tested it and now you all can use it. This is actually live now in your dev day site. You can tap your badge and you should be able to to see it and use it and find the sessions that are best for you. So we're looking forward to using it and meeting Froge and also seeing all of the new experiences that you'll now be able to build using agent kit. Thanks and back to Sam.
Sam Altman: One of the most exciting things happening with AI is that we're entering a new era that changes how software gets written. Anyone with an idea can build apps for themselves, their families, or their communities In Japan, an 89 year old retiree taught himself to code with the help of Chatchi BT. He's now built 11 iPhone apps for elderly users. He's turning a lifetime of wisdom into tools that help others live more independently. In Spain, Pal Garcia and members of domestic data streamers are helping people reconnect with memories using chat GBT, image generation and Sora. At ASU, med students needed a better way to practice the kinds of difficult human conversations they'll have as doctors. So they built a virtual patient app with our models where they can try, fail and get better before they step into a real exam room. And at Versailles in France, visitors can now walk the palace and talk to it. They built an experience where you have a live discussion with art and sculptures with our real time API. History becomes a conversation. It's awesome to see what people are building and this is why we're so excited to give developers even more tools to build faster. So earlier this year, we launched a research preview of Codex, OpenAI's software engineering agent built to work alongside developers and speed up how software gets created. Since then, Codex has become very loved and grown into a much more capable collaborator. It works everywhere you code now. Your ID, the terminal, GitHub, and in the cloud. Your ChatGPT account connects everything so you can move your work seamlessly between your tools. This model is better at tasks like code refactoring and code review and it can dynamically adjust its thinking time for the complexity of the task. Developers love this new model and the codec's usage has gone up really fast. One of our key metrics for looking at this is daily messages, the number of tasks and conversations that developers have with codecs each day. Since early August, daily messages are up 10 x across codecs and this rapid usage has also helped GPT five codecs become one of our fastest growing models ever. Since its release, we have served over 40,000,000,000,000 from the model. Our engineers that use codex complete 70% more pull requests each week and nearly every OpenAI PR goes through a codex review and from that, people get more depth than they'd expect even from a very senior engineer. today we're introducing a new set of features to make Codecs more helpful for engineering teams. First, we have a Slack integration. This has been very much requested so that you can ask codecs to write code or answer questions directly from team conversations in Slack. Second, a new codecs SDK so that you can extend and automate codecs in your team's own workflows and third, new admin tools and reporting including environment controls, monitoring, analytics dashboards and more so that enterprises can better manage codecs. From developers building side projects on weekends to high growth startups to big global enterprises. Cisco rolled out Codex across its entire engineering org. They're now able to get through code reviews 50% faster and have reduced the average project timeline from weeks to days. We're gonna show you how you can use the new Codecs and our APIs to turn anything around you into workable software, and for that, please welcome Ramon to the stage.
Unknown: And so earlier, I asked Codec CLI to create a very simple control panel interface with a very simple interface. See, a camera feed on the left, So you know as you can see, Codex can respond pretty fast for questions like this. And this all seemed promising to me. So basically I went ahead and I typed this script over to Codex to completely scaffold an integration using the vSCA protocol and wire it up to that control panel. made a plan. Apparently three tasks have to be completed. It's now exploring the file. It's figuring out how to wire up these gamepads. And what's interesting here is you can see in the IDE we also have this concept of auto context. And what that means is that your prompts can be pretty short because Codex will kind of understand your intent. It will see like the recent files that you've used and really kind of adjust accordingly. So in the meantime, what else could we do? Well I thought one exciting interface is voice. So to save us a few minutes, I've already asked Codex to integrate with our real time API and our agents SDK. And I wanted to wire up all of this into the app right here on this green little dot at the bottom right of the screen. What's great about the real time API is that it brings natural speech to speech into your app, but it also connects to any MCP server in the context of that conversation. So let me check out this task that I sent to Codex, but this time in Codex Cloud. So you can see here my prompt. I asked Codex to wire up this MCP server for this very specific model of lighting system. I gave it like the reference docs that I found and I gave it the exact interface that I wanted to have for my UI to work. now Codex is my teammate. and you'll see this agentic behavior emerge. So, to explain you what just happened here, when I sent a task to the voice agent, it also added Codecs SDK as a tool, and what that means is that now, on the fly, I can reprogram this app in real time and instantly adapt it to user needs or any kind of feedback they have. So in this case, when I asked to create a credits overlay, and all of that without writing a single line of code by hand. So really give Codex your most ambitious ideas, give Codex your most complex coding problems,
Sam Altman: We've covered a lot today but obviously models matter a lot too so I wanna share a few model updates. Back in August, we launched gbt five. Leading coding startups like Cursor, Windsurf and Bracel are using g p t five to change how software gets written and shipped in their apps And then after that, we released g p t five pro, the most intelligent model that we've ever shipped. GPT five Pro is great for assisting with really hard tasks. Domains like finance, legal healthcare, much more where you need high accuracy and depth of reasoning. We're also releasing a smaller voice model in the API with GPT real time mini. It is smaller, 70% cheaper version of the advanced voice model we shipped two months ago with the same voice quality and expressiveness. and this is a big leap towards that reality. Now I want to shift gears and talk about what's new for creators designers, You can give it detailed instructions and it holds onto the state while delivering results that feel stylized, accurate and composed. For example, you can take the iPhone view and prompt Sora to expand it into a sweeping cinematic wide shot. But one of the most exciting things that we've been working on is how well this new model can pair sound with visuals. Not just speech but rich soundscapes, ambient audio, synchronized effects that are grounded in what you're seeing. So here's an example in this kayak video. Sora two is also great for concept development. We hope that now with Sora two previewing the API, you will generate the same high quality videos directly inside your products, complete with the realistic and synchronized sound and find all sorts of great new things to build. So, one of their designers can now start with a sketch and then turn these early concepts into something that you can see and share and react to. So, let's take a look at how this works. That is a very cool new way to build toys and it's incredible to watch with AI how fast ideas can turn into shareable, workable designs. The apps SDK for building native apps inside of ChatGPT. Agent kit so you can deploy agents wherever you'd like easily and with more confidence, a more powerful codex changing the way software gets written, helping your team ship faster, and new models in the API, GPT five Pro, Sora two and Realtime Mini that expand what's possible. A huge amount of work went into this and there's a lot more work, there's a lot more happening throughout the day. So, enjoy the sessions and we'll see you later. Thank you very much.