Unknown: So this is a wonderful opportunity to sit down with you two. I cannot think of two more thoughtful, well placed people to have this conversation. Our purpose here is to kind of role model the kind of cross sector, cross cultural dialogue that needs to happen between Silicon Valley technologists and global policy leaders if we're gonna move this forward. And we're gonna explore how a leading tech entrepreneur and a revered international diplomat come at this subject. mandate covers all of the areas on which AI has implications for human rights. Privacy, work, discrimination, Sam is really unique in the valley in that he has manifest a lot of creative leadership on problem solving and taking on some of the really tough challenges. And he's chosen to apply his entrepreneurship to in the nonprofit realm and open AI dealing with the very big problem or the big questions really around artificial general intelligence. And he's also taken on this experiment at the other end of the spectrum very concretely on universal basic income and the challenges around labor displacement, distribution of wealth, and finding meaningful work. So you've both really stepped out of your intellectual comfort zones, and I really appreciate you being here. I want us to start by opening it up and asking you from where you sit, how do you come at this subject of human centered AI? I'll start with the high commissioner. Tell us about your journey into this subject. What motivated you to get involved? And maybe place it in the larger context of what's going on globally in the human rights realm. And how important you think the human rights framework will be to this human centered AI conversation?
Unknown: that you are going to propose in the next half an hour. Look, it's very exciting for us because it's almost the dawn of a courtship. A courtship between two cultures, rather complex, one legal and with a long history and the other a technological culture steeped in its own grammar, its own lexicon, its own way of thinking. that the world, the way we see it from the human rights perspective, is taking on trajectories which seem to be contradictory. On the one hand, we see a continued progression in assertion and the realization of rights all around the world. We saw most recently in Ireland two referenda which produced results hitherto thought impossible. At the same time, we look around, we see what's happening in this country, we see the populace, the demagogues, the violent extremists. We see oppression is back. Internet freedom has been on the decline for seven years in a row around the world. We see human rights defenders who are often technologists thrown into a prison for offering an opinion arbitrarily so. And so, in all of that, we see a canvas, a background which is of concern to us. And, if we have a canvas that's dotted with glass and bits of flesh and blood, can we paint a pretty picture on it through technology? And that's the question that from our perspective we must ask. In other words, will technology help us stem the bleeding and the suffering of people around the world Or will it contribute to its acceleration? And, it's a fundamental question that must be asked. It's not to say that the world of technology must supplant the human rights community globally, But, it's the effect that it's going to have on the work that we are trying to bring forward. Finally, this other thought that has occurred to us. That as we become more and more dependent on technology, which itself is increasingly and almost wholly dependent on electric power, and the threat of cyber security continues to loom large as the world continues to suffer from dislocation. Is it not the case that we are just inching our way back to a pre industrial existence should it all fail? Should we have no grid that functions properly? No internet, No power to sustain the civilization we have. Ideally, what you need then is a body of law that will keep it together notwithstanding
Unknown: but I'm curious how and when you came to understand it might be your responsibility to deal with some of the implications of technology? And how do you generally think about the responsibility of the tech sector as it relates to the implications of AI?
Sam Altman: And I think the decisions that we make in the next couple of years are likely to determine how that goes. Know, this can either be technology that is controlled by a single entity for, But
Unknown: fascinating to bring that long time horizon in and say, doesn't matter if it's ten years or a hundred years, it's short in the scheme of things. I think that's very useful to end that conversation and say let's get on it. I think that's right. So neither of you touched on the relevance of the international human rights law framework so explicitly to this movement for human centered AI. And I'm curious how much you've thought about that and how important you think that will be to this conversation.
Unknown: It's it is vital to recall that we have nine human rights treaties, the core treaties, which basically explain the position of humanity in respect of the prohibition on torture, prohibition on enforced disappearances, the assertion of a rights agenda where the child is concerned, where the ending of discrimination against women are concerned, the rights of persons with disabilities and so on and so forth. All of this came out of the appalling and bloody experiences of the early part of the twentieth century. And, it was the death of upwards of a 100,000,000 people that crystallized the thinking of humankind at the time. That the previous reliance on customary law was not enough. That we had to create codified law, contract law that would bind states to their obligations. And so, in in that sense, it is a sort of historical distillation of human experience into a legal framework where we didn't have to once again go through another cataclysm to then wake up and realize that we must develop it. It's now been developed and it takes care of most situations that are brought before us and which are thought to be rather unique. But they're not really. They're not we've gone through these sorts of permutations before, not at the pace and the acceleration that Sam was speaking about earlier. So, what we have is actually not that bad. And, when you look at, for instance, the international covenant on civil and political rights and the issue of freedom of expression and the break asserted on it in terms of the prohibition on the incitement to hatred and violence. There is enough commentary on that to give us some guidance and can be further fine tuned. But, it needs to be understood that for most of the planet, that is the applicable standard. That is the applicable law, not the first amendment. And, that countries have obligations under that particular law. And this is what we hope we can develop in terms of an understanding between AI and AI that's human centered for which a framework already more or less exists and it just needs to be fine tuned.
Unknown: Just one quick comment. I think your point that it's a distillation of human experience. It's not simply a remnant of one point in history. I think that's a very useful framing for this conversation that it it captures a lot of human thought not just one moment. Look,
Sam Altman: But it is a complex topic and I'm sure we haven't defined them perfectly yet either. One thing that I think gives me great optimism is that I believe although it is impossible for anyone to write down exactly what all of human rights should be and get everybody else on the planet to agree to that. I think we already have something good enough that if we could just have it enacted everywhere we'd be happy and I think AGI can help us do that with less emotion and bad people and things like that. I also think that we will be able to have the AI, you know, learn the collective value function of humanity. And that will be more accurate than trying to write it down in language. today, you would not say it is it is a human right to have every material
Unknown: Yeah. So at least for now, we'd go for what are the minimal standards rather than utopia.
Unknown: The I think the the key point that you raised in the beginning, Eileen, is that as technology and AI speed up in terms of their insertion into our daily lives in ways, again, that in the past we couldn't properly conceive of, there's a tendency to be hurried in the way that we think about what needs to be applied. And in the daily froth of information that comes in and out and sweeps us away, you almost feel at a loss to know where to begin. And you need an anchor point. The anchor points already exist. It's just being aware of them. And, it's true what Sam says that the interpretation of the law can shift and change subject to the circumstances that occupy us. But the grounding and the principles, the inalienable rights that we are all basically owed as rights bearers, that doesn't change. And, it gives us, I mean, Kissinger's point about needing more time for And, it's just an awareness of it. Now, the problem that I see is this, that we in the human rights community, global community, basically human rights lawyers are wedded to culture that's very technical, that's in almost impenetrable to the non legal mind in many respects. And likewise, we see this in the world of tech. So, I was saying to Eileen last, I think it was in January, I was sitting with, I was invited by I had no idea what these people were talking about. And, until toward the end I realized that it wasn't so difficult to understand. But, they had wrapped it in a jargon and in a language which made it inaccessible. on one of the treaties, you will find that unless you have legal training, you're not understand what lex specialis is. Right? And so somehow we have to break down these walls. We have to talk to each other as two sort of human beings engaged in courtship. Excited but weary. And and then out of that, develop a fusion where you can see how both sides can benefit immensely from each other and not work in in a in a sort of the different sort of black boxes unaware of what the other is doing.
Unknown: That's great. So this idea of developing a shared lexicon. enduring principles, core principles, but evolving meaning of what it takes to live up to those principles. I call that like, the doctrine needs to evolve with a changing context. We now live in a globalized digitized world. It's demanding new things, but our enduring principles remain. And that's what we've got with the human rights framework. So I wanna shift a little bit to some of the specific, more concrete initiatives you've been involved in, both of you. And Sam, as I said, you've taken on two of the most challenging AI issues at opposite ends of the spectrum as I see it. what the animating you you actually got to the animating energy behind this creation in your opening here. But say a little bit more about the primary risk you were trying to protect against. Sure.
Sam Altman: in the world is going to create sort of the children of humanity. And that may look a lot like humans, it may look very different, but in any case it is going to be this major step forward. And about a number of scenarios. One, a single tech company developing this and having sort of this one AI to rule them all that they use for whatever they want and also that is like unchecked. If you have a single AGI in the world and it's like a billion times smarter you know, either concentrated benefit or ability to harm a single AI controlled by a single company seem bad. And you can imagine all kinds of things that you could use that for that are extremely dystopic. So I think in a world where the government, the US government function like it used to, like I'd like it to, But our goal is to develop AGI, get safety right,
Unknown: Because I mean, I can't understand AGI to be honest here. Because every major advance didn't come from a majority held position. It came from a very few people who decided that they and their society were going to end slavery against an overwhelming tide of opinion against it. And, those people who were going to advocate this were at great risk to their lives. In other words, there's a willingness to self sacrifice that humans have which you somehow need to get into AGI. And, that opinion starts from a very small corner of society and then grows and then reaches a tipping point and then becomes something so obvious that everyone adheres to it. And, whether we talk about the ending of slavery, the ending of the death penalty, the ending of apartheid within the Afrikaner community, whatever it is. It doesn't start from a majority opinion. It starts from a minority opinion. I was listening I was reading an account of Charles Darwin's early thinking on the flight over. And what was so remarkable to me is that when he finished his studies and finished reading Virgil and and Homer and Euclid, and he just had to do his residency requirements in college to get through. His hand just slipped on two texts. And, it was those two texts incidentally that he sort of slipped upon that set the stage for the Darwinian revolution that is still in play today. Right? I mean it's amazing. And, can AGI replicate that? I mean, it's just it's the preparation on one side and then the incidental occurrences on the other that shape human progress in a way that is dramatic and profound and beautiful.
Unknown: gonna add, but we're it's not only that it's not just computational. I mean, this this is a thing that I haven't heard other people raise very much. This human capacity for self sacrifice.
Sam Altman: Well, I'll start with that. There are a number of things in evolution that don't seem like they have been selected for. Human capacity to self sacrifice is one of them. In a vacuum, If it's just you, you know, if you do something that gets yourself killed for the good of the group before you reproduce, that's probably bad. Psychosis, like why should why why why has that been left in evolution? That's generally quite bad for the individual. It is though in rare instances quite good for the group. that are trying to accomplish shared goals. Sometimes you do need to do something for your team. In fact, in one of these things we have a variable called team spirit and we can do the slider of how much the agents care about themselves versus their group. And you see very different behavior depending on where you put that slider. They learn very different things. They make very different decisions. But a 100,000,000,000,000 synapses, and some hormones that act on those. And that's about it. And that's about it. And then you know, like between one and ten times a second those neurons fire and do their thing. And you know we can get into metaphysical arguments if we have time, we probably don't. But let's just take as an assumption that that that's it. There's nothing else. All of that will be modelable on an equivalently powered computer. We've got ways to get there. We have maybe three orders of magnitude from our best computers now. Human brain is it's incredibly slow, but it's incredibly powerful. It's just a huge amount of compute. But we will get there, and we will figure out the learning algorithms. They don't seem to be that difficult. One of the things that's quite amazing about the human brain is, you know, if you've had an accident or something, you can take a piece of the neocortex And I think that and other things like that are suggestive that learning actually is generalizable. In our own work so far, have seen that the learning algorithms are generalizable. The same algorithm that we use to learn how to control humanoid robots can also be learned to recognize images, to do strategic planning, to write short stories. So I think we're onto something there. But it's an uncomfortable thought, but I think everything we say, well, that's never gonna be done in a computer. There is no solid logical argument about why it won't. Because it's all just
Unknown: about Okay. I wanna actually stay on the specifics that you've been involved with but go other end, UBI. Sure. Because I think a lot of people don't know about that experiment and obviously there's so much concern growing around labor displacement wealth distribution.
Sam Altman: maybe tied to some expanded definition of work, is a very partial solution to the problems that we're gonna face as a society. It does not address the satisfaction, And if we could take the pressure off people so that people could People I think given that the human drive is what it is, we'll still find new things to do and you know, like people have been projecting And so the question is, if you give people enough money so that they don't have to work to fulfill their basic needs, what do they do? And this turns out to be a sort of based on the context I meet them in about it. But we are doing an experiment. We finished up our pilot early this year where we will do this for thousands of people for five years. And we'll see what happens. We'll see what happens. I'm not interested in like the traditional economics questions about how we pay for this. And so housing aside, early next year in multiple sites around The US in multiple different treatment arms. So some conditional, some not conditional, a few other things as well. And and the goal is like when we get to this sort of like machine money and human money and you know people already put this big premium on handmade goods, So that the better like if instead of a fixed dollar amount, every citizen basically owned a share of US inc. And in a year where the GDP went up a lot and everyone better and we made good choices collectively that made our country more wealthy, you got more money that year. And in a year we did a lot of stupid things and it got worse, you got less. And I think we have so lost for good reason. I don't I wouldn't want like another war to get this, but we have so lost this sort of American unity where you're with people from very different socioeconomic backgrounds together in this intense experience where you work for the same big companies or whatever.
Unknown: The last time I was in Silicon Valley was in September 2017. And absolutely amazed by what I was hearing and seeing in the two or three days that I was here. Three or four days after that, I was in Libya in Tripoli. Libya is the one country where the UN doesn't have a presence because it is so dangerous. It's basically Mad Max country run by armed groups. There is a government, but it's not in as much control as they would like to enjoy. And when I was in that space having been in Silicon Valley, a space that would be recognizable to humanity three or four hundred years ago because really there was no difference from that existence and the existence in the German principalities during the thirty years war. The same brutalities, the same denial vitiated, the complete range of rights vitiated. And the problem I think with Sam's narrative is that it is very western centric. It is basically a narrative for the North. And you may be right that the North will not have a giant sort of therapist bench sort of depicted for it as requiring meaning whether metaphysical or existential. But the rest of the world is going to be struggling in in ever more or ever greater levels of desperation. The beauty of the human rights framework is that it covers all these different contexts. It is a single unifying set of universal principles anchored in law. And that's the bridge between the two. That's the space in which we operate. And I think what we need from the technologist point of view is a deeper understanding. Actually going to these conflict areas. Actually sitting in people's homes after they've been bombed. And, understanding that that is also part of our world. And, how are we going to ensure that the rest, the global North doesn't become like the global South in many respects and bring a country like Libya into a Silicon Valley type of existence? But I think in overall terms, what we need is a sort of deeper thoughtfulness about all of this, That we don't rush headlong in to these areas and that's why I applaud what you're doing because I think it's not just a rush to market of a product and then trying to develop market share and profitability out of it. But understanding that ultimately, if we are to think about human happiness that really comes from one thing and we all know it. It's from service. You're a happier human being if you serve other human beings. If you think selfishly about your own condition, you will be profoundly unhappy for most of your life. And that's the basic, you know in all the religious traditions, that's the underlying rule that always emerges. So the life anchored in service to others is really what a human rights life is. It's caring about the rights of everyone else, not just about my own people, my own constituency, that's not it. Know, we will find protection
Sam Altman: the countries do need to make their own framework there. But I think AGI is this is is very global by nature and it will help the people the most that are in the worst place right now.
Unknown: And so you're gonna have to help us see how that happens. You know, that that's that's what people are struggling with. It still feels alien in Silicon Valley on this trip, which is that you are thinking about opening an office here for deeper engagement with the tech community, which is interesting. I know you already have an arrangement with Microsoft where you guys are looking to figure out how tech can help advance human rights. But traditionally we think of human rights obligations as resting with nation states, and there's this new movement to have companies embrace human rights responsibilities.
Unknown: from a discussion that I heard between my 15 year old activist daughter and my wife. They live in New York and I was about to go to work one day and my daughter was going to school and she was arguing with my wife, daily occurrence in our house. And, she was arguing because she was gonna go down to Times Square And, my wife said to her, you can't do that. You have to tell the school you can't just walk out. I mean, and they're responsible for your safety. how can we possibly be protesting if we must first secure the permission of the school? I mean, sort of ridiculous, that sort of thing. What it shows me is that there's enormous energy in this generation coming through. An enormous sense that there is, there are values worth fighting for. We see the insanity, I mean I'm on record but I've said this before. We've seen the insanity, the regression into ways of thinking, the modes of operation that hark back to the twenties and thirties of the twentieth century with different parallels that you can, and even before that, 1913. And, with different parallels that you can appropriate to and inject into today's discourse. And so, there's a liveliness and an awareness that, you know, these rights can be lost very quickly. You don't often think about them until they begin to disappear. One day you have access to credit cut. Next day you can't go to this particular location because x y and z has happened. And before you know it, it all slips away through a gap. And, I think there is a realization there. So, my hope is that using this as a sort of fuel, we begin this sort of dialogue between these cultures, between a culture of technology and AI, and we've heard AGI, that for us is largely alien, but exciting and interesting and certainly augmented technology can be extremely helpful to us in investigating crimes against humanity, acts of genocide, putting evidentiary materials together for prosecution. And, not just that, but whether it's access to housing, access to safe sources of drinking water, whatever it may be, all of that I think is exciting. And, it's like Sam was saying, it's just mitigating the worst possible effects of it, is where we have to be on our guard. So, I'm here to, we're opening up an office here. My colleague, Scott Campbell, who's over there sitting at the table, he's one of our most experienced field officials and deeply steeped in human rights, the human rights framework. And, our hope is that through workshops, we and interaction, we'll understand your world better and we want you to understand our world. Ultimately, we all want the same thing, right? We want a creative life filled with a sort of meaningful a meaningful aspect to it and to better the condition of the whole. We won't get there if we're pursuing narrow agenda. And neither or moving in that direction. Because we've been there before and it was a terrible outcome. Great.
Unknown: has a burning question, get ready. I wanna ask about the debate in Silicon Valley. that Oh, you didn't see it? Oh, it's great. It's really fun. You know, I've I've understood that you have almost been asked to broker some of these debates. Let's say between Elon Musk who has likened AI to North Korea and could lead to World War three, and Mark Zuckerberg saying that he thinks Elon's being fairly irresponsible in his language. I actually think strangely enough, even within the tech community, there is people are missing each other on their language and how they're using this terminology. And in some cases referring to existing AI, you know, machine learning that we have and positive applications versus AGI.
Sam Altman: if we just scaled that up, it would transform the world economically and job wise in ways that are difficult to imagine. If we never made another single breakthrough, if we never made a bigger computer. However, I also think most people would say that at some point in the next ten to one hundred years, we're gonna get it generalized. I think like the media loves to make fights. I think people like to say things that advance whatever their company wants at that point. But I think when you back away from all of that, there's actually not much disagreement. Yep.
Unknown: I couldn't help but think when I had to stand in line at San Francisco Airport to get through immigration. There's so many issues of inconvenience in our daily lives. If only we could employ narrow AI to solve, you know, lines at the DMV or lines at the airports. And so there are practical issues which we can, if properly employed, remove the inconvenience of these sorts of hurdles introduced into the discourse of the scientists. Right? What was so amazing about it is that much of what he introduced eventually came to be. And you thought one thought one thinks to themselves, well if it didn't start in 1943, if it started earlier, right? I mean if the the physicists began to think about these issues at an earlier stage, not when the project was so advanced, would we be at this position of discussing North Korea's nuclear fire? What's going to happen? I mean, I've worked on the issue of smuggling of radioactive materials and fissile materials. And I tell you it's a frightening frightening file list. The more you know about it, the more you're frightened by it. And these are issues which should have been properly plumbed at an earlier stage. They weren't because in the middle of the second world war, there was simply an imperative to get it done. But Bohr was asking the right questions and that's what I think we need to do at this stage essentially.
Unknown: So I think I am unfortunately gonna make an executive decision here in service of the program. I apologize for going over. I'm And I'm also gonna say you guys were the first two people I asked. You were the first to say yes. This program was built around you and you have left me somewhat optimistic. We're gonna be very ambitious in service of humanity.