Unknown: you can almost think as starting to match the function of individual brain regions, like a module for vision, a module for speech. By contrast, so artificial general intelligence is going to be something much more powerful. It's going to be a system that can actually understand an entire domain of study, can actually understand multiple domains, integrate information between them, and actually discover new knowledge, work with humans to solve global scale challenges that we don't have solutions for. And that's very much the mission of OpenAI, to get to that point. And it's not just to get there. It's to make sure that it really benefits not just a small set of people, but all of humanity. And then earlier this year, you switched to a for profit, but with capped returns for the first round investors. Why why did you do that? Yeah. You know, the way that we started OpenAI was we had this mission. We wanted AGI, artificial general intelligence, to benefit everyone. And so we started, in the most obvious way, as a nonprofit. But one thing we found as we started to make progress, as we really tried to be at the cutting edge and make AI just continually do impossible things, is that it's actually a very expensive endeavor. That, just like in physics, you've got to build these massive particle accelerators. In AI, we have to build these massive supercomputers. And so we needed to be able to raise investment capital but still stay true to the mission. And so we ended up designing a structure that is neither exactly a for profit nor a nonprofit, but something where we can take investment capital and if we succeed in creating an unprecedented amount of value, that we only owe a fixed return to those investors and to employees who have equity, and everything else is owned by the nonprofit to benefit everyone. Right. And that return is capped at 100 x? So that was that's true for the first round investment. You can almost think of it as, you know, as we make progress that future investors are are willing to come in for a lower multiple. Sure. Sure. 100 x is that's a lot.
Sam Altman: but I think you have to sort of play the field as it lies. And we just need so much capital to do our work. I think, like, we'll need more capital than any nonprofit has ever raised, probably. But but I think the technology we're working on has so much promise if it works, if we're successful in our mission,
Unknown: you know, think that you don't start on these endeavors because they're they're certain. You embark on them because they're important. But you've got a technical challenge that still needs to be solved. You're not just building a social So I think that AGI is something where there's it's just like there's been sixty years' worth of debate on when this thing's going to come, and there were people in the eighties and people in the nineties who thought that it was just around the corner. And I think it might still be true that it's just around the corner. I think we can't rule it out. But I think one thing that's also really important is that along the way to AGI, it's not quite like making a singular breakthrough and either you got it or you don't, that we're producing these super valuable AI technologies that are becoming more and more powerful. And so I think that actually the upside of if we truly succeed in the mission, we build this kind of system that we're talking about, we make it benefit everyone, that's going to be something on the order of the Industrial Revolution. And if we fail on that, then we'll just have created really amazing AI technologies that can still be used to benefit the world.
Sam Altman: I think it's always, you know, maybe we'll succeed, maybe we won't. Certainly, we have an enormously difficult challenge in front of us. But I was reflecting just now on January 2016. OpenAI started in Greg's apartment. I was mostly working at YC at the time, but I came by a few days. And I remember talking about what we thought we'd be capable of over the next, let's say, three years. And that so badly undershot what three years later we're actually capable of that I suspect, are underestimating what we'll accomplish in the next three years. I think we have technology that learns. This is this amazing, amazing secret in the world. That's what makes humans special. We just figured out how to do it. And we're able to keep driving that forward. And every year, I think the field is doing things that experts say are difficult or impossible. And, like, recursively defined, if that keeps going, But once you identify, like, the one most important thing and power laws being what they are,
Unknown: Well, the ideal outcome is that we succeed at the mission. So you could imagine what we're really trying to do right now is trying to continually be at the cutting edge of making AI do impossible things, all the way up to building a system that can really be an artificial general intelligence. And it's not just about then again, it's about actually applying this to these global scale challenges that right now seems like might just be out of our reach. Think about climate change. There are just so many aspects of that problem that are super hard. Should we You think about technological approaches, think about societal incentive problems. And if we can actually have tools that can work with humans that can really help us figure out how to tackle that, like, that's what we really want to be able to accomplish. That's right. You're just doing research at this point, and it's very deliberate? Yeah. So that that's that's very important. Right? The the way to think about it is that, you know, quote of, you know, that one of the best ways of doing a start up is you just find an exponential and you write it. So it turns out that right now, we are on this insane exponential of AI progress. You can actually look at it as driven by the amount of computational power that underlies these models, which has been increasing about five times faster than Moore's Law for the past six or seven years, which is just this really unprecedented amount of People don't get that. Like, intuition on that breaks down. That's right. It just totally breaks down. So I guess one way to think about it is that it's almost like if your phone battery lasted for one day, you wake up five years later, it lasts for eight hundred years. And then you wait five more years, it lasts for 100,000,000 years. That's the kind of progress we're seeing on the computational power. it's not a waste because I think that there's so much good applications. And actually, one upside of this Microsoft deal is that it's not just an investment, it's also a partnership. And we have an arrangement where if we choose to license technology, that they're actually super excited to commercialize it, put it into Microsoft products. And that's one way that we can both continue that we're working together to Exclusively. Build That's right. That's right. And that we're working together to build these massive supercomputers and push forward AI technology. And I think that people in the world who use Azure are going to see that platform continue to just get better and better for AI development. Mhmm.
Sam Altman: Yeah. If Microsoft can collaborate with us and we can build, like, the an amazing supercomputer to train these models on, That clearly benefits us. We really need that, Microsoft's great partner to build it. But that should eventually benefit all Azure customers with what they learn from it. It's hard to find a partner that can, you know, really scale with us on the capital side over the coming years, have the capability to build the computing infrastructure we need, and have a super aligned vision for us on the mission. Mhmm. So we're super happy we found that at Microsoft. And I think I think it's it's hard to to underemphasize
Unknown: how important that mission alignment is for investors. Like, as we said at the beginning, we have a very atypical structure. We have a very atypical mission. The idea that we might actually need to serve the world over serving investors, it's not something that most investors won't sign up for. But I think to Microsoft's credit, I think this is actually super aligned with their corporate mission. Because the overall mission of Microsoft is also to build AI for the greater good of humanity? Well, so the the overall mission of Microsoft actually is to empower I might not get the word for word, but to empower every organization and individual to achieve more. And the the actual individuals at the top who who run the company, I think, are super aligned with this mission Mhmm. Of trying to make sure that artificial intelligence and general intelligence happens in a way that benefits everyone. I think I've heard Satya say the same words. That's right. Was pretty close there. Satya
Sam Altman: genuinely believes that AI will be, like, the most important trend for Microsoft to get right in the coming decade. So and he really thinks like we do about how to distribute the the benefits.
Unknown: some AIs to play hide and seek. And what they learned was the sequence of strategies and counter strategies. at first, they just learned to hide in a room and put some boxes in there. And then they learned that there are these ramps that the seekers could use to jump over the walls. And eventually, they got better and better and got to the point that actually they figured out how to break the physics. So what we're actually seeing here is one of the seekers figures out that it can actually break the physics of the simulation in order to have the ultimate out catting and mousing and can actually get to where the little blue guys are hiding in the corner. strategies that we didn't know about. There should be a second video, which hopefully we can put up, where these seekers learned, hey, I can box myself in, or and the hiders learned they could box themselves in. The seekers learned, Hey, I can actually use the physics of this world to get on top of this box and surf my way into the shelter. And so I think the thing that's really exciting about this is that we kind of have this open world where you have this very evolutionary process. We know that evolution is the thing that made us smart. And there's a question that people have had for a very long time of can you use it to make AI smart? And this kind of shows that, yeah, you can actually even have them discover strategies and ways of exploiting your physics that humans couldn't have imagined. And I think this actually really showcases why we want to build these systems at all. We really want to keep pushing the intelligence, want to make the world more open, have more diversity of things these things can do, and then you can actually have them discover things that we didn't know that we didn't know were possible. Yeah. I think in a very simple environment,
Sam Altman: you can have incredibly complex behavior emerge just because of agents having to outsmart each other. If you think about humans, you know, we don't need evolution did not give us such giant energy inefficient brains because they help us outrun a lion or run down an antelope. We have them to deal with each other. And similarly, If you imagine what we just showed, you know, on this exponential ramp of sort of eight x improvement per year, as the models get smarter and smarter, I think you can imagine enormously complex behavior coming, new knowledge discovery coming, that we ourselves didn't have from this multi agent strategy.
Unknown: Yeah. Is that a good thing? Well, so I I think I think it's actually it's an interesting thing. Right? That I think ultimately what we want are AIs that can work with us to help us solve problems we can't. Right? Why why do we want tools at all? Right? It's to extend what what humans can do. So I think there's real promise there. But I think it also highlights the fact that, yeah, we're going to have to really think about as these systems come into society, you don't want your self driving car to go and do some crazy strategy that maybe has some bad externalities. So part of what we work on, it's not just the capabilities, it's also safety. It's also figuring out how do we make these systems actually do what humans want, be aligned with our values. And I think that's really crucial. Getting the systems to do what we want, And I actually think that I think that there's a lot of people out there who I think this think this problem is intractable. Like, how are you ever supposed to write down the reward function for humanity? How are you supposed to write down what you want? You can learn it. Exactly. And this, I think, is the really key insight, that we already have technology that can learn super well all of these problems and solutions that humans can't specify, and we're trying to do the exact same thing with what do humans want. Mhmm. and, I mean, tell me more about about why. Just just in terms of you've got You're training these You may have good intentions training these models. Right? Somebody else may say the reward model is very different. So this, I think, is actually All of the things we've been talking about to date have been these technical problems, the question of capital, the question of how you build these computers. I think this is actually the most important question. Let's say you can really succeed and build these super powerful systems. What values are in there? Whose values are they? We as humanity, we're not very good at agreeing on a global value system. And I think that this is also a super important challenge. And we actually have a policy team at OpenAI whose job it is just to think about this. Mhmm. And
Sam Altman: do you have any results from that? Do you do you how do you approach that? I think there are some things that most of humanity agrees on as values, and then the you know, there's a few big spectrums and sort of world philosophies. Maybe you could talk about, like, individualism versus collectivism is one big one. How we start a process now about a global discussion about what deployment of advanced AI is going to look like, But I think what it's gonna come down to is step one is just sort of a global discussion, which has not even really been started, I think, As people start to take this seriously,
Unknown: as the non tech world starts to take this seriously, the first stage is just a discussion about what kind of world we want on the other side. Sure. And I one really key part of that is it's not just a Silicon Valley problem. I think that so far, Silicon Valley, I think, has kind of started to really isolate itself off, and it's again causing a lot of issues. And for the most powerful technology ever, we just gotta really be able to get global engagement. Sure. In part, I'm asking because you had this other result recently, just like five years ahead of where we thought we were going to get it. We generated this model that just, we just trained it by looking at Internet text. And you could ask it to write an essay on, say, why recycling is bad for the world. And it came up with this great argument of why are we generating so much waste in the first place? Like, we're just masking the symptoms with recycling. And we'd never seen anything like it. And so everyone who looked at it with an open AI thought that the implications were totally obvious, but we couldn't agree on exactly what those implications were. Some people thought it was obviously totally benign. People used it to generate books and things like that. Other people thought that it could potentially be used for malicious purposes, for example, generating fake news at massively unprecedented scale. And when we started this company trying to make AGI benefit everyone, we always knew that not publishing everything would be part of that. The day would come, and we knew it was super important to be a year too early rather than a year too late. And so we might have been a year too early, but we decided to do a staged release of rolling out bigger and bigger versions rather than everything at once. Why the staged release? So some people accuse you, oh, this is just a publicity stunt they want to show off. We're so good. Our stuff is already dangerous, so we can't just show it. But but why do this this staged release? Didn't say that it's dangerous. We said it might be. We said we're not sure. And we said it's, this is like a good time for a practice run at a minimum, or maybe like actually someone will misuse it. It certainly got reported as us saying this is too dangerous, people have like, there's really this industry working group that's thinking about these problems, and that we've seen various people replicate the model, but also hold it back. Because it's you know, to some extent, it's not about GPT-two. It's about of misuse. That's right. And so I think that it's so important that you have the norms in place. I'm actually kind of glad that we took a lot of anger and ridicule and whatever emotions people wanted to pour our way. The funny thing is there was also a lot of support and people who you know, it's funny, I tweeted something about that recycling thing, and someone replied to that and got a whole lot of likes, saying, I can't believe these people aren't taking this seriously. Why do they think this is funny? And I looked at that and was like, exactly why we're taking this seriously. Got it. When are you going to roll out the full model? When are you going to show that to the world? So, I mean, my expectation, if everything goes if nothing crazy happens, is within the next couple months, If you look at the history of our ramp, that we basically release the next version, get some data, see how people are using it, and then go from there. And internally, you already have GPT 10 You're really building up this ladder of, okay, came up with this theory that explains these phenomena and simplifies everything and now I can make the next theory. And eventually you get to whatever applications you want. The way that we think about AI, think we take much more of almost a startup mindset to it, is to think about capability. We think of it as, can we solve this super hard challenge? And in doing so, we think we'll generate these general purpose methods that can be reused elsewhere. And so, for example, we built a system that beat the world champions at the complex video game DOTA two. We actually took that exact same training system, pointed it at robotics to solve this robotic hand problem that no one had to be able to solve. And so I think the way that I think about it is, yeah, for sure, everything we generate still has limitations. But somehow they're becoming more powerful. They're able to do things that are years ahead of when we thought would be possible. And as long as we're on that ramp, then I think we're being successful. Do you think that's enough? Just making the current methods better? Or do you need a completely new technology at some point? So I think there's only one way to find out. Like, our hope, and honestly, hope with Dota was that we would find our current methods would hit a wall. We'd scale everything up, we'd push it as far as we could, and it just wouldn't do it. And that, I think, is the current unsolved challenge in AI, is trying to find a real task that we just can't solve through pushing all our existing methods. We have to fail at something to know what to do next. You ask it to add some numbers, it just starts making stuff up. Because the data you fed it, it has nothing to do with math. Exactly. Right? And maybe if you just feed some data that has to do with math, it'll work. But maybe we're missing reasoning. Maybe that's something that's not there. And so right now, we don't really know that answer. And that's one thing that we're really trying to discover. Sure. It can write a book at this point. That's right. Yeah. So I think maybe a good thing to wrap it up is I brought on stage this book. It's not written by a human. This was written by GPT two and illustrated by a human. But you can kind of look through it and see if I can get in the camera properly. And you can kind of see that it's got lots of nice game mechanics for this RPG game that it wrote. It's got lots of nice lists of different elves and items and things like that. And I think that this shows you why these technologies are amazing today, is that you can actually work with them in order to build artifacts or to build creative things that humans wouldn't be able to do on their own. So that AI is a bit of a nerd. What's next? What's what's the next big step you're expecting to see? Continue the ramp. I mean, I think that that reasoning is a good example of a technology that no one's ever seen. I think that that GPT two is on the cusp of having machines that we can actually talk to and can understand us. But we're not quite there. And I think that there's