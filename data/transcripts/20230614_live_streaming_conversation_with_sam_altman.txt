Unknown: Greetings to all of you. Hello, everyone. All the AI developers and enthusiasts, to embark on a journey into the realm of artificial intelligence, guided by the wisdom and insights of one of the brightest minds in the field, Sam Altman. So I extend my deepest gratitude to OpenAI, GDP Ventures, CORICA for organizing this remarkable event, providing us with a platform, actually, that will excel into the extraordinary advancement of artificial intelligence. In my capacity as president of CORICA, the collaborative research and industrial innovations in artificial intelligence, also as a principal engineer at the National Research and Innovation Agency, as well as a professor of AI at the University of Shakwala, I stand before you humble and honored. For the past thirty five years, I have dedicated my life to studying and researching AI, starting with the knowledge base of machine learning. So I'm really proud to represent the thriving AI ecosystem in Indonesia. and acknowledging that those who harness the power of AI possess the potential to shape the future. the cutting edge technologies like generative AI, large language model, chat GPT, Dell e, and certainly of your highest interest is the artificial general intelligence. We must face the development of the AGI head on equipped with the right strategies and measures. President Jokowi also underscores the importance of our ability to learn and to adapt to this AI technology as it grants us the power to seize opportunities, optimize AI's potentials, and determine our destiny. Today, as we come together, let us acknowledge that the growth and the prosperity of artificial intelligence ecosystem rely on talent, collaborations, and partnerships. In Indonesia, we have taken significant strides towards nurturing our AI ecosystem, exemplified by our Indonesian national AI strategy. AI toward Indonesia 2045. So I hand you this and priority areas of AI development, propelling Indonesia towards becoming a global leader in AI. And by aligning our efforts with this visionary strategy, we create an environment that foster the collaborations and integration of AI technologies across diverse sectors. strives on collaborations. In partnership with others, we pull together our knowledge, expertise, and resources to harness the full power of AI. An integral component of this effort is Corica. It acts as a platform where individuals, academia, industry, community, and government can collaborate, exchange ideas, and collectively enhance AI ecosystem in Indonesia. This collective endeavor will catalyze job creations and strengthen the economy. And now, it is my great pleasure to have Sam Altman, the visionary leader behind CHET GPT. We are really thrilled to have you and the OpenAI team here. This event, the conversation with Sam Altman proved to be really popular. The attendees snapped up 1,000 tickets in just thirty minutes. I it got me thinking, wouldn't it be fascinating if if we could combine the creative power of g p t four or DALL E just to and
Sam Altman: education is certainly gonna change dramatically. You know, I I think it is forever altered by the course of this technology. But that's happened many times before in the history of education. or a more recent one like the search engine. And if you go back and read articles at the time, they sound very similar to some of the debates we were having today. You know with the calculator, math teacher said, well if if students don't need to memorize you know times tables or like how to how to like calculate a cosine by hand or something like that, What's the purpose of mathematical education? And how will we ever test if they learn? They could be cheating with a calculator. And of course And and the same thing when Google came out. You know, teachers would say, well if if people can search for any fact, why are we making them memorize all these facts at all? What's what's the point and how will we ever, you know, know unless we're making them sit in the classroom with no computer? And the answer in both cases and many others is you embrace the technology. You say this is like a new tool that humans have with better tools, human ability, human creativity, human potential goes up. Expectations go up too. And you automate part of what people can do and you let them use their cognitive capacity and their creative ability for something new and and more powerful, more impactful, more valuable. So we What will happen is We'll still do a lot of the things that we do today. You know, writing is important as a way to learn how to think. You know, you get this like expanded external brain on the page. But we'll probably evaluate it in very different ways. And we'll probably teach it in different ways too. You know, we'll teach it in ways that reward the process of thinking of coming up with new ideas with the help of the tool But but we should do that. Like the the idea of you know giving a calculus student a you know a test not letting them like use a calculator as they're going would would seem a bit strange. And if we make people do everything the old fashioned way without using tools like ChatGPT, that'll seem a bit strange also. So I think the you know, we're seeing a whole range of outcomes here. We're seeing teachers who are really resisting this. Saying this is like the end of the way we do education and really have a very kind of, you know, rigid to it, which I understand and empathize with. And then we have teachers who are saying like, I'm gonna require my students to use ChatGPT for everything they do. And I think that's important. And if we think about what the world is going to look like and as these people go off and create things, do jobs in the world, they're gonna have these tools and not training them to be as effective and as useful and creative as possible seems really wrong. So, you know, we've seen Education I think is an interesting example of what's gonna happen in other sectors going forward. We've seen the full cycle. When ChatGPT first came out in The US, teachers and school districts were racing to ban it as fast as possible. This like moment of hysteria in the first month of launch saying this is the end of education, whatever. And it did get banned in a lot of schools. But then pretty quickly after that it was teachers themselves that were saying, we made a mistake. We're gonna embrace this. We're not only gonna unban it but we're gonna like build it into our teaching. And that has been super valuable I think for many students. The people that are starting their educational journey right now I think will be far more capable than any of us by the time they're done because they're gonna learn it with new tools. Like this is this is how humanity makes progress. We give our people, we build better tools, we build on top of the tools before we contribute back to the societal infrastructure. And people surprise us every time on the upside of what they're capable of doing. On the second question about the nonprofit to for profit switch. We started as a nonprofit because of our mission. And our mission has not changed. Our mission is to build safe AGI and figure out how to broadly distribute But our tactics have had to change as we realize just how expensive these systems are. We've raised more than 10,000,000,000 US dollars. We expect to raise way way more over time. And given the shape of the technology and the need to make these extremely large neural networks and the ability to sort of attract and retain super talented people, we weren't able to afford that as a nonprofit. But we didn't want to give up on our mission of doing this to share the benefit, the access, the you know governance and the decision making with society. And so we came up with a new structure where we're still a nonprofit but there's a subsidiary capped profit. So we can use the magic of capitalism, give our investors and employees a certain fixed return. But beyond that have the excess return go to our nonprofit. So that if we really do succeed at our crazy dream and we make this technology that is the most impactful technology human ever
Unknown: Okay. Thanks for responding to Mahs Pantry here. So I want to go to the floor actually for the next questions. We are using a multimeter for just listing the top questions. So can we have the the top questions on the screen, please? if organization meet agents that are like humans? For example, with names and answer question like a company system,
Sam Altman: I think my mental model is this is it's definitely intelligence but it's a sort of alien form of intelligence. And it'll be good at things that humans are not always good at. It sometimes won't be good at things that are very easy for humans to do. But it will get better at everything. The number one point that I think is misunderstood about the field is the rate of progress and how long we expect that to continue. They'll be incredible at some things. But I think what will happen is not this sort of standard standard conception of you know, we have this like one that are all you know being directed by different people doing things for us and contributing to this sort of like society in the same way that all of us do. Like the collective ability of this room and intelligence to do a lot of things together is superhuman. Like that is a super intelligence in some sense. Right. And I think we'll see the same thing as we have lots of agents that can do these tasks that we want even if they're weaker than humans individually.
Unknown: we will have this super assistant that
Sam Altman: And we'll all like if you imagine each person being the CEO of a company with like a thousand people and what they can get done especially if those thousand people coordinate well. I can see a future like that. So each of us just has like much more ability to get things done in the world and things just happen much faster.
Unknown: of the list. What is the one other direction that is most promising for AGI apart from these large language models?
Sam Altman: And the question or the better format of the question is what do we do next on top of the large language models. And for that I think we ask the question of what is the thing that is most valuable for the world that these large language models cannot do. And there's different opinions to that but the one that I believe is contribute new scientific knowledge to human society. I think the way that things get better is the discovery of new science and new technology. Like that is that is the sustainable improvement in human quality of life. If we can imagine a model that we can say, you know, please cure all disease. Please discover the optimal method to teach every student every anything they'd want to learn. Please help us figure out better cooperation between nations. Please discover physics that let us you know solve climate change or travel between countries very fast. systems that can discover new ideas and help us make faster scientific progress, I think that is And I think that requires a new research paradigm on top of these LLMs. If we think of an example of something historical, we could say you know Newton spent a long time reading textbooks and talking to professors and colleagues and then reading papers and journals and made all of this progress with his mathematical understanding, his ability to do math. And that was that's like a very wonderful thing and that's kind of what our current models do. But no amount of reading textbooks was going to help him invent calculus. At some point, he had to go off and like think harder and come up with new ideas that didn't exist already.
Unknown: perhaps you can elaborate on that. How does OpenAI evaluate whether an AI implementations or output is deemed good or bad? I think that is something that's this creative output I mentioned earlier. Is it primarily based on consensus among a group of experts? Or does OpenAI involve a panel of key observers? I know the reinforcement learning with human feedback is certainly involving many of humans. Yes.
Sam Altman: idea of how we align our models. How we decide what they should do and not do. Say and never say kind of where the limits are. The first part was could we come up with a technical solution? And a few years ago, it was very unclear. Know when we launched GPT three, pretty pretty well. We've shown that we can get it to to You know, who decides what values we're going to align these systems to? Who decides what the outer bounds are of what an AI will do and not do? And also importantly, and maybe even more contentiously, who decides what the default's gonna be. Within that, you know, within these wide bounds, different countries can have different rules. The AI can, you know, read for example every law of Indonesia and try to follow it. The AI can read a 10 page document of, you know, here's the values and the morals of people in this little subculture and do that. Or the AI can talk to you individually, understand your value system, and do that. And so there's a huge degree of customization But now we've got to decide like what the general rules are going to be. And it shouldn't, of course, it shouldn't be OpenAI deciding that. Right now, is our human feedback providers that do it. But we want to make this as democratic as possible. We just launched a program to give grants for people with new research ideas about how to collect this data on human preferences in a just way from the entire world. We have some of our own ideas that we're trying to about how we talk to people, understand their values, how they think about different trade offs. Also trying to help educate them. Still let them make the final decision about different perspectives. So maybe, you know, we can help make some moral progress this way. But that's our plan. Our plan is to try to hand this decision making over democratically to the world and respect different, you know, legal frameworks as we do so.
Unknown: so as a language model that is reaching diverse global audience including over this 300,000,000 Bahasa Indonesia speakers, Indonesian language speakers in Indonesia and Southeast Asia, it is crucial to ensure inclusivity and accessibility. So considering the significance of Bazaar Indonesia and the rich linguistic diversity landscape in Indonesia, as you know, are one of the largest country with four seventy two local languages and dialects. we want to know how OpenAI would like to enhance the language support and resources for Bahasa Indonesia by expanding the capabilities and resources for Bahasa Indonesia?
Sam Altman: We'd really like to improve on this a lot. You know, GPT three was good at English and barely passable at other languages. 3.5 was okay at top languages. Four is very good at say the top 20 languages and passable at maybe the next 80. We would like g p t five to be very good at even smaller languages and dialects. We need help for that. I think if Indonesia can make a dataset available and evals for those languages, we would be delighted to take that and put it into our next big model. And I think we can make something that is very good
Unknown: to its limit and won't scale further than GPT four. Is that true? And what do you think a better architecture will look like and is OpenAI researchings
Sam Altman: We still don't see any limits to how far we can push these models. We may find some at some point. It'd be be interesting if we do. But what has happened is we've like used up the easy compute overhang. And it's gotten a little bit harder to keep delivering the same levels of gains just for compute. So we do need new paradigms. We need more efficient algorithms, better data sets, all of that. There's many new directions to pursue, but the one that I think is most important by far to get to AGI is the thing that I touched on earlier with being able to generate new ideas. That is still something that our systems are really not good at, know, contributing new knowledge back into society. I I think one thing that I learned at Y Combinator and certainly for OpenAI is if you want an occasional OpenAI, you have got to make it professionally, socially, economically, whatever, acceptable to fail hard many times. If people feel like it's okay to totally fail at something, not good. It always sucks and is painful. But if it's at least tolerated, then people are willing to take bigger swings. And one of the things that we that we noticed at YC that was just always very different about Silicon Valley than the rest of the world is you could like try a very ambitious idea. You could totally fail at it and like Silicon Valley, the whole machinery would totally tolerate that and let you go on and do your next thing. And in in much of the rest of the world, if you And what what I think that does very naturally is push people to more incremental ideas with higher probability of success. And that's perfectly fine. There's good reason for that. But if you want to stay at the forefront of technological progress in a startup ecosystem, then it's not fine. Because by sort of by definition, And and so I think this is this is like a pretty important thing. And it's not it's not just like access to capital or jobs. It's gotta be socially acceptable too. It's gotta sound like okay to your friends and your colleagues to say, I'm gonna go try this this very ambitious likely to fail thing. But that was fine. Like Silicon Valley has a long history of tolerating that. So I think that's the single biggest difference I've noticed. The talent is distributed super well around the world. Capital is getting better distributed. But this sort of this culture of we're gonna take the big swings and it's okay if they don't work out. I think that has not spread as much. So
Unknown: at the congress senate hearing and I see a lot of tough questions coming from the senators about, you know, how we regulators should be doing in light of the this generative AI and AGI and things like that. That including how to actually nurture the the AI ecosystem. Right? So in some of the the question that they have posed, I think I would like to
Sam Altman: like levels of threats to address and also different time scales. Different countries are gonna have very different regulatory approaches to some of the the short term challenges. You know, how do we think about issues of bias in the systems? How do we think about issues of reliability? How do we think about sort of economic impacts? But one thing that we've been discussing on this trip around the world, this is the twenty first country we've been to in the last month, is the need for the global community to come together around regulation of AGI. So as these systems become so powerful that they affect the entire world, I think we do need a global response there. The upside is of course tremendous, but the downside gets quite severe as these systems become, say, as powerful as all of current human society. And there's been a great deal of interest on our trip around how the world comes together to say, okay, what is a licensing framework and what are safety standards for people that are at the very frontier? We certainly don't want to put regulatory burden on smaller companies, open source models. That's very important. Oh, yeah. But if someone's creating a system that they expect to be a real AGI, there are safety concerns there that affect all of us. And I think that calls for a global regulatory response. That's very hard to do of course but we've done it when we faced global risk before. Nuclear weapons, some parts of biological OpenAI than any company that I've worked closely with before is fundamentally, we're a research company. And you know, we do have to make products and we love making products but that's all well understood. So the metrics for like ChatGPT are standard and what you'd expect from any company. Know, we look at or or even our API. You can just look at like active users. You can look at net promoter score. You can look at revenue growth. And that's all like well understood and that's kind of like in the tech company lore. What what's new for us is how to build a company that does great research. And what metrics are you supposed to look at for a company that's trying to produce repeated research breakthroughs? And you know, when I came into OpenAI thinking like, oh this will just be like a software company. We'll set quarterly goals. We'll have like metrics that we look at. And that really didn't work at all. There are sometimes like when we're training a model and we have an evaluation with a metric and we get to watch it go up and that's very satisfying. Everyone loves that. It's really fun. But if you're trying to figure out a new paradigm, that kind of like just doesn't that just doesn't work. There was a researcher that used to work with us named Ken Stanley called, greatness cannot be planned. And it really changed like all of my thinking about how we manage research. And now, it's it's like much more about If if sort of it fundamentally fits our the parameters of our research taste. Most of the time it doesn't work out. When it does work, it works breathtakingly well. And then you the kind of the art is trying to notice early when something is showing these early signs and you wanna put a lot of resources behind it. Alright. Okay. Thank you. So do we have the next question here? AI capability? I think you like this question. So so one of the reasons we're doing this trip around the world is to get out of the Silicon Valley echo chamber and try to understand more how people are thinking, what they want, and just get additional input. So I'm curious just by a show of hands, how many people in the room are pretty worried about AGI risk? I think both short term risks and long term risks are very important to think about. Our mission as a company is very AGI focused. And so we spend most of our time thinking about these longer term AGI risks. There's many other companies thinking about the shorter term risks. However, we believe in iterative deployment and we think we have to address the risk at each step. So even though our mission is about these longer term risks, we spend a great deal of time trying to mitigate the shorter term risks and that helps us learn along the way. I think that g p t four is the most aligned model ever put out. And there's no existential risk from g p t four. All of the risks there are short term, clear, and present. And it took us about eight months after we finished training it until we released it to address those short term risks. So we think there's a balance but we differentially because of our mission focus our attention on the AGI risk. answer is that the future models have to decide what good data is and what isn't. What they want to train. Like the model should choose what data it wants to train on. And if the data is high quality and interesting and increases the model's capability and understanding, whether it's human generated or generated by previous LLM, that's okay. The model will have to decide, is this accurate? Is this helpful? Do I want to like believe this and add to my training set? It's the exact same thing we do. You know, when we're presented new information, we use our existing model of the world to decide if we wanna incorporate it or not. And whether a person tells us that or technology tells us that, we still just evaluate the quality and the utility of the information.
Unknown: Indonesia national strategy. Right? So Indonesia has set ambitious goals in this AI national strategy. We try to aim and harness the potentials of AI for the economic growth and societal development. And I think OpenAI has been at the forefront of AI research and development mission aligning with these goals? Especially for our national strategy, we put down four focus area is ethics, regulation and policy, data and infrastructures, talent development as well as research and education. And on top of that, as the baseline, we have these five priority areas written in that national strategy. One is healthcare, the other one is food security, research and innovations, bureaucracy reform as well as smart cities and smart mobility. And as you know, we are also are planning to have our new capital city, Nusantara,
Sam Altman: So we're certainly very aligned on the inputs. Those are I think the key things to making progress in AI that we talk about too. And But all the different ways that we can drive improvements in quality of life, those are all important to us. Mhmm. I think the best way we can support you in that is to make the model as good as possible. Certainly support local languages. Right. But our kind of plan is to make the best,
Unknown: with the appearance of larger models on health, on food and other and agriculture, yes. So let's try to go to the next question on top on the list. Recently, we've seen companies such as Palantir implement large language models for military use. What is your stance regarding this ethical application of this?
Sam Altman: a particular single sentence because of course the answer is it's like very nuanced and it depends. You know? I think we all think it's bad to have like autonomous lethal weapons. I think we all think it's fine to use these models to like defend yourself against cyber attacks which is gonna become more important. And then there's a whole bunch of stuff in the middle that
Unknown: have mistakenly implicated that students for implicated students for allegedly using ChatPGPT in their work. I think that is really something that we heard a lot. And what is your opinion on this matter? And how would
Sam Altman: So I think there will be ways we can make these detectors better. We can also probably make some watermarking systems that help encode in images and maybe in long versions of text if if something is likely to be generated by an AI. But they won't be perfect. And I think, you know, a GPT detector going off should not get a student in trouble because I don't think we can say with certainty that that's what's happening. We've just adapted the way that we evaluate students. And I think that's the long term solution. But in the short term, we'll have to figure out how we're gonna balance these. You know, I think we'll use detectors, we'll use watermarking for some stuff, but we'll understand it's imperfect. the world's ethical values and building a system that can respect those and allow customization in different contexts is super important. So if you're interested in this, I'd really encourage you to apply to our program to help with this. And I also We'd love feedback on what we're thinking about. The things that we're trying, you know, the systems that we're starting to use to collect this. We have a lot of users now. So we can talk to a broad swath of humanity and get and collect this feedback.
Unknown: but we still need to undertake to address these concerns, especially for OpenAI that is rapidly developing the large language models and others. And I think this was also being a disclaimer of by Czech GPT, right? And so can you emphasize the need for how we can respond to inquire more on the specific measures in implementing to tackle buyers in AI system and to ensure that the benefits of AI. I think because we are in Indonesia with this 300,000,000 populations coming from different ethnic backgrounds, we have layers of ethical principles. And also our ideology, Panchasila, this is the five principles that we we adopt to.
Sam Altman: I think no two people are ever going to agree that one system is unbiased. Like that's probably impossible. Right? There are going to be some things that we have to agree on globally. You know like are these systems ever allowed to go kill people is something we can probably agree on easily. And there will be other things that are very different in different countries. You know, like free speech is handled differently in The US than in other places, for example. And again, we have an incredible new tool, which is these systems are capable of learning and understanding from natural language. Mhmm. And so, we can have enforcement from the systems that is very different from what we've been able to have in, you know, like with any previous technology. And so, can imagine a world where we have a global tier and we say, alright, here is the limits on what a very powerful system just can't do. Because we think it's like it's not only unsafe but it's so unsafe it affects all of us that you can't use these systems to design like a really toxic pathogen. That seems reasonable. In different countries, then there will be different laws and different values. And then there will be very personal questions. Like, know, And it should learn what you think. And it should if you want it to give your answer, should do that. And if you want it to like push you in like suggest a new food you haven't tried before, it should do that. So there will be there will just be these different classes of how we have to think about the ethics and alignment and values issues. Yeah. But giving people a lot of control within the very wide bounds of their society and the globe, I think is quite important.
Unknown: Okay. So I see that we have reached our time actually. Okay. It went so fast. Right? So probably, I need to give the some time for you to have the closing remarks to all of us here, the Indonesian, say, I enthusiasts, developers, all the also to the minister, Mas Mantri, Nadim, and all the regulators that probably here, the ecosystem from community. I think academia, businesses,
Sam Altman: the thing I'll I'll leave with is very simple. I think this is likely to be the most impactful technological revolution that humanity has yet created. And the importance for all of us to participate in shaping that, the more we work together, the more we talk, the more we really engage with this technology and decide what we want the world of the future to look like, the better it will be. One of the reasons that we deploy these systems as we go is so that we can have this conversation. So that people can get a feel for the technology. Where it's going? What's going to be possible? What's not? What we want and what we don't want? And embracing this while figuring out how to mitigate the quite serious risks, I think is the key to the world getting much much better. And we can be in a world twenty five fifty years from now. Think that is like almost unimaginably better in terms of everybody's quality of life than the world today. I think this is like an unstoppable technology but certainly one we shouldn't stop. But we do need to guide it quite a lot. certainly one of the biggest challenges we've ever faced. But definitely one of the most exciting and the highest potential. And I think the people and the countries that embrace this the fastest will be quite rewarded. And