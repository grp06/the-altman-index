Sam Altman: So I certainly think data is still important and still valuable, but it turns out that the most the most impressive advances that we've had in the field of AI research, think, have been more about, last year with DOTA two, we beat the best team in the world with no data whatsoever. The entire thing was the agents self playing each other, exploring the environment, trying what worked, stopping what didn't, and sort of good RL algorithms to do that. Now for a lot of businesses, of course, it is important to have data, but for the field in general, I suspect going to see an explosion in the next few years of systems that can really process, understand, interact with, generate language. And it will I think it'll be the first way that people really feel powerful AI because you'll be able to interact with systems like you do by talking to somebody else. You'll be able to have dialogue that actually makes sense. You'll be able to process and computers will be able to process huge volumes of text that are sort of very unstructured and you as interacting Most people remember how bad it was five years ago, and people that use Siri or whatever have noticed that it gets a little bit better every year. Actually, not a little, a lot better every year. And now it basically doesn't mess up even in difficult environments or it doesn't mess up appreciably more than humans mess up when they're trying to understand. So I think that's one that people that seems to resonate with a lot of people because it's in it's in recent memory. When Well, we work on a very long time horizon. I think there have been three great technological revolutions so far in human history, the agricultural revolution, the industrial revolution, the computer revolution. I think we are now in the early innings of the AI revolution, and I expect that one to be bigger than all three previous ones put together. Thinking, understanding intelligence, like that really is what makes humans humans much more than our ability to get physical stuff done in the world. And so I think this is going to be a huge deal and impact life in a lot of ways. I think I've been very inspired, as many others have, by the example of Xerox PARC and the technology they created was an enablement for the computer revolution in a lot of ways. Alan Kay, who is one of my research heroes, used to sort of berate me nicely for saying these companies never think big enough. At Xerox PARC, we created, I think the number was, like, by his count, $30,000,000,000,000 of value. Who's thinking on that scale? And my hope is that the work we do enables $300,000,000,000,000 of value for other companies, and, you know, we'll also hope to capture some of that value ourselves, but but we really wanna figure out I think it is both true that this time is different and also everything has happened before and everything has happened again. General intelligence is a powerful thing. I also believe that as hard as it was the time of the Industrial Revolution to imagine the jobs of computer programmers working with big compute, desire, its creativity seems pretty limitless, and I think we will find new things to do. Betting against that has always been a mistake. definitively looks like when when computers are more intelligent in some ways than humans or when computers can do most work that humans can. So the only prediction I can make with confidence is that things will be very different, And anyone I think who says we're gonna keep everything the same is lying, but although change is inevitable, we can work really hard to make sure the future, although it's guaranteed to be different guaranteed to be different, is better. People have all kinds of ideas about things they're going to do to keep it going. Maybe they work, maybe they don't. But the version of it that is important for AI, which is for AI specifically, how big can we make our biggest models, however we get there, you know, plugging a bunch of computers together, optical interconnects, whatever it takes to be able to sort of train these massive models, that has been growing about eight x per year for about eight years now. And I think it's gonna keep going like that for about five years. So again, at this point I have this like narrow focus on this one thing that's really important to me. There's probably a lot of other things that are gonna happen for compute, but the question is are we gonna have bigger and bigger computers to train neural networks on? And the answer is yes, and that's super exciting. admin to this? I I think, yeah, it is a huge problem. There will be a handful of companies like OpenAI that can train the largest models, but once they're trained, they're not as expensive to run, not even close obviously. And so I think what's gonna have to happen is there's a few people that can train the large models and we figure out how to share them with other people who can't train them. And I think there are very rare occasional decisions where a small group of people tired at three in the morning in a conference room make a fifty five forty five call and it has a massive influence on the outcome of history, progress continues. So in terms of those like few incredibly consequential decisions, I mean there are there are like, everyone has got a story they love. The one I love is the Russian military officer that decided not to push the button to launch a nuclear bomb when he thought that they were likely under attack and it turned out to be an instrument error, and that one decision by one individual when he really, all of his training and policies are he should have pushed the button to launch, that's like a case where the world could have really gone the other way. And to be perfectly honest, I think a decision like that is bigger than all of, in in terms of one hinge decision, any single decision tech companies usually make. we have like a set of principles. We try to write up in our charter what those are and we'd like the public to hold us accountable to them. I think people can disagree with the charter though. As the stakes get higher and higher, no one organization and certainly no one person should be making decisions for what society, what the new social contract looks like and how this technology gets used and sort of how we share governance and economics. I think a thing that we will move to in the coming years or decade is more and more of our decisions will be influenced by an advisory board that we'll need to put in place of people that can kind of represent different groups in the world, which right now we don't have. I think as we get closer to something that feels like AGI, none of us deserve that right nor want that responsibility to single handedly determine because I think one thing that the technology industry gets wrong, and I myself am often guilty of this, is believing that the technology solves all problems. And second of all, I think there's a huge set of problems around public policy and people's optimism for the future and the kind of betrayal they feel by the country or sort of the American dream that didn't that kind of somewhat got taken away that most people in Silicon Valley say, well, we just need better technology and we'll solve that, you know, we just need AI and we'll solve that. And AI will help and better technology will help. But I think these are policy and governance and leadership issues that it is a mistake for the industry to say we know better than everybody else about how to solve it. like very basic one I would say is that the current generation of young people is the first generation, if you believe the polling, which who knows, in American history to not think their lives are going to be better than their parents. So this has worked for two hundred and forty four years or forty and now it doesn't. And I think starting thirty four years. And I think starting with why it doesn't, Well, I think I said or what what I meant to say is we are guaranteed to eventually do that if we don't destroy ourselves first, which is possible. should always end up creating digital intelligence, which is likely to be superior in many ways. Whether or not we ever create digital consciousness is is I think up for debate, but digital intelligence given enough time is for sure. but computers and AI turn out to be really good at a lot of things as well. And my my most optimistic hope for the future is that humans and AI are some sort of hybrid merged human and AI together. It's just sort of far more capable than than either on their own. Let's Well, I think the technical barriers are still huge and it's a mistake to say they're not. We will squiggle around the exponential curve of progress up and down, and there will be moments in a down that could last months or years where people are like, the AI winter is here. And, that loves to call, the top in markets, in research progress, and whatever. And, there's like a friend of mine that I used to think was this very brilliant predictor of recessions, and then I realized he has forecast like 18 of the last two recessions. And And at some point, people will be right, but people are so desperate to say now it's gonna stop working, and they have been for the last eight years and it's just been this relentless upward elevator of progress. What I can say with certainty is the things that we know work are gonna go a lot further. Algorithmic gains sort of keep going pretty well. And so I think there are things that will work better than we thought and worse than we thought, and we will hit some dark periods of stumbling blocks. But the biggest miracle of all is that we got an algorithm that can learn. with more compute. In my whole career, the central lesson has been that on scale, scale things up more than you think. And when everyone when people see a curve that's going like this and it stops here and they're asked to predict, you know, the next ten years of progress, my default assumption is to sort of believe that the curve keeps going for a while at least. And most people's default assumption seems to believe that it's gonna keep going on that same exponential for three more months and then perfectly flatline, I would say just more generally, generalized learning is exciting of many forms. You know, algorithms that can learn their own problems that can go off and explore, the ability to learn a lot about one task and apply it to another task, the ability to pre train these big models and then use them to, solve other problems with their knowledge of the world. human intelligence information and thoughts and apply them quickly to new problems. And But then you can learn a new thing very fast and then you can apply that new knowledge that you're told once or a few times to solve a new problem in like three seconds. And the fact that we're beginning to see that happen with AI, think, is quite remarkable. Maybe, but I think, you know, the sort of the ability for technology to lower costs and have more and more peep fewer and fewer people have more and more influence is remarkable. And I think if you think about the kind of big iron engineering projects of the past, Manhattan Project, the Apollo Project, something like that, those had to be done by nation states. So much money. And I think we actually can do it without we do need a lot of money but, like, not government scale. I would like to say there are a lot of things we would do with the Department of Defense. And I think the current mood through some parts of Silicon Valley, is like, we hate The US and we hate the US military even more, is just like an awful stance. And there's plenty of times, like, where if asked to help our country, we'd be proud to do so. I think, in general, we, The United States citizens, Western society, whatever you wanna call it, are better off if the United States government You know, you can sort of cover over deep seated cultural problems because everyone's excited by the growth. And in AI, it may be that because scale keeps working, we're not doing as much research on more efficient algorithms as we should. Returning I think when the system can start doing things like saying, you know, you asked question x. It seemed like you really meant y. Is that accurate? And being right most of the time, that to me will feel like a moment to start taking things really seriously. That was that I think that was what I someone asked me about the most important what I said there is, you know, studying that as a kid, would have had a very different answer and I would have picked, I would have tried to think really hard and pick the one specific invention that's had the most impact on the world since then. Studying it as an adult and maybe particularly given the career that I chose or used to choose, I think there was this one thing that enabled so much which was the British government decided that they would sort of grant second order sovereignty to companies and that they could have this legal thing where a bunch of people could be aligned and you could have kind of capital and people working for it, and there was sort of you got this new legal structure. Before that, you were basically limited towards small groups of people that could trust each other. And and then all of a sudden, had this that could glue a bunch of people and capital together and and it had this incentive structure where everyone wants the share price to go up, where everyone's incentives were in line. I really do believe that incentives are a superpower and if you can get incentives right or make incentives better, that's the thing you should work on. And so the British government invents this one single thing that enables this incredible boom in terms of not just coming up with inventions, I'm pretty happy with what we're doing there. I'm pretty happy with what we've been able to align. And we are trying to not think about how we get our applications a little bit better next year, but over sort of the long arc of history, what it takes to make machines that truly think. And we do all sorts of things along the way, but that's it. is the threat that advanced AI will pose to cybersecurity I think is likely to be huge. getting that, I think that's like a great problem to focus on. and I think that is, like, not a thing to be super concerned about. I think the number of logical qubits that it takes to do that are far enough off that we'll know when they're getting close. And I also think that we have plenty of time to transition to quantum resistant encryption. nature. And I think in any setting, 90 in the high nineties percentage of people are gonna be super memetic. And in my experience, it's difficult to stop, but it's okay because the other 2% of people drive the world forward. So most people will be sort of they don't invite me on VC panels anymore because I can't sort of, like, keep my facial expressions hidden. But every few years, there's, like, some buzzword. You know, we're gonna do this with social. We're gonna do this with podcasts. We're gonna do this with crypto. We're gonna do with AI. And and basically, I think by the time you get like, say, three buzzwords in the first two sentences of a startup pitch, can pretty safely ignore it. And even one, you should be like a little bit skeptical unless they're clearly doing it. So the number of startups say we're an AI driven x And and the lesson here is like startups pitch themselves however they think will work. VCs often fall for it. The good VCs dig in and don't fall for it. when we develop a technology that we think is probably safe to release but we think will eventually become unsafe as it scales up, we'd like the world to get a heads up. And I think the world had gotten used to over a period of time photoshopped images, and now people know not to trust them. But people do still trust text, press releases, news, whatever, for the most part. And, you know, like a new thing that I think will happen is we're not that far away from entirely fake videos of world leaders saying whatever you want. People tend to trust that too. And I think the world needs time was to say, there is a change. We're gonna get through it, but you need to think about this as a possibility, you being regular people who are reading the Internet, policymakers, whatever. And that was our goal with how we did that. My guess is that someday, far in the future, when world leaders give an address, they cryptographically sign it. And we just get used to that, you know, all videos can be faked I'm also happy to talk about sort of like AI more relevant to business today if people are interested. but I'm really happy it's happening. I just don't have any expert opinion to offer about how to apply AI there. Sorry. Something about the ability to learn new concepts based off of existing knowledge. The ability and maybe something about the ability to sort of learn them fairly quickly. We talk about the right metrics here, but So so right now I think it's pretty collaborative and researchers from around the world publish their work and work together quite openly. I'm nervous that is on a path to get much harder. I certainly think the optimal long term outcome for the world is close international collaboration and not sort of an arms race between nations. And I hope that will happen, but I'd say it's well out of the area where I feel like I'm an expert and could make a confident prediction. I think it's clearly in the best long term interest for the world, and one of our goals at OpenAI is to push policy to that direction. And I think one of the really great values of academia is sort of for all of for all of the flaws and faults of academia, think it has done better than any other segment at long term open international collaboration around ideas. One of the things that we've done is to start what we call, OpenAI Scholars, which is a way to get really talented people from more diverse backgrounds exposure. We mentor them, we teach them, and that helps at the rate of a handful of people per year, and that is kind of our own capacity, but that's clearly not enough to solve the problem. And I I do believe that the people who build these things, not through any intentional in AI, it's it's like incredibly striking. And it's incredibly striking in terms of being, non diverse. And so I think what has to happen is we need a lot more versions of OpenAI Scholars like programs, and people in the field need to sort of commit some of their time to mentoring, diverse people. And also we need to really figure out how to change the, ability to get new people into the field rather than wait for the pipeline of AI PhDs to catch up, is a many, many year process. I think if we don't do that, we will end up having, representation and advice. The people that build the system always have sort of a huge amount of, influence over what actually happens, again, not through any negative intentions, I used to really really worry about this. As we talked about earlier, I have shifted my own thoughts to thinking it's gonna be more about a compute edge than a data edge and I and I certainly hope that's true, because otherwise, governments like China have a huge edge in terms of more data than anybody else. I don't think the society we want is a super high surveillance and the trade off of that is just less less data. However, the Internet is giant and we forget just how giant the Internet is. Even in a world where we do need a lot of data,