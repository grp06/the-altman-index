Sam Altman: unlike most companies trying to build artificial intelligence, where people take basically deep learning and apply it to narrow ideas, and it works sometimes astonishingly well, usually at least pretty well, we are trying to build one intelligence that is smarter and more capable than humans in every way. We're trying to use this to solve the biggest problems facing the world. We're trying to use this I think we will enable And someday, I think we will sort of build the descendants of humanity and launch them off to colonize the universe. make it go really well for humanity, where the value we create, a large part of it gets shared with the world. This is just such a fundamental technology that we have to design a new corporate structure so that our investors can make a great return and our employees too, but also that we figure out a way that the whole world wins. I think this is different If we're successful in this quest, and it's very hard, but if we're successful, I think it will be the most significant technological transformation in human history. I think it will eclipse the agricultural revolution, the industrial revolution, the internet revolution all put together. And so we're trying to think about how we want that world to go. this incredible increase in computing power, and a few big breakthroughs in algorithms. Every year for the last six, the biggest neural network the industry can train has grown by a factor of 10. That'll continue for the next six at least, at which point we'll be near one human brain. I think in, let's say, ten years, we will have systems that sound today to be impossible. I think they'll be smarter than humans in every way. I think we will machines will actually feel to most people like they can think, subjectively. I think that we will be able to create I think there'll be a number of businesses started, hopefully some spun out of OpenAI, that are look like they're on a path to be bigger than any corporation that currently exists in the world. I think we will have that really sort of strikes me is when I see a very small child, like a one and a half or a two year old, even less sometimes, pick up a magazine and try to treat it like an iPad with touch gestures, because that's how they think the world's supposed to work. And I think that shows how quickly sort of we adapt to to a new world. And I think that ten years from now children that are born then will assume that all systems are actually intelligent. They will treat computers and humans very similarly. repetitive, non creative work that does not require a deep emotional connection like you want with in some scenarios. And in those cases, that my confidence interval, let's say out twenty years, We don't need any more technological breakthroughs for that. We just need system scaling. The work that we've already done at OpenAI, just as that sort of propagates out into the world and we figure out how we're gonna build business models around that and other people do too, that alone, I think, is enough to get there. I think the easiest layup right now in all of startup investing is to take narrow AI and apply it to every vertical, and just do what humans I always pay a lot of attention to, like, what the smartest, like, college freshmen are going to go spend their time learning. And they're all very good at AI. They're all very good at sort of, like, applying machine learning to existing problems. And so I do think that this is you can see this over the horizon. a general statement first. The the rate of technological job change is actually higher than most people into it. It's a little bit spiky, but it averages out over the last few centuries to every seventy five years, roughly 50% of the jobs turn over. And we as a society, although we're always anxious about it, always find a way around it. Now, there's like two cases with AI. Right? One is this is technology. It will eliminate existing jobs, we'll find new ones. And the other is this is a new life form, and it will do to us what the industrial revolution did to horses or whatever. And I'm sympathetic to both arguments, and I can't say anything with more certainty than I don't know, but I believe that human desire for status and feeling superior to each other seems to be endless. So I assume we will find new things to do, but they will look very different than work today. More different than work on one side or the other in the industrial revolution, I think. We can handle job changeover. The question is if it all comes in, like, ten or twenty years, can we handle that? That that has not been tested yet. Each major technological revolution has compressed in the time frame that it happens, but it's never compressed inside of one generation. And that feels like something new. If people are like then people actually, in one lifetime and one career, have to change what they do, not just society change. I would guess that some version of teachers are pretty safe, because there, there is something about the human connection. And until we get to real AGI, that one feels pretty good. One that I would say feels like pretty bad is cashiers. Yes. Look, when I was 18, I made this list of the five problems I wanted to sort of help contribute to in my life, and AI was at the top then, and it's been at the top for a while. But until more recently, I didn't believe that I actually was gonna be able to meaningfully solve it, and I had this sort of very great job at YC. I always try to I always want to work on the most highly leveraged thing I can do, where I feel like I contribute most to sort of innovation and sort of improving the world. And for a long time, I thought that was YC, which makes sense. Right? Because, like, we get to fund hundreds of companies a year, many of them go on to do great things, and we have a significant impact on the startup movement in general. But when I realized that I actually truly believe AGI is gonna happen in the not distant future, it was very difficult for me to be motivated to spend the majority of my time on anything else, because I think this is the platform through which we will solve all the problems that I care about. If you can truly invent AGI, it's the last thing you need to invent. think we will to everything else. We will use AGI hope to solve health care, solve climate, educate every kid on Earth. I think that I always tell people that they're always surprised by is something like 80% of the brain goes to processing sensory input and controlling the body. Only 20% is for thinking as we think of it. The hardware has had much longer in evolution. It's incredible. Incredible. people love to say the example that, like, art is the thing that stay that that will stay human. AI can't do that. We released recently released something called MuseNet. It took the same technology we used for our language model at OpenAI and and made it for music. And so we have this language model that's pretty impressive and getting more impressive every month that can do unsupervised language modeling. And someone said, well one person said, what if I do that for music? She put it together, trained it on a bunch of music and got incredible results. And we made it available for some period of time, and I heard from a number of people you could hear as many Rachmaninoff concertos as you ever wanted. You never run out. It was new every time. And I mean, we've seen this already with some online services where everyone gets a customized version. But I think that trend is just gonna keep going. meta comment first about how to think about intelligence. It's very hard to think about how much smarter we really are than other primates. It feels like a lot. Right? Like, it feels like we walked out of we're just like unbelievably more intelligent than our sort of nearest relatives, because we can at this point, we have this sort of intelligence outside of biology. This where where we kind of like have created society in this body of knowledge and this set of tools that every generation gets to build on. And it's this incredibly exponential curve, and we feel so smart. I suspect we will learn that the limits of intelligence, although I expect they exist somewhere because of the speed of light and a computer system, if nothing else, are very far. And and there will be like, we feel like much smarter than a chicken or something like that. But probably, relative to systems that we will build, sort of the the children of humanity that we will someday build, we are probably not very smart at all. And in the same way that that chicken has a hard time about, like, thinking about what we're capable of, it's probably, like, very difficult to explain to the chicken, like, the concept of, like, leaving Earth and going to the moon. I think it's very hard for us to sit here and talk about what the systems we build will be capable of. But it is my genuine belief that long after we sort of create incredible economic value and improve human lives, that these systems will someday become truly awake, biological evolution successfully bootloads digital intelligence. Digital intelligence leaves Earth on von Neumann probes, and sort of colonizes the universe until the heat death. So And I don't can't quite articulate why I should care about that so much, but it does make me happy to think much happier to think that the universe will sort of continue to observe itself rather than the kind of light of consciousness going out. I can make a reasoned argument. I can make a lot of cases against it. But I think it's a dumb debate. I think it is a very small minded short term debate. If we can accept that there's a 75% chance of getting to this most important moment in human history in the next hundred years, that should be enough for worldwide effort and focus on this. I believe it's much shorter. But whether you think it's ten years or twenty years, like, there's so much energy that goes into debating that. And I think if it's within a few decades, there's nothing more important in the world to work on. Well, not only that, I'd add like, let's say backpropagation We think my guess is that we need, like, one more 10 and about 10 more sevens, and algorithmically, might be it. We do need much more compute, Yeah. There's no new I mean, there's new physics. There's no miracles required there. The flip side of AGI doing far more than humans can People that are either really tired or, like, And so it is true that AI systems, you can trick them, and people love to talk about that. You can trick humans too. And I think a lot of the work that we are doing now, as we make more progress with unsupervised learning, for the first time, I think we're actually having systems get to some semblance conceptual understanding. And it is my hope that in the next few years, we will have a system that never makes this mistake as with a visual classifier. And that, I think, be a pretty So one year ago, I would have said the biggest the most important piece in front of us that was missing was unsupervised learning. And now, with our GPT two results from earlier this year, I believe we have something pretty important figured out there. We have longer to go, but the fact that we can train these models, the same model can generate a story, and then be state of the art in almost every text task without being specifically trained for it, it's the first time I felt like the machine is a little bit conscious. performance using that same model to solve all the other language tasks that it wasn't even trained for is surprising. A big thing in front of us now is reasoning. So can how can we teach a system to have some data and keep thinking? And the more it thinks, the better it does. How can we build a system that can prove unproven mathematical theorems? We're working a lot on that now. We're also interested in how can we rerun evolution. So how can we build these very large simulations and have agents with long memories and a lot of autonomy that have to interact with each other and develop sort of social intelligence? It's actually an interesting question why humans why why evolution endowed humans with such big brains. Incredible waste of energy. They make us, in our very early months and years, sort of easy prey for other animals, huge parental investment, and this ongoing tax of like 25% of all the food you eat just to run. We don't need those to outrun a lion, we don't need those to run down an antelope. We we have them to deal with other humans. And I think this idea that you generate intelligence by interaction with other agents is gonna turn out to be quite important. the beginnings of that, and big GaN has a bigger Yeah. We have some amazing results there, watching the simulation as agents sort of because you have this continually escalating, curriculum if you have to sort of deal with the other agents in an environment. So that's cool. a bad guy uses it to sort of conquer the world. where we sort of maximize human