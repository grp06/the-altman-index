Unknown: I'd love to start in we've had multiple conversations along this line. Imagine
Sam Altman: Evolution takes a long time. Biology is baked in very strong drives over a very long period. But in terms of what's like possible in the like tech stack and what one person can get done, that feels like it will be very different.
Unknown: there will be a faster demise of the Fortune 500 in the 2030s than we've ever seen. And which ones survive and which ones don't is going to depend on what they do, but a faster device. Do you agree? Do you disagree?
Sam Altman: time outside of OpenAI I spent with sort of software companies and I've thought that I have understood the sort of like physics of software companies for a while. But if we're heading towards a world where any software you want can be like just in time written and if you wanna do something, you can just like type something into an AI chatbot and get a great piece of software built. And so instead of going to buy this SaaS company's product or that one, just say, Run. And it happens. That that feels like a very significant change that is not that far away. would bet I would bet that most current companies fail to adapt quickly enough and get, you know, some significant sense is that the acceleration of growth in new companies and sort of taking share from the incumbents, I think that this is like one big exponential where new companies can get bigger and more important faster.
Unknown: In that time period, is there a profession in our audience are entrepreneurs that are building AI doctors, AI therapists, AI oncologists, AI structural engineers, AI chip designers, of course, software engineers, AI salespeople, AI marketing people, AI accountants. Like, that's all happening today. I'm presuming all that whether these companies are successful or some competitors successful will be mature. All their jobs, 2035 and beyond, in the intellectual world, and we'll come back to the physical world,
Sam Altman: very oriented towards caring about other people. I actually have wondered if we should make, a AI startup investor. I I
Unknown: We'll have time to look after elders. I think I think that part of the human relationship thing, I agree with. But services like teaching and medical care and others, I might disagree. I guess I was trying to think about what jobs can't be done, and then the question becomes in the shorter term, who then makes that happen?
Sam Altman: And that yeah. Like, of that will be replicated by relationships with AI, but I I think this whole thing is gonna be stranger and more complicated and more just like lot of different jobs, maybe almost all of almost all jobs, and yet
Unknown: we'll still have influencers in the attention economy, and we will still have caring for our children and elders and families. And we'll have much more time for all these things. So I agree with that. But let me move a little closer in. It was roughly three and a half years ago that the ChatGPT moment, as people call it, happened. So five years from then, if you look forward, which even eighteen months from now, the '26, how much of a change in AI capability from today to the '26, and I'm getting very short term now, versus what we saw between ChatGPT moment and today,
Sam Altman: progress on the merits will be astonishing, but
Unknown: quite grok the magnitude of the change the next 10 x from 10 to a 100. But I could be wrong. So, all this is happening because of scaling laws basically. How many different scaling laws? Talk to us about sort of the mechanism of this exponential change. Like, what's driving it?
Sam Altman: very much. We keep discovering better algorithms, so we keep finding steeper and steeper scaling laws. We keep figuring out how to build bigger computers more powerful chips and also connecting more of them together. We keep finding more data and better data. I think we'll start to get to a place where we put these systems together in new ways and we have kind of continuous learning where the systems just run forever and get smarter and smarter. But I mean, the really the story of the last few years has just been better algorithms, bigger computers, more data. And that I wish I had, like, a deeper and more insightful thing to say. You know, a lot of the algorithmic progress has been incredible. Certainly, the the reasoning stuff that we figured out and the original idea of unsupervised learning, I would put both of those in that category. And there have been some smaller ones too. But that's just kind of like you grind this out, it reminds me of any other
Unknown: So there are the scaling laws and multiple scaling laws depending upon if you ask Chad GPT, it defines multiple types of scaling laws. But there's another factor I think about, which is when do you think AI scientists start doing most of the AI research?
Sam Altman: you know, like let's say today that an OpenAI researcher is using codecs and it's generating 10% of their PRs and then 2030% and then it's and then it's like starting to actually go test some new model architectures itself but it's still like the researcher sort of directing it and then it's like But the researcher feels like they're just working more efficiently and the researcher might say, I'm doing 100% of the research. I just have better tools. But if that researcher is now outputting 2x or 10x what they could before, You know, let's say 10x, do you count the AIs doing 90% of their research at that point or 0% because it's not fully autonomously doing the whole loop?
Unknown: joint acceleration. So one doesn't have to measure it, but one can look at the rate of change of new of progress and acceleration. That's sort of to me the ultimate measure, what happens to the rate of change
Sam Altman: beyond just the scaling laws which we've just talked about. I think we will move much faster on the research front every year from here on because we have better tools. AI assisting humans or humans assisting AI or AI doing the research itself, the net effect will be much faster, not only algorithmic process, but the whole supply chain. If AI is helping us build data centers faster, if it's helping us develop chips, including pretty exotic stuff much faster, think that all counts. And so, if you look at it as sort of a rate of progress and as long as the AI is somehow helping that go faster, you count that as the AI doing part of the research,
Unknown: then I think much, much faster. To me, the key test is it's AI coming up with new hypotheses that it tests by itself and then modifies those hypotheses. Like, that's a cycle that seems vicious in a good way.
Sam Altman: I I I have a slightly different perspective. I I think what we care about is just how quickly we get much better research done. And if the AI is coming up with the hypothesis itself or enabling a human to come up with a hypothesis that human just couldn't do on its own,
Unknown: There's a question many LPs here ask me, which is, will the people who are winning reinforce their lead through this acceleration? And I if if this hypothesis of acceleration in research is right, then then you and some of the other leaders get to perpetuate their lead. It's harder for new players to play from scratch unless they come up with completely orthogonal approaches. And as you know, we are working on some orthogonal approaches. But does that perpetuate, give OpenAI an advantage in having an increasing valuation, which we all hope for? I, for sure, hope for that.
Sam Altman: I would be spending 0% of my time trying to figure out how to invest in another AI research lab and a 100% of my time figuring out how to invest in the thing that comes next. The the thing that I mean, GPs are also horrible at this, but, you're not. You're pretty good. But but in general, so much of the kind of mind share of investment capital in the world goes to chasing the last set of winners. And you almost never make money. You almost never make a lot of money there. a very unproven thing where you have some sort of differential insight. So right now everybody wants to invest in the next OpenAI and probably the next multi trillion dollar company will not be another AGI research lab. It will probably be the thing that got built because AGI now existed as a new technology in the same way that when OpenAI was starting, what most people wanted to invest in was like,
Unknown: When we invest in OpenAI, it's the only time in twenty years I sent an apology letter for investing to all our LPs
Sam Altman: Anyway, I I I think it's like I think it's a tremendously exciting time because the the amount of new space that's been opened up is greater than any time I've ever seen before. When OpenAI was started, our space had been opened up by a research breakthrough like the crypto market subsidizing GPUs and a lot of new progress there, I I I remember that time period. The list of things that I thought were worth working on was was very, very short. The list of things I think is worth working on right now is tremendous, And I think that's what people should be going off to fund. Whether the return whether the research returns continue to accrue to OpenAI or someone else, I think almost as a capital allocator at this point very large and successful consumer product and a handful of other things. But the there were a few companies that were true transistor companies, and then most of them went away and a few survived. And now kinda look at a number of things in this room that have transistors in them. We don't talk about this as a transistor device or a transistor company. It just became like a piece of technology that enabled a whole set of new companies, including OpenAI. And is what to go for. And and again, tremendously exciting time to be a capital allocator and you should chase the future, not the thing that worked in the past.
Unknown: So let me go back to the launch of ChatGPT. What surprised you the most? And then
Sam Altman: so the the tradition in tech companies is you start a product company, you build up a successful product company, and then decide to bolt a research lab onto it. This has worked out sometimes quite well like Xerox PARC, sometimes quite badly like some of the more modern companies, but we are the only example I know of where we started with a really well run research lab and then bolted on a badly run company We hadn't started thinking we were ever going make products but eventually it became clear that because of the scaling laws and the amount of capital we needed, So you need to have a product to have a company. And we had this model called GPT three and we were trying I, like, was turning up the urgency on the company to figure out a product and we just couldn't. It wasn't good enough. It was cool, but it wasn't good enough to make something that worked. And I remember that Paul Graham had this advice that always stuck with me which is you should always make an API. No matter what, you should make an API. It's just there's like good stuff will happen. So out of ideas to make a product, we said, well, let's, like, crowdsource this to the world and we'll put GPT three into an API The whole world figured out exactly one thing to do with it. The only thing that made money with GPT-three was these copywriting applications. I actually forget the company's names now, there were a few companies that very quickly became billion plus valuation companies that were just using GPT-three to generate copy for websites to resell. And that was it. People tried other stuff but no other companies worked. However, Some people, not a lot, but some people would just chat with that thing all day. It was before we had figured out RLHF and before we had even GPT 3.5, so it wasn't very good, but there was clear user signal that people wanted to talk to the models. So we started doing some research to make the models easier to chat with and you didn't have to do these complicated prompts and we had a better model and then we decided we would put out a chat interface. We were planning to have to build some other stuff. There was like a big concern inside of OpenAI which is if you can chat, does it have to be a specific kind of assistant trying to help you learn something or trying to help you accomplish some goal? Because if you're just chatting about nothing, are people gonna wanna do that or is that gonna just feel like you don't know what to say? And and so we almost held it, but we put out this preview where you could just chat about whatever. stuck with it. Retention was atrocious. But for the users that did retain, their usage increased over time. And again, we almost didn't launch it because of that. And an important learning that I've later reflected on is if you have a product that has any retention at all, you're actually in really good shape. If it's 5%, that can be totally fine. The default is almost always all the way down straight line to zero. But I quite understand that as intuitively at the time. a small suite of products and a platform that you can use with any other service to be sort of your default personal AGI. This is a system that will get to know you, will be connected to your stuff, that will behave the way you want. And if you wanna use it as a through a chat interface, fine. If you wanna go use social products or entertainment products in a new way, great. If you want to get a lot of work done with agents and a bunch of other stuff, also great. And if you want to be able to just log into any other service and sort of bring your intelligence there, we'll have that too. Over time that'll expand to new kinds of services. I think there's like a really important new kinds of computers to build and we want our users to have those too. But, you know, I think people are going to have a very important relationship with an AI that will help them be more productive, better, happier, whatever, smoother life and we'd like to be that. creating great experiences and integrating that in different ways. And
Unknown: so if you look at the next billion users for ChatGPT, is it the same use case? Is it expanded in some way? Is it lots of different segments? What do you imagine?
Sam Altman: are still at the terminal stage of AI. And I actually happen to like terminals. I think that was a nice way to use computers. But as we go build the rest of the modern computer interface around this, which will not literally be what computers look like but that kind of equivalent step forward, I think it'll just be easier to use and much more capable and approachable for most people.
Unknown: personal use is enterprise use. Speak to that a little bit.
Sam Altman: this idea of virtual coworkers and the ability to use AI to just make all of the things an enterprise has to do that right now it's limited on better. That one's awesome. And you can already see some examples of this. The the coding agent stuff is one where you can see the obvious application to enterprises. What's happening in customer support with AI agents is like pretty great. And I think at most enterprises the proportion of work, the proportion of tasks done by AI versus done by people is just gonna go up and up and up. And that seems obvious and, you know, well on its way. The one that I the thing I'm more excited about is A scientific case of scientific discovery is maybe the most exciting one, but there's a lot of other, like, really hard valuable problems that enterprises have that if they could just solve it, which they can't right now, would enable something new. And as we have AI systems that can go think super hard about a problem, I think that is fundamentally gonna be like a in the domain of enterprises you know, discover new material, figure out how to optimize
Unknown: If you look at the shorter term in the enterprise context, we've talked about AI scientists outside of AI scientists for AI, AI scientists, material science or biology or other areas. And I think you've talked about that layer of capability coming at some point in all these models.
Sam Altman: AI software engineer will be the most disruptive thing to enterprises. It it like, the companies are investing the most in that the fastest. There's a bunch of, like, quirky reasons why it's a particularly great environment. It so directly translates to current limiting reagent for most companies and a revenue opportunity. So I I would bet that that will be kind of like the big story for the rest of the year. There will be other things too, but like you will just start to see companies who get good at this or teams that get good at this significantly able to outperform other ones.
Unknown: me switch to a different topic. You've gone through a lot of decisions at OpenAI in extreme uncertainty around will this technology work? How will it develop? How what will competitors do? What are some of the hardest things you've decisions you've made? And some that turned out right and others that turned out wrong.
Sam Altman: And I haven't ever found really good advice on this So we're sort of struggling through it. I think if we could Silicon Valley wisdom for how to make many decisions quickly. I think there is good advice about how to make a few decisions or a few big decisions. And that's kind of the thing that I think would would like most help if we could do that. We have asked chattypiti. Didn't help. It
Unknown: The best recent use I heard of chattypty is somebody said they're a much better executive coach than any executive coach they've used.
Sam Altman: The general case of it being really good at therapy coaching has surprised a lot of people. figuring out how to do something new or deciding to do something new. Like, if a a thing that we pride ourselves at OpenAI on is that we we try to do new things instead of copying competitors or other research labs. So if you look at other research labs, they mostly try to do the same research OpenAI is doing, and they mostly try to, like, exactly clone our products. Like, they look shockingly the same across the industry. And it's really hard to, like, make the decisions that are that go into doing something new. And if you know something else works, it's so tempting. I now so understand it to just, like, copy that and not to make these decisions. I don't I don't think it's like how the world gets pushed forward, but I really do empathize now with why people do it.
Unknown: So so there's the research labs and there's a lot of copying each other and building on each other's ideas. A lot of the entrepreneurs on what the models will have and how to make decisions in extreme uncertainty. And it may not be the quantity of decisions, but they're really critical strategic decisions. Any advice for them on how to handle this uncertainty? Because that's a question I get probably more than any other question. Where will AI capability go and what assumptions should I make and where how do I build my start up on top of the current models?
Sam Altman: I mean, the thing that's worked historically is just assume that the models are gonna get better in every way, every dimension. Better, cheaper, like more multimodality, better intelligence, better reasoning capability, lower cost. And I don't see any reason to believe that's gonna slow down in the next few years. I think there will be some temporary slowdowns where people rewrite And just as just like ask yourself the question of, okay, an oracle tells me that this model is gonna get roughly 10 x better every year on roughly every dimension. What product should I build when? And I'm super impressed by the people building AI applications at this point. I think they understand the models and exactly what they can do and what they can't and how to get the most performance out of them. I don't have like a wish list of what else I'd like them to understand. like, 20 of the most impressive, like, new generation AI founders I I had heard about or met. And, like, I was just going around the table, I was like, man, these are, like, such better founders than the world had five or ten years ago. Like, I don't really have a wish list here. I'm super impressed. I
Unknown: wanna talk about some of the global implications of AI but before I go there, I wanna ask you my favorite question and I wanna leave enough time for audience questions too.
Sam Altman: I would bet the company
Unknown: I've been speculating it probably already has started. It's pretty amazing thought to think about
Sam Altman: that could be generated, that could be discovered and taken through trials by one person plus,
Unknown: you know, 50,000 GPUs or something. Yeah. I just saw an article. Somebody sent me an article this morning in my email which I haven't had time to read. Said that using AI only discovered a cure for macular degeneration. So your point is right. But the same could happen in entertainment and Let's go to sort of the global rollout and how do we ensure benefits of AI spread more globally, more equally. So there's there's sort of this worry of the rich get richer because then the inside circle, the situational awareness paper talks about there's only a few 100 people who understand how large this change is and how fast it's happening. But talk about the global implications and how we ensure equity and broader usage and spread of the benefits. First, within even this country because there's an asymmetry to it, and and then globally.
Sam Altman: I resent the question, but I don't wanna be all negative on it because I think there is something important to it. But, you know, ChatGPT is, I think, the fifth biggest website in the world now. It is if it stays on its current trajectory, which is hard to do, it will be the biggest website in the world. There will be billions of people that are using free AGI at some point. Everyone will have access to great medical advice. Everyone will have access to great education. Everybody will have the ability to, like, ask for any software to be made and for free. We will just do it for you. And that is, like, that is how technology works. And I think capitalism is like really great. It does not mean we don't have to correct around the edges. I think we do. I think there's important stuff that that we will will need to address differently. But but on the whole, the way this benefits the world is the technology benefits the world. You put tools in the hands of people, you make them free or low cost, and people do amazing things. And and I think, like, I think there are a lot of people who think that, oh, you know, the world is not ready for this. They can't handle it. There's only a few people that can understand it. There that paper, there's many others. But I think people really kinda do know what they need and they're pretty good about learning how to use new technology. And it's and again, this is not, like, theoretical. This is, like, at mass scale happening today. And I think it's quite wonderful. And it's happening because, like, the existing systems and incentives in the world are phenomenal in many ways. If AI starts making massive scientific discoveries, you know, some company will use AI to discover cures for every cancer and I hope those people get really rich but also everybody else in the world, I hope they get a cheap cure for cancer. Someone use AI to make fusion commercializable. Hope those people get really rich but the whole world will benefit from plummeting electricity prices. There will be some things that I think are different about AI. You can imagine a world where compute becomes a crazy scarce resource and deciding, somehow democratically deciding what problems we go after in what order with massive amounts of compute needs a little bit of a different approach than we've had in the past. You can imagine access to compute for, you know you could imagine like most capital in the world decides it wants to like have more equality in the world, not further divergence.
Unknown: In what I expect AI will cause is a hugely deflationary economy in the twenty thirties, and the risk of sentient AI. You pick which ones or all of them you wanna address.
Sam Altman: I I think it should be hugely I hope it's deflationary. I think it should be. But all of the excess wealth that's gonna get created will need to go somewhere. And I hope that things like water and food and health care and education and, you know, access to nature and time with family, whatever else, I hope that becomes something that, like, everyone can afford. And then that we all find the silliest status games to play and go bid up, you know, da Vinci paintings to a trillion dollars or whatever or galaxies to a quadrillion dollars to make it so that people who, like, wanna play status games and wanna create value for other people and wanna have something for But but I think it is an interesting question about, like, if we want everything to deflate wildly, if we want everything to get super cheap and people are, you know, wildly ambitious and creative and, like, being used to the terrorists and gonna work hard and we're gonna have some way to keep track of that, where is the excess wealth gonna go?
Unknown: believer in how deflationary the economy will be. And if it's a good measure of GDP, which I don't think it will be, how we measure GDP growth, which I think will explode. So you'll see these two things happening at the same time.
Sam Altman: I'm not sure I understand the difference. Like, I think it it's gonna enable someone and disadvantage somebody else. And but I'm not sure how to act differently. Like, yeah, all of your competitors and new ones are gonna try to, like, outgun you with AI and and also there's so much demand in the world that we will need many gigawatts, tens of gigawatts, eventually, probably hundreds of gigawatts of energy to run the world's sort of AI demand even with super efficient models. I'm like very quite optimistic about new energy sources coming online and I think it'll be abundant and cheap and I think AI and energy are obviously tremendously related things. Energy is valuable for other reasons too. You want the ability to go make stuff happen in the world, but the degree to which ten years from now I expect There will need to be some guardrails here. Over regulation, I think I we can all see how that could be a disaster, but under regulation for this could be pretty bad in its own way. So setting some global rules, especially as the systems get much more powerful, think that can only come from governments. I think the companies will propose it and figure out what it should be, but I think it can only be enforced I mean there will be some agent that's job is like to output to like evaluate the outputs of other agents and decide what to surface to you when and when it can make a decision or you know understand the decision you would make. So that'll be another kind of job for a different kind of agent and I bet it'll be pretty effective. There is probably like you know you'll have like a custom generated UI for you that'll be evolving in real time based off the output of those agents and it'll do a good job of figuring out how to get your attention when. But the executive function there. I think there are some areas of science where with no more data, a smarter intelligence can just make new breakthroughs. I think it's possible in physics, for example, that without a single additional data point, without ever building another particle accelerator, AI could just solve physics. I don't think it's super likely, but maybe all the data we need is there and we just have not been we've not had the ability to solve a hard enough integral or whatever. And then there are areas like biology where I'd say it feels way less likely to me that without ever running a single additional wet lab experiment we could cure every cancer. Not impossible, but unlikely. new systems that are it doesn't have to be an automated wet lab. It's fine if it just sends an email to a scientist saying, Can you run this experiment for me and tell me the answer? But somehow like active learning systems that can keep getting new data points. But what problem will fit into what category and exactly where the limiters will be?