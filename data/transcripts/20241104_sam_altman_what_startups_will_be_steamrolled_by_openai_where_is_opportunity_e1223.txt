Sam Altman: We are gonna try our hardest and believe we will succeed at making our models better and better and better. And if you are building a business that patches some current small shortcomings,
Unknown: we have many, many questions from the audience, and so I wanted to start with one. When we look forward, is the future of OpenAI
Sam Altman: I hope reasoning will unlock a lot of the things that we've been waiting years to do. And the the ability for models like this to, for example, contribute to new science, help write a lot more very difficult code, But eventually, I think we can offer really high quality no code tools, and already there's some out there that make sense. But you can't you can't sort of in a no code way say I have like a full startup I wanna build. And if you are building a business that patches some current small shortcomings, If on the other hand, you build a company that benefits from the model getting better and better, if, you you know, an oracle told you today that o four was gonna be just absolutely incredible and do all of these things that right now feel impossible and you were happy about that, And that is the general philosophical message we try to get out to startups. Like, we we believe that we are on a pretty, a quite steep trajectory of improvement and that the current shortcomings of the models today will just be taken care of by future generations. And,
Unknown: If you're thinking as a founder today building, where is OpenAI gonna potentially come and steamroll versus where they're not? Also for me as an investor, trying to invest in opportunities that aren't going to get damaged.
Sam Altman: market cap that gets created, new market cap that gets created by using AI to build products and services that were either impossible or quite impractical before. And there is this one set of areas where we're gonna try to fight so hard to get them to do what you wanna do. But all of this other stuff, One of the surprises to me early on was and this is no longer the case, but in like the GPT three point five days, it felt like 95% of startups, something like that, wanted to bet against the models getting way better. like the great AI tutor or the great AI medical advisor or whatever. And so I felt like 95% of people that were were like betting against the models getting better, 5% of the people were betting for the models getting better. I think that's now reversed.
Unknown: have hurt us on what we intend to do. So it's it no longer seems to be such an issue, but it was something we used to fret about a lot because we kind of we saw what was gonna happen to all of these very hardworking people. You you said about the trillions of dollars of value to be created there, and then I promise we will return to these brilliant questions. I'm sure you saw I'm not sure if you saw, but Massa sit on stage and say we will have I'm not gonna do an accent because my accents are terrible. But there'll be $9,000,000,000,000 of value created every single year, which will offset the $9,000,000,000,000 CapEx that he thought would be needed. I'm just intrigued. How did you think about that when you saw that? How do you reflect on that? I can't put it down to, like,
Sam Altman: mega technological revolution of which this is clearly one. But, you know, like next year will be a big push for us into these next generation systems. You talked about when there could be like a no code software agent. I don't know how long that's gonna take, but if we use that as an example and imagine forward towards it, think about what think about how much economic value gets unlocked for the world if anybody can just describe like a whole company's worth of software that they want. This is a ways away, obviously. But when we get there and have it happen, think about how difficult and how expensive that is now, think about how much value it creates if you keep the same amount of value but make it wildly more accessible and less expensive. That that's really powerful. And I think we'll see many other examples like that. We I mentioned earlier like healthcare and education, but those are two that are both you know, I don't smarter people than me it takes to figure that out, but but the value creation does seem just unbelievable really good open source models that now exist. I think there's also a place for, like, nicely offered, well integrated services and APIs, and, you know, I think it's and provide minimal supervision When people talk about an AI agent acting on their behalf, the the main examples they seem to give fairly consistently And either it can, like, use OpenTable or it can, like, call the restaurant. Okay. Sure. That's that's like a mildly annoying thing to have to do and it maybe, like, saves you some work. you can just do things that you wouldn't or couldn't do as a human. So what if what if instead of calling one restaurant to make a reservation, my agent would call me, like, 300 and figure out which one had the best food for me or some special thing available or whatever. And then you would say, well, that's like really annoying if your agent is calling 300 restaurants. The category I think that was more interesting is not the one that people normally talk about where you have this thing calling restaurants for you, but something that's more like a really where you can, like, collaborate on a project with and the agent can go do, like, a two day task And it's not like you're not like paying per seat or even per agent, but you're like it's priced based off the amount of compute that's like working on a
Unknown: How do you respond and think about that? And when you think about the increasing capital intensity to train models, are we actually seeing the reversion of that where it requires so much money that actually very few people can do it? It's definitely true that they're depreciating assets.
Sam Altman: This thing that they're not though worth as much as they cost to train, that seems totally wrong. there's a positive compounding effect as you learn to train these models, you get better at training the next one. But the actual, like, revenue we can make from a model, I think, justifies the investment. To be fair, I don't think that's true for everyone. And there's a lot of there are probably too many people training very similar models. And if you're a little behind or if you don't have a product with the sort of normal rules of business that make that product sticky and valuable, Reasoning is our current most important area of focus. I think this is what unlocks the next, like, massive leap forward in in value created. So that's we'll improve them in lots of ways. We will do multimodal work. We will do other features in the models that we think are super important to people, like, when they're babies and toddlers before they're good at language can still do quite complex visual reasoning. So clearly, this is possible. What is really hard and the thing that I'm most proud of about our culture is the repeated ability to go off and do something new and totally no. I'm not talking about AI research, just generally. A lot of organizations talk about the ability to do this. There are very few that do across any field. And in some sense, I think this is one of the most important inputs to human progress. So one of the like retirement things I fantasize about doing is writing a book of everything I've learned about how to build an organization and a culture that does this thing, not the organization that just copies what everybody else has done. Because I think this is something that the world could have a lot more of. It's limited by human talent, but there's a huge amount of wasted human talent because this is not an organization style, a culture, whatever you wanna call it that we are all good at building. So I'd love way more of that and that is I think the thing most special about us. Sam, how is human talent wasted? Oh, there's just a lot of really talented people in the world that are not working to their full potential because they work at a bad company or they live in a country that doesn't support any good companies or a long list of other things. I mean, the one of the things I'm most excited about with AI is I hope it'll get us much better than we are now at helping get everyone to their max potential, which we are nowhere nowhere near. There's a lot of people in the world that I'm sure would be phenomenal
Unknown: unbelievable hyper growth. You say about writing a book there in retirement. If you reflect back on the ten years of leadership change that you've undergone, how have you changed your leadership most significantly?
Sam Altman: is just the rate at which things have changed. At a normal company, Having to do that so hard it is or how much active work it takes to get the company to focus not on how you grow the next 10%, but the next 10 x. In growing the next 10%, it's the same things that worked before will work again. But to go from a company doing say like a billion to $10,000,000,000 in revenue requires a whole lot of change and it is not the sort of like let's do last week what we did this week mindset. And in a world where people don't get time to even get caught up on the basics because growth is just so rapid, I I badly underappreciated the amount of work it took to be able to like keep charging at the next big step forward while still not neglecting everything else that we have to do. There's a big piece of internal communication around that and how you sort of share information, how you build the structures to like get the company to get good at thinking about 10 x more stuff or bigger stuff or more complex stuff every eight months, twelve months, whatever. So I I think there was either no playbook for this or someone had a secret playbook they didn't give me,
Unknown: Raboy did a talk and he said about you should hire incredibly young people 30, and that advice, you build great companies by building incredibly young, hungry, ambitious people who are 30, and that is the mechanism.
Sam Altman: classes of people. Like, we have they bring amazing fresh perspective energy, whatever else. On the other hand, when you're, like, designing and I think what you really want is just like an extremely high talent bar of people at any age and younger people or I'm only gonna hire older people, I believe would be misguided. I think it's like somehow just not it's not quite the framing that resonates with me, but the part of it that does about y combinator four is inexperience does not inherently mean not valuable.
Unknown: for coding tasks.
Sam Altman: and something about the way that we currently talk about it or think about it feels wrong. May maybe if I had to describe it, we will shift from talking about models to talking about systems, Without going into detail about how it's going to happen, the the the core of the question that you're getting at is, is the trajectory of model capability improvement going to keep going like it has been going? caused us a lot of consternation that we really didn't know how to solve. We figured it out, but there was there was definitely a time period where we just didn't know how we were gonna do that model. And then in this shift to o one and the idea of reasoning models, that was something we had been excited about for a long time. But bet on this next product or that next product? Or are we gonna like build our next computer this way or that way? They they are kind of like really high stakes one way door ish that like everybody else I probably delay for too long. But but mostly the hard part is every day it feels like there are a few new fifty one forty nine decisions that come up that kinda make it to me because they were fifty one forty nine in the first place and then I don't feel like particularly likely I can do better than come to believe has good instincts and good context in a particular way. It's probably not it's well, I guess I could quantify it this way. It is not my top worry, I think it's all gonna work out fine, but it feels like a very complex system. Now this kind of like works fractally at every level, so you can say that's also true like inside of OpenAI itself. power availability with the right networking decisions, with being able to like get enough chips in time and whatever risk there's going to be there, with the ability to have the research ready to intersect that so you don't either like be caught totally flat footed or have a system that you can't utilize, with the right product that is going to use that research to be able to like pay the eye watering cost of that system. So it's supply chain makes it sign sound too much like a pipeline, but but yeah, the overall ecosystem complexity at every level of like the fractal scan which is everybody likes to use previous examples of a technology revolution to talk about to put a new one into more familiar context. And a, I think that's a bad habit on the whole, and but I understand why people do it. And b, I think the ones people pick for analogizing to AI are particularly bad. So the Internet was obviously quite different than AI and you brought up this one thing about cost and whether it costs like billion or a 100,000,000,000 or whatever to be competitive, it was actually really easy to get started. Now, another thing that cuts more towards the internet is mostly for many companies, this will just be like a continuation of the internet. It's just like someone else makes these AI models and you get to use them to build all sorts of great stuff and it's like a new primitive for building technology. But if you're trying to build the AI itself, that's pretty different. Another example people use is electricity, which I think doesn't make sense for a ton of reasons. The one I like the most caveated by my earlier comment that I don't think people should be doing this or trying to like use these analogies too seriously is the transistor. It was a new discovery of physics. It had incredible scaling properties. It seeped everywhere pretty quickly. You know, we had things like Moore's Law in a way that we could now imagine like a bunch of laws for AI that tell us something about how quickly it's going to get better. And everyone kind of bent like the whole tech industry kind of benefited from it. And there's a lot of transistors involved in the products and delivery of services that you use, It's there's a very complex, very expensive industrial process around it with a massive supply chain, and incredible progress based off of this very simple discovery of physics led to this gigantic uplift of the whole economy for a long time, even though most of the time you need to think about it. And you don't say, oh, this is a transistor product. It's just like, oh, alright. This thing can, like, process information for me. Doesn't have to, like, literally be infinite context, but some way that you can have an AI agent that, like, knows everything there is to know about you, has access to all of your data, the whole field and incredibly talented, incredibly hardworking people. I don't mean this to be a question, Dodge. It's like I can point to I thought about, like, a bunch of researchers I could name. But in terms of using AI to deliver a really magical experience that creates a lot of value in a way that people just didn't quite manage to put the pieces together, I think that's it's really quite remarkable. And I specifically In this context, latency is what you want. If you but if you were like, hey, Sam, I want you to go, like, make a new important discovery in physics, you'd probably be happy to wait a couple of years. And the answer is it should be user controllable. The thing I'm struggling with most this week is I feel more uncertain than I have in the past about what our like, the details of what our product strategy should be. I think that product is a weakness of mine in general, and it's something that right now the company, like, needs stronger and clearer vision on from me. Like, we have a wonderful head of product and a great product team, but it's an area that I wish Focus, what we're gonna say no to, like, really trying to speak on behalf of the user about why we would do something or not do something, like, really trying to be rigorous about not not having, like, fantastical dreams. have an unbelievably rapid rate of improvement in technology itself. You know, people are like, man, the AGI moment came and went, whatever. The like the the pace of progress is like totally crazy and we're discovering all this new stuff both about AI research and also about all of the rest of science. And that feels if computers were gonna pass the Turing test, they would say no. And then if you said, well, what if an oracle told you it was going to? They would say, well, it would somehow be like just this crazy breathtaking change for society. And we did kinda satisfy the Turing test roughly speaking, of course, and society didn't change that much. It just sort of went whooshing by. And that's kind of example of what I expect to keep happening, which is progress scientific progress keeps going, outperforming all expectations,
Unknown: amazing. I had this list of questions. I I didn't really stick to them. Thank you for putting up with my meandering around different questions.