Unknown: And I should say that the questions I'm asking were It's a common icebreaker or.
Sam Altman: the point is it's not zero. And I think branches of it. We would just sort of have the doom come to play. But I think society always I think ten years ago, I probably had a more naive conception of AI as this creature that was going to be off doing stuff or this magic super intelligence in the sky that would figure And now I think of it much more like any other technological revolution, hopefully the biggest and the best and the most important and the greatest benefits. But we have a new tool in the tech tree of humanity, and people are using it to create amazing things. I think it will continue to get way more capable and way more autonomous over time. But even then, like, I think it's just going to integrate into society in an important and transformative way, but something that is somehow going to be I think we like, if AGI got built tomorrow, and you asked me what would happen the next day, ten years ago, I would have said, can't really imagine it. It should be just this absolute transformation, singularity, everything is different all at once. If we make something that is as smart as all of the super smart students here, that's a great accomplishment in some sense. But
Unknown: So just moving stepping back a little bit and looking at the current models, thinking about AI systems like ChatGPT, what do you think is necessary to remove bias
Sam Altman: I think we've made surprisingly good progress about how we can align a system to behave according to a certain set of values. I think for as much as people love to talk about this and say that, oh, you can't use these things because they're just like spewing toxic waste all the time, like if you use GPT-four, it behaves kind of the way you want it to and reasonably well. And we're able to get it to follow not perfectly well, but better than I at least thought was going to be possible by this point, what bias means and what values mean? How decide do what the system is supposed to do? How much does society define broad bounds around the edges versus how much do we say, you as a user, like we trust you to use the tool. Not everybody will use it in a way we like, but that's kind of the case of tools. I think it's important to give people a lot of control over how they use these tools. And even if that means that they may use them in ways that you or I don't always like. But there are some things that a system just shouldn't do, we'll have to kind of collectively negotiate what those are. I mean, it's interesting thinking about whether
Unknown: bias against certain demographic groups, for instance. They're actually trained on the way our human doctors are behaving, right?
Sam Altman: They are, but then we do this RLHF step where we can exert quite a lot of influence. Humans are clearly very biased creatures and often unaware of it. And I don't think that GPT-four or five shares our same psychological
Unknown: have sort of been part of the public consciousness in terms of concerns, etcetera, are privacy issues, which loom large for a lot of people when considering, the future of LLMs. So, you know, how do we navigate the balance between personal privacy and the need for shared data to train AI models?
Sam Altman: can imagine this future in which, if you want, you have a personalized AI that that knows, has read every email, every text, every message you've ever sent or received, has an access to a full recording of your life, knows every document you've ever looked at, every TV show you've ever read, everything you've ever said or heard or seen, like all of your bits of input in and out. You can also imagine the privacy concerns that that would present. And I think if we stick on that frame and not say, well, should AI be able to train on this data or to that data, but how are we going to navigate the privacy versus utility versus safety trade offs or security trade offs that come with that? And like, what does it even mean? Like, do we need a new definition of privileged information so that your AI companion never has to testify against you or can't be subpoenaed by a court? I don't even know what the But this question of where we all will individually set the privacy versus utility trade offs and the advantages that will be possible for someone to have if you say, I am going let this thing train on my entire life, That's like a new thing for society to navigate. I don't know what the answers will be. I don't know where most people will make the trade offs. I don't know what we'll say is even permissible in the bounds. But we faced a little bit of that before about where we trade off some privacy for utility with the services we all use. But that can go so incredibly far with AI. There are all these things that we've had to negotiate with the internet, things about how we think about privacy, how we think about online ads, that when you intersect them with AI become much higher stakes and much bigger trade offs that I think we're going to start really facing. Yeah, no, that's interesting in terms of
Unknown: also how much individual control there's exerted. In other words, when you're talking about aggregated data a good example, when you talk about higher stakes again, is health record data, etcetera, and how much we can build into it some of personal ability to sort of set that sliding scale
Sam Altman: you want out of GPT-five or six or whatever is for it to be the best reasoning engine possible. It is true that right now, the only way we currently know how to do that is by training on tons and tons of data. And in the process of that, it is learning something about how to do very, very limited reasoning or cognition or whatever you want to call it. But the fact that it can memorize data or the fact that it's storing data at all in its parameter space, I think we'll look back and say, that was kind of like a weird waste of resources. Like, it is true that GPT-four can kind of act like a database, but barely. It's slow. It's expensive. It doesn't work very well. It's not really what you want. It's just kind of as a side effect of the only way we know how to make a model that a reasoning engine right now, it has all these other properties. But I assume at some point, we'll figure out how to separate the reasoning engine ness from all of the make a great free AI tool available. We don't run ads. We just do this as like a public good because we think it's important to put the tool in people's hands. And we want it to be very widely available, very easy to use, very helpful. I actually think that says something legitimately great about human expectation and striving and why we all have to continue to make things better. And I think it's great that a baby born today will never know a world in which the products and services they use are not intelligent. We'll never know a world in which cognition is not abundant and part of everything that you use. So I think this human discontent with the state of things and the expectation that the world should get better every year, I think that's awesome.
Unknown: new electricity, speaking of less exciting edge of things, the new electricity demand from AI and data centers has been cited as an environmental concern. At the same time, many, you know, talk about AI assisting and decarbonization. What are your thoughts about this, the tension between its effect on climate climate, its ability to potentially help us, fight the impact of climate change, as it moves forward?
Sam Altman: I'll answer that specifically in a more general observation. It is true that AI needs a huge amount of energy, but not huge relative to what the rest of the world needs. If we have to spend and I don't even think if we spent 1% of the world's electricity training powerful AI, and that AI helped us figure out how to get to non carbon based energy or do carbon capture better, that would be a massive win. And even if we didn't do that, if that 1% of compute that we spent on AI let people live their lives better and have to like I read this thing about the compute Google used once compared to the amount of carbon that people used to spend driving in their cars, places to get information. And you have people saying Google's so horrible, we should shut it down. It's like spending all this energy. And it's intellectually a very dishonest thing to say, because it was a net savings in energy, I think, pretty clearly. The internet in general and what it lets us do for telecommitting, probably also a savings. So I think it is important to address this issue, but we will in all of these fantastic ways. But I think this points to something else, which is the you opened asking about pDoom and the level of doomerism in society right now, I think the way we are teaching our young people that the world is totally screwed, that it's hopeless to try to solve problems, that all we can do is like sit in our bedrooms in the dark and think about how awful we are, is a really deeply unproductive streak. And I hope MIT is different than a lot of other college campuses. I assume it is, but you all need to like make it part of your life mission to fight against this. Prosperity, abundance, you know, a better life next year, a better life for our children, That is the only path forward. That is the only way to have a functioning society. And there will always be people who want to sit around and say, we shouldn't do AI because we may burn a little more carbon, or we shouldn't do AI because we haven't fully addressed bias. And it turns out, a couple years later, we made a lot of progress on both of those things. And the anti progress streak, the anti like people deserve a great life streak, who are usually the people that have quite a lot of privilege in the first place,
Unknown: what the Does OpenAI intend to build tools that will specifically impact science and engineering, or will you be more focused on sort of business and consumer applications?
Sam Altman: For AI to increase the rate of scientific discovery. I believe that is the core engine of human progress and that it is the only way we drive the sustainable economic growth that we were talking about earlier. People aren't content with GPT-four. They want GPT-five. They want things to get better. Everyone wants more and better and faster. And science is how we get there. So of all of the things, of all the great things that AI will do, this may all be more one dimensional than we think. If we make a great AI tool that can help people solve any kind of problem in front of them, that can help people reason in new ways,
Unknown: sort of getting in your old car and driving to the library.
Sam Altman: great chief of staff or PhD student or whatever analogy you want that's off helping us optimize ourselves and do our best work and our best ideas and whatever. And then maybe at some point, it's like each of us has a full company full of brilliant experts of anything just working super productively together. Cool.
Unknown: for making a real impact in the world? And you alluded to it in terms of thinking about possibilities and not sitting in your bedroom in the dark. I think that's a good base recommendation.
Sam Altman: First of all, I think this is probably the most exciting time to be launching your career in many decades, maybe ever. I don't know. But it's like, whatever it is, it's a really big deal. And the fact that you have this huge tailwind means I think having this be a period where you work really hard. I certainly would be biased to do something with AI. the most important lesson to learn early on in your career is that you can kind of figure anything out and that no one has all of the answers when they start out, but you just sort of stumble your way through it, have like a fast iteration speed, try to like drift towards the most interesting problems to you, and be around the most impressive people, and have this like trust that you'll you can do more than you think faster than you think. but it gets you And so like, you can just do stuff sounds like not real advice or like very empty advice, but I think it's it's much more profound than it sounds on the surface. The other thing I would say is early on and this takes some practice, kind of like what your own personal I don't even know what to call it like passion, mission statement, like the kind of way you want to spend your time or what you really care about. And we talked about this concept of techno prosperity and better lives for people, that's been something for me that has always really resonated, and I've always tried to figure out how to work on that. But having some sort of like letting yourself develop some sort of guiding principle of how you make decisions about how to allocate your time and where to try to like steer things,
Unknown: and doing you can't always be so strategic about what you think is good for you to do. You want to do something that imagines all those possibilities and follows those passions.
Sam Altman: Again, I think this is the best time for new startups in particular in a very long time. Startups tend to succeed right around the time of big platform shifts. Big companies are slower and less innovative than startups, but they have a lot of other advantages. When you get the speed and iteration and cycle time advantage the most is when the ground is shaking. And right now, I think you can There was a moment like this, although smaller after mobile. There was another moment also smaller after AWS and this idea of cloud services. And then for a very long time, like more than a decade, we'd just been sort of waiting. it's an amazing time to start a company. And the advantages you have as a company are you can move much faster. You can live in the future more than big companies that have quarterly or annual or whatever they have planning cycles. And With any new tech platform, you can always drive phenomenal short term growth. that were not building an enduring business. But instead, we're building this sort of novelty thing that was and you kind of delude yourself because you get amazing fast growth, and because there's this like magic new technology and the dust hasn't settled yet. Just because there's a magic new technology, it does not excuse you from the laws of physics of a business. You still have to figure out a way to build some sort of switching costs, some sort of relationship with customers, some sort of compounding advantage over time. And the gold rush moments,
Unknown: other thing it's interesting how this question was phrased, and I'll read it in a minute. But now that I've heard you talk a little bit, I might phrase it a little bit differently. The question was phrased, in what ways might technology like ChatGPT threaten versus help the future of work? But it sounds like you tilt much more towards help, but also thinking about what that means in real terms. How does it help people in their future employment?
Sam Altman: and this is going to change the way that a lot of current jobs function, and this is going to create entirely new jobs. That always happens with technology. It's probably never happened this fast, although again, we may be drinking the Kool Aid too much, and the inertia of society may be such that it's slower than we think. But I kind of expect we're only a generation or two away from models that for the first time show some degree of real economic impact, good and bad, but something you can measure. And there will be classes of jobs that totally go away. There will be classes of jobs where you have to change what you do a lot. There will be classes of jobs where the productivity, compensation, whatever you want to talk about, whatever measure goes up by like a giant factor. And then there will be things that feel like jobs to the people of the future that to us today look like a complete indulgence and waste of time, I think as long as you believe that humans very deeply want to create and be useful and feel like they're making relative differential progress, all of which are things I would bet on hard. I love reading contemporaneous accounts from people living through previous technological revolutions and what they say about, man, we're all gonna only work four hours a week if we work at all, and we're just gonna, you know, like, And as a matter of degree, it might be. And as a matter of speed, I really think it will be. And I have some concern about how quickly we can adapt to this kind of change. But I have no real concern that we can eventually adapt to this kind of change. most jobs will be different in the future than they are today. But the deep human drivers don't seem to me likely to go anywhere.
Unknown: There is a concern that regulatory frameworks might solidify the position of established players, might stifle innovation, competition, accessibility. How do you envision AI be regulation? Because we really are at a critical moment being designed to uphold innovation and competition while ensuring that the field remains accessible for emerging players to pioneer the next transformative technologies?
Sam Altman: that the food you buy in a grocery store I think for AI systems, there will be some threshold above which we say, Okay, this system presents a level of risk that we don't want to take without reasonable safety precautions. we should open source these and let people use them. And there should be no regulatory burden on companies developing them because we're willing to make the innovation and freedom trade off for I totally get the impulse to say any regulatory action is unacceptable because it's just big companies that are going to use it for And if society decides we don't want to regulate AI at all and we'll just take our chances, I'll accept the outcome of a democratic process. It seems to me good to have some voices saying like then it seems to me useful to have some voices saying,
Unknown: won't be the same as the last one. I think we all think that. But there are lessons to be learned from 2020. What are these lessons? How can we mitigate the risk AI poses to the democratic process and to, the future of democracy in America?
Sam Altman: maybe I think there will be better deepfakes, of course. And there will be better troll farms, of course. What I think is more interesting is trying to get ahead of to the degree that we can, this is easier said than done but trying to get ahead of the new things that just weren't possible before. So like customized one on one persuasion, where an AI system reads all of your social media posts and targets something just at you. That wasn't really possible with all of the sort of online disinformation and trolling of the last election. And
Unknown: A more local question. One of MIT's educational priorities is to train tomorrow's leaders to be, in essence, computing bilingual, meaning that regardless of their chosen field, they will need to be fluent in computer science and AI to advance their work. Can you comment on the impact of our way of thinking? I don't know if people talk to you about blending blended computing, thinking about how we train bilinguals, how they can start thinking about the future careers, really learning two different fields and using computing as a way into other areas.
Sam Altman: general observations I make about the history of computing, it has gotten increasingly more accessible and more natural over time. I level programming languages were a step forward, And then you get to current programming languages, and they're way more accessible and way easier to use, also way more expressive and more powerful. Along with that evolution, you go from the command line to a GUI, to like a mouse and a keyboard, to just like touching your phone. Like, you don't that was not very that was pretty natural. which is supernatural. People are very good at language. And And this is, I think, a more profound thing than it sounds like on the surface. The degree to which we can push AI and people to have the same kind of interface So I'm more excited about humanoid robots than I am about other forms, because I think the world is just very designed for humans, and we should keep it that way. But we want the benefits of robots that can help us. I think we want AI systems to do their cognition and language, to communicate with us in language. It's a very human
Unknown: an over an overwhelming number of our students are obviously interested in CS. And now that we've sort of rolled out, you know, these blended areas, I think, you know, it may be sort of the sort of first generation of what you're talking about. In other words, people who who then, you know, next generations, it will be completely intuitive. Totally. Yeah. So that's really interesting.
Sam Altman: haven't thought as much as I would like to about any specific area, because figuring out how to get the general purpose intelligence and what that means has been pretty all consuming. Education and health care are kind of maybe the two specifics that I've thought about the most. On something like the financial system, I expect AI to impact that kind of as much as everything else.
Unknown: You know, we think a lot about how,
Sam Altman: I think with what you're seeing people do already, just with regular GPT-four and ChatGPT, when you say, please pretend you're a tutor and help me learn this thing, if that can work so well, then as people start to take next generation models and customize them for learning experiences, we're And it's like pretty awesome to see what people are building already.
Unknown: How you came to this? In other words, what was your sort of path, and how did you
Sam Altman: I spent like a lot of time reading sci fi or watching Star Trek or whatever, and I like that was already a time where all contemporary sci fi was pretty dystopic. But the older stuff, the older Star Trek episodes, those were still like kind of optimistic and cool. You sort of like saw how AI And then life went on, I got a little bit older. I went to school, and I decided to study in the AI lab. I also got really interested in energy, but this was kind of when I started to believe that if we could use technology And And then in 2012, It took me a couple of years to really internalize. Maybe this thing was finally working. or second most exciting time in OpenAI history right now. So Kind of all the obvious stuff, like original thinkers, smart, driven, dedicated people, people who sort of like are particularly driven to work on AI versus there's always like a set of people who will go work on whatever the hot area is. And then there's like a set of people who are like, nah, this is like the thing. We like people for whom this is their thing. Number one, I kind of suspect that forever onwards from now, it's always going to feel like, man, this next five year period is so critical. This is when I can really contribute. And after that, who knows what happens? But in practice, there's always going to be it's always going to feel like, man, this curve of intelligence is rising so fast. Right now, I can use these tools to outachieve, but eventually to outrun it. But at any point on that exponential, you'll always be able to use the tools to do amazing things, and it'll always feel like then you get totally outrun. But you never quite do because we just become more capable. And I didn't used to really think this way. I'm still not sure I'm right. But it does seem to me like we will just be able to accomplish more, do more things. We'll have ridiculous the expectations will go up too to participate in the economy in some sense. But before we feel like this wave is going to crash over us, we'll always feel like, man, in this five years, this is my window, but it's going to be a rolling five years forever. And I think we'll find that humans are really good at like, humans are we're just we're so wired to care about other humans. We're so wired to like focus our energy, know what other people want, be focused on like delivering value for others that, I can see very extreme worlds where human money and machine money are just different things. But there's an increasing premium on the human money part of that world. the biological drivers of humanity not changing that much. More bullish on startups building end user applications or infrastructure? benefit from the models getting better, not the ones that are betting the model actually doesn't get better and kind of are fixing the current generation problems. But I think there's like a lot of value to unlock there. Building infrastructure can be great, too. I think you can really succeed both ways. But the number of what feel like $100,000,000,000 application layer companies right now where you can really do something incredibly useful to people quickly seems pretty exciting to me. think it's so far we're so far away from when we start to level off that that's not currently on our mind. I think, at least for the next three, four model generations, I believe we can make it so incredibly much better every time. We should focus on that. And if we can do that, everything else will kind of work out. Like I think our niche or whatever is we want to deliver great, useful, impressive cognitive capability for as abundantly and inexpensively as we can. And there's other good things we can do, but if we can just focus on that, I think it'll be like a great service to the world, and we can go on that for a while. longer think there will be a time where the world agrees, okay, this was the year we crossed the AGI threshold. I think that the phrase has become the only way I know how to form the question well at this point is what is the range of time that we get to capability x, y, and z? But I think that the AGI I can't make myself do this because it's too much of like a it's too deep in the OS at this point. But I try to not use the word AGI anymore. It's a total like I'm never going to succeed at that. But it's like, when we do that new scientific discovery in some areas? When can we add a lot of economic value? I don't know. I expect that by the end of this decade, we have systems that create really significant economic value. I guess what's your thought process on trying to get unstuck? I somehow try to change context. I try to talk to different people about it. I try to, in an extreme case, I'll go travel somewhere to really kind of change things. Like the one time I like jet lag is if I'm really stuck on a problem, and I wake up in the middle of the night in some new context, and it seems to be helpful.