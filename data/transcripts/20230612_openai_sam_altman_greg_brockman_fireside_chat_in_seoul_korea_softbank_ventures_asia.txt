Unknown: be just really taking off here and be just like something that is supported and that people are just excited about, the degree to which people love ChatGPT,
Unknown: reinforcement learning, and even RL for the game playing and so on. But somehow recently you are hyper focused on language models and then you know how to use those language models into conversational agents and whatnot. So what made you make this kind of pivot? And then have you actually given up on all the other directions that you have you've pursued all your own?
Unknown: And one thing I've always believed is that you need to You you can kinda measure our progress towards AGI by the size of our largest and there just wasn't any way to do this earlier on. Right? So we had very diverse bets. We were working in robotics. At some point actually, we we had a great success in robotics with, you know, controlling a a robot hand. Mhmm. Super cool video by the way, you if you go look that up. that we have this attitude of really confronting reality as it is. of those predictions matching reality, You know, 10 x, a 100 x, like it really cannot be quantified because there's so many people who are applying it in their daily lives and then we can learn from that. We can improve the model. And so it's this kind of project moving from just OpenAI, a company in Silicon Valley to really this global project to make an AI that's actually beneficial to everyone. And that I think is is where the real gold is.
Unknown: in terms of the publication recently is that the author there's no author list. It's only OpenAI team. So how did you actually manage to implement this, let's say, culture where individual, egoful, let's say, scientists all work together to the point that they actually give up their spot on the author list, which is considered as a kind of one of the holy grails in academia. see. So perhaps, Nati, I can just follow-up on that again. I tend to follow-up a lot. So now in order to do that, you had to be able to sell your vision to the people that you recruited and then making sure that the scientists and engineers all work together in order to build this somewhat elusive AGI. And then I think a lot of people here who are the founders of the startups, they are probably asking the same question. What is the right way to sell this vision that is often very vague? How did you do that?
Unknown: lot of failed attempts, I guess. I think the first thing is you have to believe yourself. Right? And I think that the truth is, in this field especially, that you will never have And so, you know, we had an initial answer that that didn't stand the test of time. Right? And I think that our initial picture was, well, open source every gets
Sam Altman: very deep conviction, not that we were gonna succeed, but that AI was possible. We started the company on the observation that neural networks were working and seemed to be getting better with scale. and research and all of the other stuff we do together and I wasn't sure how that was going to go. We have met many leaders and heads of state. To a person, everyone has really understood what's happening, the potential tremendous upsides, the need to come together globally to mitigate the downsides as we look out ten years from now or whatever. So the conversations have been
Unknown: coordination not possible, governments will never get it, like people will just be thinking about me, me, me, like what can I do for my company, my constituents, my country And just the fact that we're seeing all this engagement, that there's all these people here today, And I think that first of all, with AI like it must be possible. Right? It's like we're pulling technology because it can benefit humans. Now, the change that it's going to bring I think is clearly immense. Right? Things will change fast and the details of the technology today may not be the same a year from now. And so the best advice that I have is to really lean into that. Right? Become an expert in what exists. But you shouldn't expect that your specific skills of oh, this is exactly how I do the prompt engineering or this is the specific piece of technology that I use, you should not expect that to last. That will go away. But this meta skill, seeing something new, leaning into it and saying how can I use this? What problems that used to be impossible, I can now solve.
Unknown: And then there, the United EU actually called for the regulation, proactive regulation from the governments or the international organization. But one thing that I couldn't actually hear was anything that is concrete. So for instance, some of these algorithms that you use already in order to build CHECH GPT and GPT-four are used for various purposes or the various sectors, including the defense. So some of the optimizers that you guys are using in OpenAI are used to train convolutional neural net that actually are used in the law enforcement or in defense purposes as well. So do you have any concrete suggestions on how we would be able to regulate even the current use of these AI technologies?
Sam Altman: different markets and different categories, that makes a lot of sense.
Unknown: But then I think you also cannot lose sight of where we're going in this this sort of idea of super intelligence of building systems that are very powerful that we see coming. Right? You know, on the scale of a decade. Right? Not today, but something that's coming in the future will require massive computational resources and really be a collective effort of humanity. And I think getting that right will be critical in a way that we have not seen for any previous technology. I think some organizations that will choose to not integrate it, not sort of you know, of change as a result AI pastors, for example. And so the constituents can ask questions to this pastor that can cite bible verses and it can can give advice. And so I think that there's innovation happening within spheres that you may not think about if you're just thinking about it from a tech lens. Fundamentally, AI is a thing about enhancing human activity, but not just activity, also I think human relationships and how we we sort of interact with each other. I think it can actually really benefit us on on those dimensions too. So I expect I expect to see some changes, but I also would not be surprised if there's some things that look exactly the same as they did thousands of years ago.
Unknown: Let me see.
Sam Altman: it's very easy to look at OpenAI from the outside as just like this sort of clean upwards exponential trajectory towards success, but like we have been very near to failure very many times. There have been like quite memorable moments where people that were like you know very associated with OpenAI early on said like you have a 0% chance of success rounded up. And in the early days, like we had some ideas but stuff just wasn't working and those are hard times. And you know, when kind of you're showing up and not that many people have belief and you have no actual proof or external signs that things are working and just some sort of like slightly promising research projects and you don't know like how you're gonna get enough capital or enough talent or enough compute or really like have any product strategy or even research strategy.
Unknown: was from 2016. By the way, who's heard of OpenAI Universe? in San Francisco in January,
Unknown: So the question is, looking at successful examples such as stable diffusion, it's pretty clear that open source has a strong competitive advantage thanks to its collective intelligence. Do you think or do you have any distinctive feature to compete against the open source approaches at OpenAI?
Unknown: into real flame, think is actually really important. And that's one thing I think from the conversation about regulation and things that that I think we really want to shine through is that like open source innovation, there's like so much potential there and it's really important to find a way through. So I think that's maybe maybe an answer. Right? Is that like we think that like what we see is that there's going to be these open source models and that they will be of limited size and capability. Right? Because there's only so many people who can run these giant models. Right? They're training these giant models. You know, there's a cut off between a $10,000,000 training run, a $100,000,000 training run, a billion dollar training run, and wherever we're gonna go in the future. So what OpenAI does is I think that we really focus on the models that are, like the problems that are currently impossible. Our goal is to do one impossible thing a year. And so that is something that will be at the far edge of all resources, of all the engineering talent, all of the research talent, all of the compute power, all of the sort of the whole supply chain oriented around these models. But we also see these open source models that will cover more breadth, and I think that those two together can then make something that's far more useful. So I think they actually complement each other quite well, and our approach is going to be sometimes Google open source models. We've done it in the past. We will do it in the future.
Unknown: I guess she is a founder of Softly AI. So her question is, how and what do you define PMF for AI startup, product market fit? Now that you're actually finding it, as far as I can tell, so how are you finding it?
Sam Altman: to '10 at the most where people would talk about being a mobile startup. And then after that you never heard someone say they were a mobile startup again really because like it was just an expected piece of the technology infrastructure to integrate for every company. I think the same thing is gonna happen for AI and it's probably a mistake even now to think of yourself as an AI startup. You have to just be a startup that makes something people love and wanna use. And that product market fit can be defined all kinds of ways. One that I've always used is something so good that people spontaneously tell their friends about it but there's like hundreds of other good definitions. You certainly know it when you feel it and it's like, you know, everything is on fire all the time but in a good way.
Unknown: Now, one important thing I believe is that I think that in AI, because things change so fast, you know, we hear from people saying, how do I know that my business won't be just obviated by the next model release? And so I think that here, That's probably a good sign that you want to double down and if you can get product market fit there, that will be something that will last.
Unknown: So here comes an even trickier and perhaps even spicier question. Now what we see with the new technology throughout the humanities history is that those new technologies tend to disrupt the job market, labor market. And in fact, Sam, you've been talking about it for some time and more recently so with the HIV and PTA and whatnot, people started to talk about the job displacement and whatnot. Now the question is in fact from the Yong Hako from the Korea Business Angels Association. How should our society prepare prepare for it, in particular in the context of chat GPT and advanced AI that is being deployed?
Sam Altman: as it seems to predict. Already we're seeing that these models are doing more to enhance people, make them more productive at existing jobs. But there's such a there's so much like surplus demand for labor in so many fields that if you can make computer programmers two or three times more productive, Over time, they will get better and they will automate more and more, but I really think that human creativity, demand for being fulfilled, doing things for other people, status, you know, all the reasons we work hard and want to create things, wanna wanna put something back into society, that's not going anywhere. And I think human ability there will remain important. The thing that I worry about is not our inability to adapt. It is the speed this might all happen. So if you study the history of technological revolutions, seems like roughly in two generations we can adapt to almost any amount of labor market change. And I think what will really happen is not that none of us have jobs, but we have different kinds of jobs that may not look much like the jobs of today, but that we like much much more. And when people a hundred years from now look back at us now, they'll be like, wow, I can't believe they lived like that. It sounded so awful. Just one one thing to add by the way is that for me the biggest
Unknown: surprise, the biggest theme and kind of reflected in Sam's answer is that I think this technology is not gonna play out like any of the kind of stereotypical ways that we that people have been thinking. Right? Kind of the conversation ten years ago about AI would have been okay when AI happens, which who knows whenever that will be, which job will come for first. Right? And of course, it will be, you know, manual labor, working a restaurant or something like that, and the last job to go, of course, will be a poet or, you know, writing software or something. Like, those are clearly the hardest things. And I think the theme of AI is that our expectations about what is hard, what is easy, about what this technology will do, how we'll integrate with our society, how people will respond, and it's a positive thing. Right? I think that like the shape of this is that it actually enhances human creativity in a way that no one really saw it coming. It removes a lot of the drudge work and, you know, maybe that will change and that we will change with it. Right? And I think that that maybe the most important thing with sort of navigating through AI is to go in it with this extremely open mind and learn from from reality. Like really not just, you know, not just the positive, right, but the negative. But really leaning to what we're seeing and not just our own preconceptions.
Unknown: Okay. So Now let's talk about the a viewing AI from a slightly different perspective. So question is, what do you think of the view that AI is becoming more of a basic utility, like water and electricity? And then in that case, the so called, let's say, foundation model companies running their own application creates any kind of conflict of interest because we wouldn't want the electricity company to dominate all those, let's say, appliance markets. So what do you think about it? Will OpenAI actually go down this route or will be a utility company?
Unknown: Right? GE, you know, General Electric, And was it really because that's what they wanted to do, or was it because to get adoption of the technology required you basically needed the people who are experts in it to really show it almost as a marketing technique? And were the washing machines the most important thing that came of electricity? Like, no. that our goal is to build AGI, safe AGI, deploy it that to benefit all of humanity. That's what we wanna do. That's what we're here for. Building an API, building ChatGPT,
Sam Altman: person that has a water hookup to your home. And I think a lot of the
Unknown: Sure. Look, we really think that it's important for us to find a way, you know, not just as OpenAI, but as a society and and and a world for content creators to benefit from this technology. Right? If you are, you know, creating great works and that's something that ends up helping AI get better, like somehow there should be compensation to you. It should be something where you're just better off in this future world than than today. And I think that the figuring out those mechanisms is gonna require experimentation. We plan on experimenting with them and that this is one of the reasons that we engage so directly with people in the community and that we talk to a lot of artists, a lot of people who who are are creating. But I I believe fundamentally that AI can, we're starting to see it a little bit, make And we actually did a bunch of of really interesting work in games. You know, 2017, 2018, we worked on the competitive video game Dota two, and there we ended up building bots that got better and better. But honestly, we've kind of moved on. Right? I think that that we will But the reason that we've kind of moved on is that they're also so limited. Right? That what we want and what we're sort of able to encounter now is the complexity of the real world. But there's still something missing. Right? Because if you look at the game AIs, they had this competence that I thought was so interesting. Right? That like they're really focused on a goal. And by the way, think this is something that's also a little scary. Right? Something to think about of if you do have a very very competent AI system that's pursuing a goal, how do you make sure that it's a good goal for humanity? Right? And I think that some of these some of these questions But I also think that AI starts the language models start to change the equation because people worry about, okay, if you have to literally specify what an AI is supposed to do and it just pursues that, that could be quite bad. But with language models, have this sort of competence and this sort of common sense which changes the equation. So I think that games may be an aspect of the future technology development, but we look at it through this lens of how can we get to the next level of reliable, trustworthy, and capable systems through
Unknown: directed to Sam that is very relevant. I think you're about to say this. So to Sam, I heard you've written SF fiction about positive perspective or the prospect of AGI. Is there anything that is different between your fiction and the real world that we live in now? And what kind of breakthroughs from computing hardware would you like to see for the AI innovation in the future?
Unknown: as in the multiplication of many terms, of kind of all of the imaginable inputs. Right? That is the compute power and that's the one that gets the from field. You know, we talked to all the players. We've sort of paid attention to that field for a long time. One of the barriers there is actually building great software that's very usable. And so that's actually one place that there's a real opportunity is to build better chips that are sort of optimized for not just today's models, but to really be able to see where the models are going and making it so people can experiment very flexibly and build new architectures. So I think there's actually a lot to be done. Inference hardware is another one where just like part of the reason it's hard for us to give g p d four to everyone is because there just isn't enough compute power for the demand. And with hardware that I'm sure we'll have five years from now, it would be a different story. So I think that there is so much fruit and so much that everyone in this industry can do to contribute to the production of better models, but also the ability to scale them to everyone.
Sam Altman: many peep for many use cases, you still will want to call a service in the cloud even if you're using it locally. We're very very excited about like a personal super assistant that helps you with everything, but you want that, I think, to have as much intelligence as possible.
Unknown: subdivide a problem and farm out the hardest part of the problem to the cloud model. So I think that this hybrid world that we'll move to with edge compute plus cloud, I think will just look quite different from today's inferencing architecture.
Sam Altman: to thrive in that world. I think the skills that matter are deep familiarity with the tools and staying abreast of changes and just working with them very closely to develop a great intuition for where things are going and how to make use of it. Resilience, But those are the kind of skills that I think are going to be very much rewarded. The one
Unknown: is actually a really important skill to build. And some of it is it's not going to be about the mechanics. It's not going to be about the specifics of the programming language. Not going be about those details. But fundamentally, getting computers to do what you intend. Like that's the world we're heading to. And so understanding the low level details of how it works, that's a starting point. And you know, we do see people who have never coded before, who have built some who built some of the best AI applications. And so that's actually new. Right? That you can actually get computers to do things without having to do it in this like contort yourself to the machine. But I think it's still a really important skill and the more that you can find sort of classes that help you layer your understanding from the low level to this highest level of communicating intent and sort of philosophy of what it is that you even want from this machine, what do you want it to do, I think all of that will