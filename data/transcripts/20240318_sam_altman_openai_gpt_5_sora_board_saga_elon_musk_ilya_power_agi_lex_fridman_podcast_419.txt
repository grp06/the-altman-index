Sam Altman: possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow, that's really remarkable. There were great things about it too, I wish I wish it had not been in such an adrenaline rush that I wasn't able to stop and appreciate them at the time. But I came across this old tweet of mine, or this tweet of mine from that time period, which was like it was like, you know, kind of going to your own eulogy, watching people say all these great things about you, and just, like, unbelievable support from people I love and care about. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was gonna be something crazy and explosive that happened, but there may be more crazy and explosive things still to happen.
Unknown: go through that as, like you said, iterate as often as possible in figuring out how to have a board structure, how to have organization, how to have the kind of people that you're working with, how to communicate all that in order to
Sam Altman: you know, like something that was in the past that was really unpleasant and really difficult and painful, but we're back to work and things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after there was like this fugue state for kind of like the month after, maybe forty five days after, that was I was just sort of like drifting through the days. I was so out of it. I was feeling so down. Really painful. And hard to like have to keep running OpenAI in the middle of that. I just wanted to, like, crawl into a cave and kind of recover for a while. But, you know, now it's like we're just back to working on the mission.
Unknown: Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research and product development and money and all this kind of stuff so that you, who have a very high potential of building AGI, would do so in a slightly more organized, less dramatic way Yeah. In the future. So there's value there to go, both the personal psychological aspects of you as a leader and
Sam Altman: And I think that is it is valuable that this happened now in some sense. I think this is probably not, like, the last high stress moment of open AI, but it was quite a high stress moment. My company very nearly got destroyed. And we think a lot about many of the other things we've gotta get right for AGI, but thinking about how to build a resilient org and how to build a structure that will stand up to, like, a lot of pressure in the world, which I expect more and more as we get closer, I think that's super important. are well meaning people on the whole. And people understandably make suboptimal decisions. And I think one of the challenges for OpenAI will be we're gonna have to have a board and a team that are good at operating under under pressure. but one of the things that we did see is in in most corporate structures, boards are usually answerable to shareholders. You know, there's sometimes people have, like, supervoting shares or whatever. In this case, and I think one of the things with our structure that we maybe should have thought about more than we did, is that the board of a nonprofit has, unless you put other rules in place, like, quite a quite a lot of power, they don't really answer to anyone but themselves. And there's ways in which that's good, but what we'd really like is for the board of OpenAI to, answer to the world as a whole as much as that's a practical thing. I think, didn't have a lot of experienced board members, and a lot of the new board members at OpenAI have just have more experience as board members, a lot of ups and downs. And we were trying to agree on new board members that both sort of the executive team here and the old board members felt would be reasonable. Larry was actually one of their suggestions, the old board members. Brett, I think, I had even previous to that weekend suggested, but he was, you know, busy and didn't wanna do it, and then we really needed help and would. We talked about a lot of other people too, but that was I felt like if I was going to come back, I needed new board members. I didn't think I could work with the old board again in the same configuration, although we then decided, and I'm grateful, that Adam would stay, but we wanted to get to we considered various configurations, decided we wanted to get to a board of three, and had to find two new board members over the course of sort of a short period of time. So those were decided, honestly, without you know, that's like you kinda do that on the battlefield. You don't have time to design a rigorous process then. For new board members, since new board members will add going forward, We have some criteria that we think are important for the board to have, different expertise that we want the board to have. Unlike hiring an executive where you need them to do one role well, the board needs to do a whole role of kind of governance and thoughtfulness not as individuals one at a time. And, you know, thinking about a group of people that will bring nonprofit expertise, expertise running companies,
Unknown: if you reach AGI or you reach some of these incredibly impactful products and you build them and deploy them, what's the conversation with the board like? And they kind of think, alright,
Sam Altman: Look, I think you definitely need some technical experts there, and then you need some people who are like, what can how can we deploy this in a way that will help people in the world the most, and people who have a very different perspective? You know, I think a mistake that you or I might make is to think that only the technical understanding matters, and that's definitely part of the conversation you want that board to have. But there's a lot more about how that's gonna just, like, impact society and people's lives that you really want represented in there too. And you're just kinda are you looking at the track record of people, you know, there's some roles where I kind of totally ignore track record and just look at slope, kinda ignore the y intercept. and experience is sometimes very hard to replace.
Unknown: that weekend. What were some of the low points psychologically for you? Did you consider going
Sam Altman: my phone was just, like, sort of nonstop blowing up with nice messages from people I work with every day, people I hadn't talked to in a decade. I didn't get to, like, appreciate that as much as I should have because I was just, like, in the middle of this firefight, but that was really nice. But on the whole, it was, like, a very painful weekend and also just, like, a very it was like a battle fought in public to a surprising degree, and that's that was extremely exhausting to me, much more than I expected. Friday afternoon. I really couldn't get much in the way of answers, but I also was just like, well, the board gets to do this, and so I'm gonna think for a little bit about what I wanna do, but I'll try to find the the blessing in disguise here. And I was like, well, I you know, my current job at OpenAI is, or it was, like, to, like, run a decently sized company at this point. The thing I always liked the most was just getting to, like, work on work with the researchers. And I was like, yeah, I can just go do, like, a very focused AGI research effort. And I got excited about that. It didn't even occur to me at the time to, like, possibly that this was all gonna get undone. This was, like, Friday afternoon. So you've accepted Like, within, you know I mean, I went through, like, a little period of confusion and rage, but very quickly. And by Friday night, was, like, talking to people about what was gonna be next, and I was excited about that. I think it was Friday night evening for the first time that I heard from the exec team here, which is like, hey, we're gonna, like, fight this and, you know, we think, well, whatever. And then I went to bed just still being like, okay, excited, like, onward. Were you able to sleep? sort of didn't sleep much, didn't eat much, and still kinda had, like, a surprising amount of energy. Was you learn, like, a weird thing about I immediately didn't wanna do that, but I thought a little more and I was like, well, I don't really care about the people here, the partners, And and then the most painful time of all was over the course of that weekend, while the whole world was trying to break it apart, people trying to recruit, whatever. We kept being told, like, alright, we're almost done, we're almost done, we just need, like, a little bit more time. when, again, like, every few hours I expected that we were gonna be done and we're gonna, like, figure out a way for me to return and things to go back to how they were, but I felt a lot of love that whole weekend. Mhmm. It was not Other than that one moment, Sunday night, I would not characterize my emotions as anger or hate, but I really just, like see leaders in the moment, in, like, the crisis moments, good or bad. But a thing I really value in leaders is how people act on a boring Tuesday at 09:46 in the morning. Mhmm. And in in just sort of the the the normal drudgery of the day to day,
Unknown: in part part of this drama with the board and all that kind of stuff. What's your relationship with him now?
Sam Altman: I don't have anything I can, like, say about his plans right now. That's that's a question for him. But I really hope we work together for, you know, certainly the rest of my career.
Unknown: You know, there's a there's a meme that he saw something. Like, he maybe saw AGI,
Sam Altman: is he takes AGI and the safety concerns, broadly speaking, you know, including things like the impact this is gonna have on society very seriously. And we as we continue to make significant progress, Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to ensure we get it right, to ensure that we succeed at the mission. credit to humanity in terms of how much he thinks and worries about We wanted to get new board members in place first, but, you know, we clearly learned a lesson about structure throughout this process. I don't have, I think, super deep things to say. It was a crazy, very painful experience. I think it was like a perfect storm of weirdness. It was like a preview for me of what's gonna happen as the stakes get higher and higher and the need that we have, like, robust governance structures and processes and people. that it has definitely changed, and I really don't like this, it's definitely changed how I think about just, like, default trust of people and planning for the bad scenarios. You gotta be careful with that. Are you worried about becoming a little too cynical?
Unknown: But in terms of structure see, I'm more interested on a human level. Like, how do you surround yourself with humans that are building cool shit, but also are making wise decisions Yeah. Because the more money you start making, the more power the thing has, the weirder people get.
Sam Altman: And I have just like enormous gratitude and trust and just thinking we were gonna be a research lab and having no idea about how this technology was gonna go. It's hard to because it was only, you know, seven or eight years ago, it's hard to go back and really remember what it was like then. But before language models were a big deal, this was before we had any idea about an API or selling access to a chatbot. This was before we had any idea we were gonna productize at all. So we're like, we're just, like, gonna try to do research and, you know, we don't really know what we're gonna do with that. I think with, like, many new fundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need to do different things and also have huge amounts more capital. So we said, okay, well, the structure doesn't quite work for that. How do we patch the structure? eyebrow raising to say the least. But we got here gradually with, I think, reasonable decisions at each point along the way. And doesn't mean I wouldn't do it totally differently if we could go back now with an oracle, but you don't get the oracle at the time. But anyway, in terms of what Elon's real motivations here are, I don't know. you know, Elon said this set of things. Here's our characterization, or here's the sort of not our characterization, here's, like, the characterization of how this went down.
Unknown: empathy for this. I mean, I I do think that there's personal stuff here, that there was a split, that OpenAI and a lot of amazing people here chose to part ways with Elon. So there's a personal Elon chose to part ways.
Sam Altman: The the the choosing to part ways? He thought OpenAI was gonna fail. He wanted total control to sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort. At various times, he wanted to make OpenAI into a for profit company that he could have control of or have it merge with Tesla. We don't wanna do that, and he decided to leave, which that's fine. My memory is the proposal was just like, yeah, get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was. So what
Unknown: is the word open in OpenAI mean?
Sam Altman: One of the things that I think OpenAI is doing that is the most important of everything that we're doing is putting powerful technology in the hands of people for free as a public good. Not we're not you know, we don't run ads on our free version. We don't monetize it in other ways. We just say it's part of our mission. We wanna put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of open is really important to our mission. I think if you give people great tools and teach them to use them, or don't even teach them, they'll figure it out, and let them go build an incredible future for each other with that, that's a big deal. So if we can keep putting, like, free or low cost or free and low cost powerful AI tools out in the world, I think that's a huge deal for how we fulfill the mission.
Unknown: in in the land of memes
Sam Altman: Look, I mean, Grok had not open sourced anything until people pointed out it was a little bit hypocritical, and then he announced that Grok will open source things this week.
Unknown: but friendly competition versus, like I personally hate lawsuits. Or, like, at least took the first step in the game of chess of, like, really open sourcing the model. Of course, it's not the state of the art model, but open sourcing llama. And Google is flirting with the idea of open sourcing a smaller version.
Sam Altman: Have you what are the pros and cons of open sourcing? Have you played around with this idea? Yeah. I think there there is definitely a place for open source models, particularly smaller models that people can run locally. Think there's huge demand for. I think there will be some open source models. There will be some closed source models. This it won't be unlike other ecosystems in that way.
Unknown: of going from nonprofit to this cap for profit. Where do you hope this goes with Elon? What this this tension, this dance, what do you hope this like, if we go one, two, three years from now, your relationship with him on a personal level too, like friendship, friendly competition, just all this kind of stuff. And just compete and win and and explore these ideas together. I do suppose there's competition for talent or whatever, but It truly is amazing on a product level, but also just on a philosophical level. So let me just technical slash philosophical ask, what do you think it understands about the world more or less than GPT four, for example? Like, world model when you train on these patches versus language tokens.
Sam Altman: I think all of these models understand something more about the world model than most of us give them credit for. underlying physics looks so well represented over a lot of steps in a sequence. It's like, oh, this is this is quite impressive. But fundamentally, these models are just getting better, and that will keep happening. If you look at the trajectory from Dolly one to two to three to Sora,
Unknown: is basically modeling the physics, three-dimensional physics of the world sufficiently well to capture those kinds of things. Well or is it just, know, bigger model or better,
Sam Altman: like, technical details or better data, more data is going to solve those the cat sprotting say yes to both. Like, I think there is something about the approach which just seems to feel different from how we think and learn and whatever.
Unknown: And then also, think it'll get better with scale. Like I mentioned, LLMs have tokens, text tokens, and SORA has visual patches. So it converts all visual data, a diverse kinds of visual data, videos, and images into patches. Is the training to the degree you can say fully self supervised or is there some manual labeling going on? Like, what's the involvement of humans in all this? self supervised learning because you what you mentioned in the technical report is Internet scale data. That's another beautiful it's like poetry. So it's a lot of data that's not human labeled. It's like Yeah. It's self supervised in that way. Yeah. And then the question is how much how much data is there on the Internet that could be used in this that is conducive to this kind of self supervised way, if only we knew the details of the self supervised. Do you have you considered opening it up a little more details? We have. For you mean for Sora specifically? Sora specifically, the shrink. Because it's so interesting that, like, can this l o can the same magic of LLMs now start moving towards visual data? And what does that take to do that?
Sam Altman: a level of efficiency that will deliver the scale people are gonna want from this. So that I don't wanna, like, downplay that, and there's still a ton ton of work to do there. But, you know, you can imagine, like, issues with deepfakes, misinformation. Like, we try to be a thoughtful company about what we put out into the world, and it doesn't take much thought to think about the ways this can go badly. should be or is fair use under copyright law? I think the question behind that question is, do people who create valuable data deserve to have some way that they get compensated for use of it? And that, I think the answer is yes. I don't know yet what the answer is. People have proposed a lot of different things. We've tried some different models. for humans to keep doing cool shit. Everything I worry about, humans are gonna do cool shit and society is gonna find some way to reward it. I I That seems pretty hardwired. We wanna create, we wanna be useful, we want to, like, achieve status in whatever way. Mhmm. That's not going anywhere, I don't think. But the reward might not be
Unknown: monetary, financially. It might be like fame and celebration
Sam Altman: of other cool Maybe financial in some other way. Again, don't think we've seen like the last evolution of how the economic system's gonna work. Yeah. But artists and creators are worried. AI generated content do you think in the next five years? People talk about, like, how many jobs they are gonna do in five years. And and the framework that people have is what percentage of current jobs are just gonna be totally replaced by some AI doing the job. The way I think about it is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of the, like, five second tasks in the economy, the five minute tasks, the five hour tasks, maybe even the five day tasks, how many of those can AI do? And I think that's a way more interesting, impactful, important question than how many jobs AI can do, because it is a tool that will work at increasing levels of sophistication and over longer and longer time horizons I think that for videos on YouTube, it'll be the same. Many videos, maybe most of them, will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together,
Unknown: humans like in the Adobe Suite type of way where you can just make videos much easier and all that kind of stuff. generating faces. Oh, it's it's getting there, but generating faces in video format is tricky when it's specific people versus generic people. Let me ask you about GPT four. There's so many questions. It's But for me, looking back, GPT four, Chad GPT is pretty damn impressive, like, impressive.
Sam Altman: look at GPT-three and you're like, that's unimaginably horrible. I expect that the delta between five and four will be the same as between four and three, and I think it is our job to live a few years in the future and remember that the tools we have now are gonna kind of suck looking backwards at them, and
Unknown: What are the most glorious ways that GPT-four sucks? limits of those best things that allow you to say it sucks, therefore gives you inspiration and hope for the future.
Sam Altman: You know, one that I've been using it for more recently is sort of a like a brainstorming partner. I promise for that. There's a glimmer of something amazing in there. I don't think it gets you know, when people talk about it, it what it does, they're like, it helps me code more productively, it helps me write more faster and better, it helps me, you know, translate from this language to another. All these, like, amazing things, but there's something about the, like, kind of creative brainstorming partner when I can help on longer horizon tasks, you know, break down something in multiple steps, maybe, like, execute some of those steps, search the Internet, write code, whatever, put that together. When that works, which is not very often, it's, like, very magical. Both, like, you know, to break it down and then do things at different layers of abstraction and put them together.
Unknown: That said, I mean, Chad GPT was a transition to where people, like, started to believe it. There was a kinda
Sam Altman: That was more about the ChatGPT interface than the
Unknown: those two each of those things are important? The underlying model and RLHF or something of that nature that tunes it to be more compelling to the human, more
Sam Altman: super important, but the the RLHF, the post training step, the, you know, little wrapper of things that which is not just about the actual product work itself, but this whole other step of how you align it and make it useful. that were both, like, I would say quite significant achievements, and then a lot of things like scaling it up that other companies have had to do before.
Unknown: How does the the context window of going from 8 k to $1.28 k tokens compare from the
Sam Altman: you know, if we dream into the distant future, we'll have, like, like, way distant future. Mhmm. We'll have, like, context length of several billion. You will feed in all of your information, all of your history over time, and it'll just get to know you better and better, and that'll be great. For now, the way people use these models, they're not doing that, and, you know, people sometimes post in a paper or, you know, a significant fraction of a code repository or whatever. And he just couldn't seem genuineness couldn't imagine that the world would eventually need gigabytes of memory in a computer or terabytes of memory in a computer. And you always do. Or you always do just need to, like, follow the exponential of technology, and we're gonna, like we will find out how to use better technology. So I can't really imagine what it's like right now for context links to go out to the billions someday. And they might not literally go there, but effectively it'll feel like that.
Unknown: trillions upon trillions. Sure. There'll be some kind of breakthrough
Sam Altman: this is mostly younger people, but people who use it as, like, their default start
Unknown: reading partner for reading books. It helps me think help me think through ideas, especially when the books are classic, so it's really well written about, and it actually is is I I find it often to be significantly better than even, like, Wikipedia on well covered topics. It's somehow more balanced and more nuanced. Or maybe it's me, but it inspires me to think deeper than a Wikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration, I'm not sure where the magic is. If it's in here or if it's in there, or if it's somewhere in between. I'm not sure. But one of the things that concerns me for knowledge task when I start with GPT is
Sam Altman: That's obviously an area of intense interest for us. I think it's gonna get a lot better with upcoming versions, but we'll have to, you know, work on it, and we're not gonna have it, like, all solved this year. Well, the scary thing is, like, as it gets better,
Unknown: Well, I think the bigger journalistic efforts that take days and weeks and and rewards great in-depth journalism. Also, journalism that represents stuff in a balanced way where it's, celebrates people while criticizing them even though the criticism is the thing that gets clicks, and making shit up also gets clicks, and headlines that mischaracterize completely. I'm sure you have a lot of people dunking on, problem about human civilization I'd love to see solved, just where we celebrate a bit more. You've given Chad GPT the ability to have memories. You've been playing with that about previous conversations. not optimally, I suppose. What what have you seen through that, like playing around with that idea of remembering conversations or not? We're very early in our explorations
Sam Altman: a model that gets to know me and gets more useful to me over time. but that's where you'd like to head. You know, you'd like to use a model and over the course of your life, or use a system, there'd be many models, and over the course of your life, it gets
Unknown: little factoids and preferences and so on. What about remembering like, don't you want GPT to remember all the shit you went through in November
Sam Altman: integrate the lessons of that Yes. And remind me in the future what to do differently or what to watch out for. And, you know, we all gain from experience over the course of our lives, varying degrees, and I'd like my AI agent to gain with that experience too. So if we go back and let ourselves imagine that, you know, trillions and trillions of context length,
Unknown: People sometimes will hear that and be concerned about privacy. Is there what what what do you think about that aspect of it? The more effective the AI becomes at really
Sam Altman: You and I may have different opinions about where on that privacy utility
Unknown: Well, that's beautifully said, but there could be some lingering stuff in there. Like, what I would be concerned about is that trusting that you mentioned, that being paranoid about people in in my part time explorations, I've been diving deeply into the Zelensky administration, the Putin administration, and the dynamics there in wartime in a very highly stressful environment. And what happens is distrust, and you isolate yourself both. And you start to not see the world clearly, and that's a concern. That's a human concern. You seem to have taken it in stride and kinda learned the good lessons and felt the love and let the love energize you, There's just some questions I would love to ask in your intuition about what's GPT able to do and not. So it's allocating approximately the same amount of compute for each token it generates. Is there room there in this kind of approach Will it be similar, like, architecturally as what we're seeing now with LLMs? Is it a layer on top of the LLMs?
Sam Altman: I can imagine many ways to implement that. I think that's less important than the question you were getting at, which is do we need a way to do a slower kind of thinking where the answer doesn't have to get, like, Maybe. I mean, there's a lot of things that you could imagine working. What, like, the right or the best way to do that will be, we don't know.
Unknown: This does make me think of the mysterious but
Sam Altman: rather than go build in secret until we got all the way to GPT five, we decided to talk about GPT one, two, three, and four. and also the world, people, institutions, whatever you wanna call it, need time to adapt and think about these things. And I think one of the best things that OpenAI has done is this strategy, and we we get the world to pay attention to the progress, to take AGI seriously, to think about what systems and structures and governance we want in place before we're, like, under the gun and have to make a rushed decision. I think that's really good. But the fact that people like you and others say you still feel like there are these leaps makes me think that maybe we should be doing our releasing even more iteratively.
Unknown: But people tend to like to celebrate people celebrate birthdays. I don't know if you
Sam Altman: I I totally get that.
Unknown: So that goes to the question of like, what what's the way we release this thing?
Sam Altman: We'll release over in the coming months many different things. I think that'd be very cool. I think before we talk about, like, a GPT five like model called that or called or not called that, or a little bit worse or a little bit better than what what you'd expect from a GPT five, I think we have a lot of other important things to release first.
Unknown: for
Sam Altman: It's all of these things together. Like, the thing that OpenAI, I think, does really well this is actually an original Iliad quote that I'm gonna butcher, but it's something like believe is that it's sometimes useful to zoom out and look at the entire map. I think this is true for, like, a technical problem, I think this is true for, like, innovating in a business, but things come together in surprising ways and having an understanding of that whole picture, In fact, one of the things that I used to have, and I think was super valuable, was I used to have like a good map of that all of the front or most of the frontiers in the tech industry. And I could sometimes see these connections or new things that were possible that if I were only, you know, deep in one area, I wouldn't I wouldn't wouldn't be able to, like, have the idea for because I wouldn't have all the data. And I don't really have that much anymore. I'm, like, super deep now.
Unknown: Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing perhaps that you said. may have a foundation of, like, insight there.
Sam Altman: in the world. it's an unusual I think it's gonna be an unusual market. You know, people think about the market for, like, chips for mobile phones or something like that, and you can say that, okay, there's 8,000,000,000 people in the world, maybe 7,000,000,000 of them have phones, maybe there are 6,000,000,000, let's say. They upgrade every two years, so the market per year is 3,000,000,000 system on chip for smartphones. And if you make 30,000,000,000, you will not sell 10 times as many phones because most people have one phone. But compute is different. Like, intelligence is gonna be more like energy or something like that, where the only thing that I think makes sense to talk about is at price x, the world will use this much compute, and at price y, the world will use this much compute. Because if it's really cheap, I'll have it, like, reading my email all day, like, giving me suggestions about what I maybe should think about or work on, and trying to cure cancer, and if it's really expensive, maybe I'll only use it or will only use it to try to cure cancer. So I think the world is gonna want a tremendous amount of compute, and there's a lot of parts of that that are hard. Energy is the hardest part, Building data centers is also hard. The supply chain is hard. And then, of course, fabricating enough chips is hard. But this seems to me where things are going. Like, we're gonna want an amount of compute that's just hard to reason about right now. like, quite amazing, and I hope as a world we can re embrace that. It's really sad to me what how the history of that went and hope we get back to it in a meaningful way.
Unknown: You. How do you decrease the theatrical nature of it? You know, I've already starting to to hear rumblings because I do talk to people on the on both sides of the political spectrum. Hear rumblings where it's going to be politicized. AI is going to be politicized. It really worries me because and it's like maybe the right is against AI, and the left is for AI because it's going to help the people or whatever whatever the narrative and formulation is that really worries me.
Sam Altman: How do you fight that? I think it will get caught up in, like, left versus right wars. I don't know exactly what that's gonna look like, but I think that's just what happens with anything of consequence, unfortunately. What I meant more about theatrical risks is, like, AI is gonna have, I believe, tremendously more good consequences than bad ones, but it is gonna have bad ones. And there'll be some bad ones that are bad but not theatrical. You know, like, a lot more people have died of air pollution than nuclear reactors, for example, but we worry most people worry more about living next to a nuclear reactor than a coal plant. But something about the way we're wired is that although there's many different kinds of risks we have to confront, the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very bad over a long period of time but on a slow burn. answer to this that maybe I can think of more nuance later, but the pros seem obvious, which is that we get better products and more innovation faster and cheaper and all the reasons competition is good. And the con is that I think if we're not careful, it could lead to an increase slow timelines for the start of AGI, long timelines, and then a short takeoff or a fast takeoff, I think short timelines, slow takeoff is the safest quadrant and the one I'd most like us to be in,
Unknown: And so there's always attention and incentives and motivations, and in the end, I do hope humanity prevails.
Sam Altman: I was thinking someone just reminded me the other day about how the day that he got like, surpassed Jeff Bezos for, like, richest person in the world, he tweeted a silver medal at Jeff Bezos.
Unknown: Yeah. And we can each be the best version of ourselves and strive to do so. Let me ask you. I think it's fair to say in terms of the access, the world's access to information, how we interact, and so on. And one of the nerve wracking things for Google, but for the entirety of people in this space is thinking about really take on this thing that Google started twenty years ago, which is how do we
Sam Altman: go, you know, like, people should use a better product. But I think that would so You know, Google shows you, like, 10 blue links well, like, 13 ads and then 10 blue links, and that's, like, one way to find information. But the thing that's exciting to me is not that we can go build a better copy of Google search, but that maybe there's just some much better way to help people find and act and on and synthesize information. Actually, I think ChatGPT is that for some use cases, and hopefully, we'll make it be like that for a lot more use cases. But I don't think it's that interesting to say, like, how do we go do a better job of giving you, like, 10 ranked web pages to look at than what Google does? Maybe it's really interesting to go say, how do we help you get the answer or the information you need? How do we help create that in some cases, synthesize that in others, or point you to it in in yet others? I think ads needed to happen on the internet for a bunch of reasons to get it going, but it's a more mature industry. The world is richer now. I like that people pay for ChatGPT and know that the answers they're getting are not influenced by advertisers. LLMs, and I'm sure there's a way to, like, participate in the transaction stream in an unbiased way that is Like, I know I'm paying, and that's how the business model works. And when I go use, like, Twitter or Facebook or Google or any other great product, but ad supported great product,
Unknown: But then does that system always result in the ads driving the kind of stuff that's shown all that? It's is sustainable from a business perspective?
Sam Altman: business that pays for our compute needs without ads, that I think the answer is yes.
Unknown: also bias and just a skepticism in general. And in terms of interface, because I personally just have like a spiritual dislike of crappy interfaces, which is why AdSense when it first came out was a big leap forward versus, like, animated banners or whatever. But, like, it feels like there should be many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in a big fundamental way, which is, like, what you were saying. Like, it will manipulate safety, but also bias and, like, safety in the short term, safety in the long term. The Gemini one five came out recently. There's a lot of drama around it, speaking of theatrical things, and it generated black Nazis and black founding fathers. I think fair to say it was, you know, a bit on the ultra woke side. So that's a concern for people that, you know, if there is a human layer within companies that modifies the the safety that fits sort of an ideological lean within a company.
Sam Altman: How do you deal with that? I mean, we work super hard not to do things like that. We've made our own mistakes. We'll make others. I assume Google will learn from this one, still make others. It it it is is all like, these are not easy problems. One thing that we've been thinking about more and more is think this was a great idea somebody here had, like, it'd be nice to write out what the desired behavior of a model is, make that public, take input on it, say, you know, here's how this model's supposed to behave and explain the edge cases too. And then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended and you should debate the policy. And right now it can sometimes be caught in between, like, black Nazis, obviously, ridiculous, but there are a lot of other kind of subtle things that you can make a judgment call on either way. you know, the Google's AI principle is a very high level. That doesn't that's not what I'm talking about. That doesn't work. Like, I'd have to say, you know, when you ask it to do thing x, it's supposed to respond and wait y.
Unknown: So, like, literally, Because there's this anecdotal data that people tech in general as well. Do you feel the pressure of that with the within a company that there's like a lean
Sam Altman: that affects the product, that affects the teams? I feel very lucky that we don't have the challenges at OpenAI that I have heard of at a lot of other companies. I think think part of it is, like, every company's got some ideological thing. We have one about AGI and belief in that, and it pushes out some others. Like, we are much less caught up in the culture war than I've heard about at a lot of other companies. OpenAI as I'm sure it does in all sorts of subtle ways, but not in the obvious. Like, I think we of the technical alignment work. It'll be societal impacts, economic impacts. It it'll it's it's not just like we have one team thinking about how align the model, and it's it's really gonna be like getting to be getting to the good outcome is gonna take the whole the whole effort.
Unknown: What aspect of the leap and sorry to linger on this even though you can't quite say details yet, but what aspects of the leap from GPT four to GPT five are you excited about?
Sam Altman: but I think the really special thing happening is that it's not like it gets better in this one area and worse at others. It's getting, like, better across the board.
Unknown: That's, I think, super cool. Yeah. There's this magical moment. I mean, you meet certain people. You hang out with people, and they you talk to them. You can't quite put a finger on it, but they kinda get you. Like, it's not intelligence, really. It's like it's something else. And that's probably how I would characterize the the progress of GBT. It's not like, yeah, you can point out, look, you didn't get this or that. But it's just to which degree is there's this intellectual connection between, like, you feel like there's an understanding in your crappy formulated prompts that you're doing that it grasps the the deeper question behind the question that you yeah. I'm also excited by that. I mean, of us love being understood, heard and understood. That's for sure. That's a weird even like with the programming, like when you're programming and you say say something or just the the the completion that GPT might do, it's just such a good feeling when it got you, like what you're thinking about. And I look forward to getting you even better. On the programming front,
Sam Altman: I mean, no one programs, like, It's like how get like that last 1% to to close the gap, how hard is that? Yeah. I think with most other cases, the best practitioners of the craft will use multiple tools, and they'll do some work in natural language. And when they need to go, you know, write c for something, they'll do that. that as part of this transition, as this phase change, we also get we will return to robots in some way at some point.
Unknown: That sounds both inspiring and menacing.
Sam Altman: I used to love to speculate on that question. I have realized since that I think it's, like, very poorly formed and that people use extremely definite different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability x or y or z rather than, you know, when we kind of, like, fuzzily cross this one mile marker. It's not like like AGI is also not an ending. It's much more of a it's closer to a beginning, but it's much more of a mile marker than either of those things. And but what I would say in the interest of not trying to dodge a question is I expect that by the end of this decade and It maybe changed the world's expectations for the future, and that's actually really important,
Unknown: Like, singularity level transition? No. Definitely not. But just a major like the Internet being like a like Google search did, I guess.
Sam Altman: This is personally important to me. I don't know if this is the right definition. I think when a system can significantly increase the rate of scientific discovery in the world, that's like a huge deal. I believe that most real economic growth comes from scientific and technological progress.
Unknown: actual rate, like, measurable rate of scientific discovery. But even just seeing a system
Sam Altman: find it surprisingly difficult to say what I would ask, that I would expect that first AGI to be able to answer. Like, that first one is not gonna be the one which is like, go, like,
Unknown: Are you interested in that side of things too? The formalized exploration of ideas?
Sam Altman: I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I nor any other one person have total control over OpenAI or over AGI, and I think you want a robust governance system. board drama from last year, it does make it harder for me to, like, look you in the eye and say, hey. The board can just fire me. I continue to not want supervoting control over OpenAI. I never have, never had it, never wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should be making these decisions and that we really need governments to put rules of the road in place, and I realize that that means people like Marc Andreessen or whatever will claim on going for regulatory capture, and I'm just willing to be misunderstood there. It's not true, and I think in the fullness of time, it'll get proven out why this is important. I think I have made plenty of bad decisions for OpenAI along the way, and a lot of good ones, and I am proud of the track record overall, but I don't think any one person should, and I don't think any one person will. I think it's just like too big of a thing now and it's happening throughout society in a good and healthy way. I don't think any one person should be in control of an AGI, that would be or or or or this whole movement towards AGI,
Unknown: this idea that the board can fire you is legally true, but you can and human beings can manipulate the masses into overriding the board and so on. But I think there's also a much more positive version of that where the people still have power. So the board can't be too powerful either.
Sam Altman: is a thing that can really, like, take over how people think about this problem. And there's a big group of, like, very smart, I think very well meaning AI safety researchers that got super hung up on this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that, because I think we do need to think about this more. But I think it pushed aside, it pushed out of the space of discourse a lot of the other very significant
Unknown: I think it's the same reason. There's, like, this poet, e Cummings, that doesn't mostly doesn't use capitalization
Sam Altman: as I think most Internet kids didn't, or maybe they still don't. I don't know. has gone down over time. Like, you read, like, old English writing, they capitalized a lot of random words in the middle of sentences, nouns, stuff that we just don't do anymore. I personally think it's sort of like a dumb construct that we capitalize the letter at the beginning of a sentence and of certain names and whatever, but, you know, I don't that's fine. And then what I and I used to, I think, even like capitalize my tweets because I was trying to sound professional or something. shorter form, less formal stuff has slowly drifted to, like, closer and closer to, like, how I would text my friends. If I, like, write if I, like, pull up a Word document and I'm, like, writing a strategy memo for the company or something, I always capitalize that.
Unknown: that have a momentum, Given Sora's ability to generate simulated worlds, let me ask you a pothead question. Maybe a simulated world generated by an AI system.
Sam Altman: it that I guess that was not a big update.
Unknown: The fact that you can generate worlds, they're novel. They're based in some aspect of training data, like, when you look at them, they're they're novel.
Sam Altman: very simple sounding, but very psychedelic insights that exist sometimes. So the square root function. to a child and exists by even, like, you know, looking at some simple geometry, then you can ask the question of what is the square root of negative one. the lowly square root operator can offer such a profound for me, the fact that Sora worked is not in the top five.
Unknown: I do think, broadly speaking, AI will serve as those kinds of gateways at its best, That's pretty exciting. I haven't done Ayahuasca before, but I will soon. I'm going to the aforementioned Amazon jungle in a few weeks. Excited? but it's also nature and it's the machine of nature. And you can't help but appreciate the machinery of nature in the Amazon Jungle because it's just like this system that just exists and renews itself like every second, every minute, every hour just in it's the machine. It makes you appreciate like this thing we have here, this human thing came from somewhere. This evolutionary machine has created that and But at the same time, I think I'm pretty confident And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like, and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger.
Sam Altman: know, huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring. you know, one one thing that I wonder about is is is AGI gonna be more like some single brain, or is it more like the sort of scaffolding in society between all of us? You have not had What you know is dramatically different, and scaffolding that we all contributed to built on top of.
Unknown: Yeah. Me too. It's a pretty awesome life. I get to enjoy awesome creations of humans, of which I believe Chad GPT is one of and everything that OpenAI is doing. Sam, it's really