Unknown: We had here. We are bringing together obviously leaders in experts in leading AI to discuss the transformative power of AI both in academia, industry, but also in society. So with this very few introduction of words, let's drive into the conversation and please welcome our panelists, for drafting texts and also summarizing, right? Basically, it helps us to save some time, also, you know, cut corners.
Unknown: steps towards that in areas like for instance pathology or earth observation or others where those foundation models are in the making and an important aspect here. So really using the tools in order to improve programming. So this is, for many scientists, and whatnot, training, we're actually in this space adapting the models with respect to texts, with respect to data that is specific to the domain, right, which is really There's of course also interesting opportunities that one can explore in how to leverage large English models for students, right? Or in general foundation models. And of course, I mean, the obvious use cases that students use nowadays that are text summarization and then we have to look at when some questions are answered by GPT, how do we ensure that the learning effect for the student is still there, right? However, what is really interesting is to have a chatbot as a tutor, right? So a really individualized tutor that the focus of the course. So I think there's lots of potential in that space which will eventually revolutionize teaching to some degree because it will relieve
Unknown: of the news that we had today between OpenAI and Bifold. You have just started basically a research partnership that will allow OpenAI to provide 50 ks worth of API credits to facilitate of the O3 model in this case,
Sam Altman: we'll be able to use it to do incredible things, and you know, we want more science to get done. We want more, we wanna enable researchers to do things they couldn't do before. This is the history of, this is like the long history of humanity. It does feel a little different this time because of what this can enable, but if scientists can do things because they have a crazy high IQ tool and they can focus more on figuring out the right questions to ask, address things quicker, do their search space faster,
Unknown: so on the high level of course, collaborating with a company always grounds the research to some degree, which gives it more practicality and helps to focus on some actual real world problems. There's always a risk if you work on problems just from academia that you are maybe researching in a direction that might miss the core angle of practicality. models to the sciences, and we already talked about that before with Sam, you know, foundational models in pathology, foundational models in quantum chemistry and others, right? So how can one leverage the systems in order to, for very interesting scientific disciplines, which is one of the cool things of BioWalt, come up with novel insights. There's a huge promise for personalized medicine, there's a huge promise for improving care, we talk about we are just starting to touch those. On the other hand, Pivot, we are also doing systems research and I'm a data management person. So that is data curation, data integration, just the preparation steps. Because if we look at how our model is trained, so we have all those different steps, how can we make those reproducible and documented? look at the entire process that includes both systems to make things scalable, and this is one thing we are doing in BioWorld, conducting research on more clever algorithms, on systems architectures, on also programming models that help us with efficiently writing AI applications. And on the other hand, of course, algorithms, which is really the machine learning side. As we all know, and probably will touch on that later, training is still quite expensive. So can we, even while maintaining large data sets become more efficient? Because of course we can train on small data and do model composition, which have of course promise and fear, I should argue, but that we still find very exciting.
Unknown: maybe moving on to AGI, artificial general intelligence. Your former colleague Sam in this case, Daria Wamadi, recently wrote an essay where he talks about his vision on next generation AI, basically AGI, and he mentions referring to AGI, he says it's basically a country of geniuses in a data center.
Sam Altman: in the next couple of years that many people will look at and say, ten years of science in a year or whatever. That's further out, but that will be the moment where I think the world really kind of changes much more quickly and gets huge benefit. one thing I've learned in general is whenever you see these steep exponential curves in technology, you shouldn't bet against them, and I think you should all be very skeptical when people start saying this is about to run out or that's about to run out or we're gonna hit this limit. It does look to us like we have this fundamental The only thing that matters is the amount of utility it provides to people. So, if it's useful to you, if it's useful over some threshold, if it's useful to you, great. And if a bunch of experts want to debate if it's AGI or not, I don't think it matters at all. I think, and again, I think it is gonna be just this continuous exponential and this continuous rising utility, and the only question is how much is it helping people, how much is it helping the world.
Unknown: probably a moving target, so we will see that over time as the technology progresses, right? AGI will move forward and forward, but indeed the usefulness
Unknown: that the individuals who are using and getting the benefits out of it are the ones who could, yeah, could be the ones who can Okay, switching gears a little bit towards investments in AI. There have been exciting times also in The US. OpenAI has recently announced a big Stargate project, a new company that will invest around $500,000,000 four years, But basically, apart from the big pile of money, what will Stargate do you think enable
Sam Altman: in this case, the joint venture? Yeah, there are two things. One, with a much bigger computer, we can train much better models. And I think the world doesn't want us to stop at GPT five or GPT six, but would like to see that keep going. And I think Stargate will enable us to stay on the same curve of improvement we've been on for the last few years for the next few, and should get us to, I think it'll get us to extremely capable models. The other thing is people really wanna use these systems a lot. As we make more progress, we continue to drive the cost down. Roughly speaking, every year we can take last year's intelligence And also, as we train these truly capable models that can use a huge amount of compute inference time, you might really be willing to go use quite a lot of compute to cure cancer. And so the amount of compute we think the world is gonna demand to run these services is going up pretty steeply, and if you look at our own inference consumption, it's going like that. and they use them a lot, and they want to use them even more.
Unknown: particularly in the SMEs here in Germany that could be leveraged from production processes, the customer databases. There's lots of things that are currently not harvested at the same scale, and I think there's a huge opportunity here to do that of course there's the hardware scale and all that stuff, but I think a key aspect is really leveraging the data, and I think we have to get there.
Sam Altman: Germany is an incredible market for us, it's the biggest market in Europe, it's top five globally, people are doing incredible work with the tool. I'm very convinced that most Europeans want AI to be used here, to happen here, they want it to revitalize economic growth, they want to drive science, they want European infrastructure, and so do we. We would love to do like a Stargate Europe. We need help, but we'd love to build something in Europe for Europe to govern and operate someday, I think that'd be great. really important to the world, and I think Europeans will do great stuff with the technology. I think the European people will have to decide how they want to set the rules for this technology, we'll obviously comply with whatever they are, but, and obviously I'm biased here, I think it is in Europe's interest to be able to adopt AI and not be behind the rest of the world. But, you do have to decide. Like, if there are benefits to different regulatory regimes and there are gonna be
Unknown: that we do not cut ourselves off from technologies and that we have the ability to experiment, to try out and explore. And of course, there sometimes I think regulation comes prematurely and then there's a huge danger to that of cutting off the economy, cutting off markets, and in general,
Unknown: And at the university, in general, in science, open source and open data is extremely important in relation to something we discussed earlier, explainability in AI, but also reproducibility. So, I guess after the release of DeepSeekR, you recently also said yourself, I think on Reddit, that OpenAI has been on the wrong side of history in regard to open source. So, what does this release actually, or in general open source, mean for OpenAI in terms of competitiveness,
Sam Altman: We can debate whether or not AGI should be open sourced,
Unknown: There's a couple of different aspects from a scientific perspective. Of course, it's very valuable to have all three open because this allows further exploration, this allows learning, this allows for also improving scientifically. It's of course understood that companies that invest and it may help with explainability even. So I think this maybe they became open source. It's also sometimes a marketing strategy for effective followers and whatnot, right? There's also issues about that. But I would say that in the future we'll probably see more open source models, more hopefully open source training processes and data in specific domains, in particular in the sciences. Make things open. And this is what in my opinion
Unknown: a direct question. I'm wondering if you think also that this has kind of made the field now more even, basically, in terms of also companies, I guess, energy consumption. Something that I think a lot of also in the audience are thinking about. So, guess until recently, AI companies were more focused on training But we have now the shift, and again we chatted about it earlier, towards more reasoning models, right? And to arrive basically at better answers, these models are using increasing computation also at inference time.
Sam Altman: answer back, yeah, probably we're more efficient than asking a human and the food they need to eat to run their own, they're getting super super efficient. there was this sort of moral panic because people said, oh, you're like, to ask that one question, And I remember thinking, well, what that's replacing is someone like turning on their car engine, driving for fifteen minutes to the library, looking something up, driving back. And even if they do it a 100 more times because the friction is lower, it's way less energy use than before. AI today uses a tiny amount of the world's energy, it's actually quite efficient per query. And the framing that I'm most interested in, like there is totally a world where we say, we're gonna ban AI, it uses too much energy. We're gonna ban computers, we're gonna ban light bulbs or ad, and we're gonna sit in the dark and we're not gonna burn any energy. That is a world we could choose. There's another one where we say we're gonna use AI, and even if we have to use hundreds of megawatts or gigawatts on this problem, if we can use AI to discover how to do efficient fusion, cheap fusion, and then very quickly replicate the thousands of gigawatts of generating capacity that's burning carbon around the world, that would be a huge win. We clearly have failed to do that fast enough on our own without AI. Let's try it with AI. I think fusion will be the way that most energy on earth is generated in another couple of decades. I think we're gonna be totally fine.
Unknown: assessing the importance of high quality data,
Unknown: look at how we use the data efficiently and the models, but there's because that might bring things down, there may be improvements in hardware that will help us as well, in addition to of course using what we have now. But this is of course the role of science, to bring us forward here, and we can do this algorithmically and we had some discussions about already coming over overcoming the quadratic complexity of training, right? So this would be a huge win already for energy efficiency if one can bring that down to, let's say, n log n. There might be other models than the attention mechanism and transformers in the future that will help us to improve on that, right? And this is also active research, this active research that we are conducting in our groups and many people around the world are looking into that. At the same time, of course, I'm not a hardware person, but improvements in hardware are another aspect where energy consumption will hopefully go down eventually. And so it's a holistic thing, and we have to look at this holistic process. But where I agree with Sam also is to look at energy consumption holistically. AI has a cost, but it also has a benefit that trades off the energy consumption. And I think in this way, looking at this holistically, sometimes one might otherwise forget the benefits. whatever AI does, it helps to solve it and not make it worse.
Unknown: Would you expect the first foundational models that exhibits AGI to be able to self replicate, self improve, self modify, for example, based on an emergent need? And I guess maybe more importantly, even if you expect that, would OpenAI even be aware of this, or companies be aware of this desire? you see in companies like OpenAI have in reporting this and its potential social impact?
Sam Altman: I will count it personally as AGI well before that milestone. think it'll feel sort of gradual. People talk about self improvement as this very binary thing, but really it'll be like, it can help the scientist work a little faster, and then a lot faster, So, don't think it's a clean binary, but we'll see something about the rate of input, the slope changing.
Unknown: different questions, so what we did actually, we clustered these questions and selected them from topics as representative for these questions, and I will be reading some of these questions and asking this to the audience. We will The first question comes from Oliver, and Oliver says, I'm curious to hear the panellists take on progressing society and economy with products that work like magic, that is we don't understand what they work on, an analogy, how could we have succeeded with building cars that come out of a black box factory. So how can we basically trust also these models that are all black box for us currently, in terms of explainable AI?
Sam Altman: what an input will, like that a particular input will give me a particular output, I can use incredibly complex systems, like maybe I understand how a car works, I think we do need to have a high level of robustness and predictability and generally accepted safety for AI models, but also, users are sophisticated, and they understand, like again, a lot of people use ChatGPT for important stuff, and on the whole they understand super well when to trust it and when not to. And yeah, probably most of them couldn't go train it, but that's okay. That's the economic value exchange of the world. Some people build something, they understand enough about how it works. We don't understand it perfectly, but we understand it surprisingly well.
Unknown: in systems, right? And we traditionally build engineering systems, we design them, we try to make things provably correct, look at many different aspects in order to ensure that we can use them. And the way I see it, with AI, we are not yet there, obviously, right? And so the question is for what kind of mission critical AI in this context. But personally, I would say there is still some areas where
Unknown: Veronik asks, how can we ensure that AI only uses trustworthy data for training, congruent with the fair data principles, findable
Unknown: having some forms of validation, and there's of course also semi automatic ways to do that, right? There's a lot of research in the areas of information extraction, information integration, some of which again using machine learning models to do so. One has to apply those, right, and have checks and balances to validate. So, of course, I would imagine that in many cases one does that. So, for instance, when one builds foundation wells for specific copper, like pathology, or whatever, you look at having those kinds of sources.
Sam Altman: Better algorithms, bigger computers, harder data sets to train on, building better products that help us learn more from feedback with users about what's not working, but I cannot overstate how much progress I think we're gonna make in the next two years. Like, it is We know how to improve these models so so much, and there's not an obvious roadblock in front of us.
Unknown: multimodal data and some of the more sparse data from specific application domains in a meaningful way. So I think this will be quite
Sam Altman: think the less obvious things are adaptability and resilience, these are gonna be hugely important skills. These are super learnable and super practical, they get much better with practice. is gonna be really important, and humans are so hardwired to care about other people and what other people create, and I think that's gonna keep going. You know, some people say it's figuring out what questions to ask, but I really think the answer is figuring out what other people want, what's gonna create value, how do you like, how are you gonna direct the AI? And one of my, I used to be a startup investor at this firm called Y Combinator, and one of my biggest takeaways from that is the degree to which people can learn this skill is much higher than I would have thought, and something that I think we should teach.
Unknown: answer first, because this is a very common question and my answer is always the same. The answer is, So, instance, in computer scientists, as Samurai said, programming but also a specific specialization area, be it machine learning algorithms or data systems or robotics or whatever, So you should have one specialization area that you know in-depth. And even if some of the knowledge that you learned in-depth is eventually outdated, you will still be able to have learned how to adapt, how to carry it over, which was the other tiltability that was just discussed about. And this was an interesting aspect, so you have depth knowledge and breadth knowledge. And when we had Eric Schmidt here, we had exactly the same question, then but you also need to have some technical depth in combination to that. And that's why I would say always follow this guideline of the T shaped student. We're really seeing a time of drastic changes in the scientific process support through AI systems, how that can be leveraged. So I'm really excited about that. I'm also excited about new systems developments. So I think it's an exciting time in general to be a researcher in the field of AI and data because there's lots of opportunities, opportunities for entrepreneurship, So there's some options that we have there and I'm sure you will talk to that as well. And there's lots of options for deep foundational research because it's still, even though it's evolving, it's still a very open field in many areas and I just can encourage many of you to join this endeavor, either in academia or as entrepreneurs or in industry because it's