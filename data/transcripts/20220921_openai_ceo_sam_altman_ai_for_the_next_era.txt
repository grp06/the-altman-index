Sam Altman: So I think so far we've been in the realm where it's you you can do like an incredible copywriting business or you can do like a sort of like education service or whatever. And I think people are really starting to think about like how do the fundamental things change and that's gonna be really powerful. chatbot interface that actually works this time around, like I think like many of these trends that we all made fun of were just too early. Like the chatbot thing was good, was just too early. Now it can work and I think, you know, having like new medical services that are done through that where you get great advice or new education services like this these are gonna be very large companies. I think we'll get multimodal models in not that much longer and that'll open up new things. I think people are doing amazing work with sort of agents that can use computers to do things for you, use programs. And this idea of like a language interface where you know, you say in natural language what you want in this kind of like dialogue back and forth. You can iterate and refine it and the computer just does it for you. You see some of this with like DALL E and Copilot in very early ways. with this as the interface and more generally that like these very powerful models will be one of the genuine new technological platforms which we haven't really had since mobile. And there's always like an explosion of new companies right after. large models out there that other people build on. But right now what happens is, very large model of the future and tune it, And then those those companies will create a lot of enduring value because they will have like a special version of they won't have to have created the base model, but they will have created something they can use just for themselves or share with others that has this unique data flywheel going that sort of improves over time and all of that. So I think there will be a lot of value created in that middle layer. I think the biggest like, systemic mistake in thinking people are making right now is they're like, alright, you know, maybe I was skeptical but this language model thing is really gonna work and sure, like, images, video too. But but it's not gonna be generating net new knowledge for humanity. It's just going to, like, do what other people have done and, you know, that's still great. That still like One is there are these science dedicated products, whatever, like AlphaFold and those are adding huge amounts of value and you're gonna seeing this like The anyway but there's like another thing that's happening which is like tools that just make us all much more productive, that help us think of new research directions, that sort of write a bunch of our code so you know we can be twice as productive. And that impact on like the net output of one engineer or scientist I think will be the surprising way that AI contributes to science that is like outside of the obvious models. But even just seeing now like what I think these tools are capable of doing, Copilot as an example, just, you know, it'd be much cooler stuff than that, that will be a significant like change to the way that technological development, scientific development happens. But then so those are the two that I think are like huge now and lead to like just an acceleration of progress. But then the big thing that I think people are starting to explore is I hesitate to use this word because I think there's one way it's used which is fine and one that is more scary. But AI that can start to be like an AI scientist and self improve. And so when like can we automate our own jobs as AI developers very first? The very first thing we do. Can that help us solve the really hard alignment problems that we don't know how to solve? Like, that honestly I think is how it's gonna happen. you know, editing your own code and changing your optimization algorithm and whatever else. But there's a less scary version of self improvement which is like kinda what humans do. Which is if we try to go off and like discover process we do that is like special to humans, teaching AI to do that, I'm very excited to see what that does for the total I'm I'm a big believer that the only real driver of human progress and economic growth over the long term is the the structure, the societal structure that enables scientific progress and then scientific progress itself. And either in conflict with ours, many sci fi movies about what happens there, or goals where it just like doesn't care about us that much. And so the alignment problem is how do we build AGI does The way that I think the self improving systems help us is not necessarily by the nature of self improving but like we have some ideas about how to solve the alignment problem at small scale and We have some ideas about what to do next. I'll start with like the higher certainty things. I think language models are going to go just much much further than people think and we're like very excited to see what happens there. that we're going to have a very exciting time. Another thing is I think we will get true multimodal models working. And so not just text and images but every modality you'd like in one model able to easily I think we will have models that continuously learn. So very excited about all of that. And if you just think about what that alone is going to unlock and the sort of applications people will be able to build with that, would be like a huge victory for all of us and just like a massive step forward and a genuine technological revolution if that were all that happened. But I think we're likely to keep making research progress into new paradigms as well. We've been like pleasantly surprised on the upside about what seems to be happening. And I think, you know, all these questions about like new knowledge generation, how do we really advance humanity, AI has become like the mega buzzword, But historically, that's a very bad sign for new startup creation or whatever. Everybody is like, I'm this with AI, and that's definitely happening now. So a lot of the you know, we were talking about, like, are there all these people saying I'm doing these RL models for fusion or whatever, and as far as we can tell, they're all much worse than what you know, smart physicists have figured out. I think it is just an area where people are going to say Many things will be true. I do think this will be like the biggest technological platform of the generation. But I think it's like we like to make predictions where we can be on the frontier, is do the next thing in front of us when we have high confidence and take 10% of the company to just totally go off and explore which has led to huge wins and there will be wait like And But I think that's My basic model of the next decade is that the cost of intelligence, the marginal cost of intelligence and the marginal cost of energy are going to trend rapidly towards zero, like surprisingly far. And those I think are two of the major inputs into the cost of everything else except the cost of things we want to be expensive, the status goods, whatever. And because these seismic shifts that happen when the whole cost structure of society changed, which happened many times before, the temptation is always to underestimate those. So I wouldn't make a high confidence prediction about anything that doesn't change a lot or that doesn't get to be applied. But one of the things that is important is it's not like the thing trends I I would bet that container for software and you know, a new way a new computer interaction thing. And AI turns out to be something on the order of like a legitimate technological revolution. So I think the currently available models are kind of not good enough to have like made a big impact on the field. At least that's what like most life sciences researchers have told me. They've all looked at it and they're like, it's a little helpful in some cases. this is one of these areas where there will be these like, you know, new 100,000,000,000 to trillion dollar companies started and those areas are rare. But like when you can really change the way that if you can really make like a future of pharma company that is just hundreds of times better than what's out there today, that's going be really different. As you mentioned, there still will be the rate limit of like bio has to run at its own thing and human trials take however long they take and that's so I think an interesting cut of this is like where can you avoid that? Like where are the synthetic bio companies that I've seen that have been most interesting are the ones that find a way to make the cycle time super fast. I'm a huge believer for startups that like the thing you want is low costs and fast cycle times. And if you have those, can then compete as a startup against the big incumbents. And so I wouldn't go pick like cardiac disease as my first thing to go after right now with this new kind of company. But, you know, using bio to manufacture something, that sounds great. I think the other thing is the simulators are still so bad. And if I were a bio meets AI startup, I would certainly try to work on that somehow. Sort of all of the deep biological things. Like, I think we will still really care about interaction with other people. Like, we'll still have fun and like the reward, you know, systems of our brain are still going to work the same way. We're still going to have the same drives to kind of create new things and compete for silly status and form families and whatever. So I think the stuff that people cared about you know, we turn our focus to like exploring and understanding the universe as much as we can. there's not like one sci fi universe that I could point to and say, think all of this is great. But like the collective optimistic corner of sci fi which is like a smallish There are people who say, The EA community is like I'm not doing that because they're all going to die. The kind of like techno optimists are like Well, it's just like I want to merge into the AGI and go off exploring the universe and it's going be so wonderful and I want total freedom. But I think all of those I find quite depressing. I think having a lot of kids is great. But you will just like, you know, either with text or voice depending on the context, you will just like interface in language and get the computer to do whatever you want. And that will apply to generate an image where maybe we still do a little bit of prompt engineering but it's kind of just going to get it to go off and do this research for me and do this complicated thing or just be my therapist and help me figure out how to make my life better, or like, you know, go use my computer for me and do this thing, or any number of other things. But I think the fundamental interface will be natural language. about going that extra mile. A 100%. I just hope it's not like figuring out how to hack the prompt by adding one magic word to the end that changes everything else. I like What will matter is the quality of ideas and the understanding of what you want. So the artist will still do the best with image generation, AGI is basically the equivalent of a median human that you could like, you know, hire as a coworker. So and then they they could like say do anything that you'd happy with a remote coworker doing, like just behind a computer. Which includes like, you know, So for me, that's kind of like AGI. And then superintelligence is one that's like smarter than all of humanity put together. it is as divergent as I think it could be for some people doing incredibly well and others not, I think society just won't tolerate it this time. And so figuring out when we're going to disrupt so much of economic activity, and even if it's not all disrupted by twenty or thirty years from now, think it'll be clear that it's all going to be. What like what is the new social contract? Like how my guess is that the things that we'll have to figure out are how we think about fairly distributing wealth, access to AGI systems which will be like kind of the commodity of the realm, and governance like how we collectively decide what they can do, what they don't do, things like that. But I do think like the concept of wealth and access and governance, those are all going to change and how how we address those will will be huge. So you've initiated some research Yeah. On this stuff. Yeah. So we run the largest UBI experiment in the world. you know, I think like we should have like 10 more things like that that we try. We've explored more recently like how this technology can be used for reskilling people that are gonna be impacted early. I I think and I think we're seeing this now that tools for creatives are that that is gonna be like the great application of AI in the short term. People love it. It's really helpful. And I think it is, at least in what we're seeing so far, not replacing. It is mostly enhancing. It's replacing in some cases but for the majority of the kind of work that people in these fields want to be doing, it's enhancing. And I think we'll see that trend continue for a long time. Eventually, yeah, it probably is just like, you know, we look at a 100 years. Okay. It can do the whole creative job. I think it's interesting that if you asked people ten years ago working in the factories, truck drivers, whatever. Then it will come for the kind of like the low skill white collar jobs, then the very high skill like really high IQ white collar jobs like a programmer or whatever, and then very last of all and maybe never, it's gonna take the creative jobs. And it's really gone exactly the and is going exactly the other direction. And I think this like there's an interesting reminder in here generally about how hard predictions are, but more specifically about, you know, we're not always very aware, maybe even ourselves, of like what skills are hard and easy. Like what uses most of our brain and what doesn't or how like difficult bodies are to control or make or whatever. How would the startup differentiate from another? How would one large language model startup differentiate from another? I think it'll be this middle layer. I think in some sense the startups will train their own models, just not from the beginning. They will take like, you know, base models that are are like hugely trained with a gigantic amount of compute and data and then they will train on top of those to create the model for each vertical. Those start up so in some sense they are training their own models, just not from scratch. They're doing the 1% of training that really matters for whatever this use case is going to be. Those startups I think, they will be hugely successful and very differentiated startups there but that'll be about the kind of data flywheel that the startup is able to do, the kind of all of the pieces on top and below. Like this could include prompt engineering for a while or whatever. The sort of the kind of like core base model. I think that's just going to get too complex You may as well like you gotta you gotta like plan for the worst. Certainly like, it's not a strategy to say it's all gonna be okay. But you may as well like emotionally