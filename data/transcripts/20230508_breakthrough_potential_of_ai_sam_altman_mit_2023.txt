Sam Altman: mean, I I think I hope this will be like a, you know, a page or a chapter in history books, but I think that over the, you know, next several billion years, such unbelievable things are going to happen that this will be just sort of like, you know, one small part, there'll be new and bigger and more exciting opportunities and challenges in front of us. Yep.
Unknown: open source iterations, you had a whole variety of ways of taking that source code and making a vertical company out of it. Or an adjacent company, something a federated In learning or the future iteration of these companies,
Sam Altman: It doesn't nothing lets you off the hook for building a product that people love, for being, like, very close to your users, fulfilling their needs, for thinking about a long term durable business strategy. Like, that's actually probably only more important during a platform shift, not not less. Like, if if we think back to the launch of the App Store, which is probably the most recent similar example, there were a ton of companies that built very lightweight things with I don't wanna call them, like, exploitative mechanics, but just like, you know, it was it was not something durable. And the tech the technology is just like, this is a new enabler.
Unknown: And then what about foundation models just as a starting point? You know, if I look back two years, one of the best ways to start was to take an existing foundation model, maybe add some layers and retrain it in a vertical use case.
Sam Altman: We're we're still trying to figure out exactly what developers want in terms of model customization. But people are doing pretty amazing things with the base model and for a bunch of reasons often seem to prefer that. So we're we're, like, actively reconsidering what customization to prioritize given what users seem to want and seem to be making work.
Unknown: of the specifics, the trend is toward as the models are getting bigger and bigger and bigger, so you go from 1,000,000,000,000 to 10,000,000,000,000 parameters, the amount you can achieve with just changing prompt engineering or changing the the tokens that are feeding into it is growing disproportionately that he's ever encountered. And of course, all these people are are wrestling with their business strategy and what exactly to build and where. And so I'm I've been asking you questions that are more or less vertical use cases that sit on top of GPT four and Chatea and soon GPT five and so on. But there's also all these business models that are adjacent. So things like federated learning or data conditioning or just deployment and and that's in the ecosystem,
Sam Altman: case, I I I don't think it'd be true. I think there are people who are, like, unbelievable business strategists, very long time horizon capital intensive, difficult technology bets.
Unknown: A couple questions about the underlying tech platform here. So I've been building neural networks myself since the parameter count was sub 1,000,000. And then GPT-three and now GPT-four. in GPT-three and it was just mind blowingly different from GPT-two and then GPT-four is even more mind blowingly different. So the raw underlying parameter count seems like it's on a trend, just listening to NVIDIA's forecast, and then they're saying up to 10 in a decade. So you've got
Sam Altman: But this reminds me a lot of the gigahertz race in chips in the, like, nineties and February Yep. Where everybody was trying to, like, point to a big number and then event, like, you don't need probably, most of you don't even know how many gigahertz are in your iPhone, but it's fast. Yep. Like, what we actually care about is capability, and I think it's important that what we keep the focus on is rapidly increasing capability. And if there's some reason that parameter count should decrease over time or we should have, like, multiple models working together, each of which are smaller, we we would just do that. Like, what we wanna deliver to the world are the most capable, useful, and safe models.
Unknown: is that surprises you with raw horsepower regardless of whether you measure it in parameter count or some other way. It does things that you didn't anticipate purely by putting more horsepower you coded it up, you run it on a computer that's 10,000 times faster, it doesn't really surprise you. It's nice and responsive, it's still a spreadsheet. Whereas this class of algorithm does things that it just couldn't do before. society benefiting use case
Sam Altman: I I think in new areas like this, one of the right approaches is to let tactics become strategy instead of the other way around. I think you never wanna lose sight of vision and focus on the long term, but a very tight feedback loop of paying attention to what is working and what is not working and doing more of the stuff that's working and less of the stuff that's not working and just very, very careful user observation can go super far. So, like, you know, I can speculate on ideas. You all can speculate on ideas. None of that will be as valuable as putting something out there and really deeply understanding what's happening and being responsive to it. Mhmm. long that it's like you kind of know with gradually increasing to to really try to understand what's going on and mitigate as much as you can. That's important. that we totally I totally agree with. I also agree that as safety cape as as capabilities get more and more serious, the the safety bar has got to increase. But, unfortunately, I think the letter is missing, like, most technical nuance about what's where we need the pause. Like, it's actually but, like, OpenAI, an earlier version of the letter claimed that OpenAI is trained in g p d five right now. We are not and won't for some time. But I think this technology is going to so impact all of us that we believe that engaging everyone in the discussion, putting these systems out into the world, deeply imperfect though they are in their current state, so that people get to experience them, think about them, understand the the upsides and the downsides. It's worth the trade off even though we do tend to embarrass ourselves in public and have to change our minds with new data frequently. So we're gonna keep doing that because we think it's better than any alternative. And a big part of our goal at OpenAI is to, like, get the world to engage with this and think about it and and and gradually update and build new institutions or adapt our existing institutions to be able to figure out what the future we all want is.
Unknown: the process toward infinity or in the singularity view of the world to absolute infinity. And so now a lot of the companies that I'm an investor in or have been co founder of are starting to use LLMs for cogeneration. And it's interesting, very wide range of lift or improvement in the performance of an engineer, ranging from about 5% to about 20 x.
Sam Altman: or or not. I think what will happen is that long it takes it will not work out like it will not somehow like, most of these things don't end up working out quite like the sci fi books, and neither will this one. But the rate of change in the world will increase forevermore from here as humans get better and better tools.