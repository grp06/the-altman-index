Unknown: Hey, everyone. Really excited to jump into our next fireside chat with Sam Altman. Sam is the CEO and cofounder of OpenAI and the former president of Y Combinator. Sam and I first met when Scale was actually going through the Y Combinator program back in 2016. Sam is one of the people in technology who works on the most interesting set of problems in both technology and hard tech between OpenAI has had some incredible What do you think sets OpenAI apart from other AI research organizations?
Sam Altman: combination of we are we are good at research, engineering, and the sort of safety policy planning thing that think tanks usually do. And we have all of those in one small organization that on a per head count basis is extremely well resourced and extremely focused. So we're willing to concentrate a lot of resources into a small number of bets and bring these sort of three different pieces together to do that. You know, we sort of we have a plan that we think goes from here to AGI. I'm sure it'll be wrong in important ways. But because we're because we have such a plan and because we're trying to think about how the pieces fit together and we're willing to make high conviction bets behind them, that has let us make, I think certainly relative to our size and capital, outsized progress.
Unknown: You know, one question I have is how how intentional was this, you know, magical mix of of multidisciplinary
Sam Altman: most of the groups were really strong in one of the area, maybe one and a half, but no one in all three. And so we very consciously, like, we call those the three clans of OpenAI, have always wanted to be good at all three. But then the other thing is I just, like, I think really talented people that are focused together in one not only one vision, but one plan to get there is, like, that is the rarest commodity in the world. And, like, the best people are are, you know, Steve Jobs used to say this, I think, more eloquently than anyone else, but the best people are so much better than the pretty good people that if you can get a lot of them together in a super talent dense environment, you can sort of surprise yourself on on the upside. The the sort of the central learning of my career so far has been the exponential curves are just super powerful, always under almost always underestimated, and usually keep going. And so in some sense, like, And so that, like, the miracle was already behind us when we started, and it then became a process about doing, like, a really good job at just executing on the engineering, sort of figuring out the remaining research breakthroughs, and then thinking about how it all comes together in a way that is good for the world, hopefully.
Unknown: Sam, you're you're such a good student of history, especially in terms of, you know, the the situations and the events that led to many incredible innovations in the past, like the Internet or GPS or even the computer originally.
Sam Altman: stupid not to try to take those into account. But I think the most interesting learnings and the most interesting way to think about it is, like, what about the shape of this new technology? What about the way that this is likely to go is going to be super different than what's come before? And how do we solve this And
Unknown: You mentioned that Open AI, you know, part of the strategy has been to be relatively concentrated. Pick a small number of bets Yep. That that you guys have high conviction on. How how do you go about picking these bets, and what what represents a good bet versus a bad bet?
Sam Altman: Do more of what works is part of the answer. And I think weirdly, most research labs have a do less of what works approach. You know, there's this like thing of like, oh, like once once you once it works, like it's no longer innovative, we're gonna go look for something different. We just wanna build AGI And we try to pursue a portfolio of research bets that advance us to those
Unknown: you know, AGI is this thing that, at least theoretically, is certainly possible because, you know, it's to accomplish it through our brains. And and there's this interesting question of like, is the what is the technological path to arrive at at AGI actually look like? And obviously, is a this is almost a philosophical question more than a sort of technical, a real technical question. But based on what you know today, all the research you all have done at OpenAI, what you've learned through that research, what do think is the most likely path from here to something that represents AGI?
Sam Altman: Either creating AGI in a computer is a certain possibility. Either physics all works like we think, and we're living in the physical universe, and consciousness is the intelligence is sort of emergent property of energy flowing through, you know, in your case, a biological network and the computers, silicon, but but but it's gonna happen. Or we are living in some sort of weird simulation or we're like a dream in universal consciousness or nothing is like what we think. And in any case, you should live as if it's certainly going to happen. And so I I still find it odd that people talk about like maybe it's possible. It's it's like one I think you should certainly live as if it's possible. In terms of what we need, you know, we don't like to talk too much about unannounced research, but but certainly, I think most of the world, ourselves included, have been surprised by the power of these large unsupervised models and how far that can go. And I think you can imagine combining one of those that sort of understands the whole world and all of human knowledge
Unknown: Yeah. You you In in that answer, you brought up one thing which I think is
Sam Altman: let's sort of say Descartes was right, and you can either, you can say that like, I have this certain knowledge that I I am, I exist, my own subjective experience is happening, and you can't be certain of anything else. So like maybe this is all like you're in a virtual reality game, you're dreaming, it's like some apparition of a god, whatever. Or like it really is just like physics as consensus understanding is, but in that case, like, it's totally possible to to recreate whatever the subjective experience of of self awareness is. And so it's like either you believe that physics is physics or not, but in the or not case then, like, something else is very strange, so who cares? Right.
Unknown: And I think if you have a strong belief in exponentials, then the question for these great technologies is never like yes or no. It's usually when. Yep.
Sam Altman: to sort of, like, maximize value creation in your own life, this is, like, the one. Right? It's understanding these exponential curves. For whatever reason, evolution didn't prioritize this. We're very bad at it. It takes some work to overcome. But if you can do it, yeah, it's super valuable. Makes you look like a visionary.
Unknown: There's actually a lot of neuroscience research that shows that, you know, people in their brains, they have circuits to do all sorts of mental operations, like addition, like subtraction, etcetera. We're very bad at exponentials.
Sam Altman: Do you have a do you have a personal sort of Turing test of something where if it happened, it would be sort of evidence that that we've achieved it? In terms of something that's like people always use this term slightly differently. Something that's like self aware, or something that's just like really generally intelligent that can learn fast, what do you mean by it? education sort of can do anything that humans could do. Yeah. I I think that's actually like not a super hard test. For for what you were just saying, like, there's like a lot of that would be so economically valuable to the world that it will show up that way. And so like once it can start doing some significant percentage of human labor really well, that would pass the test for me. Yep.
Unknown: One topic that's really come up a lot, especially recently, is this topic of responsible and ethical AI. Know? Yeah. I think any powerful technology has the ability to, that will change the world, has the ability to be responsible, ethical, good for the world overall, or bad for the world overall. How do you think about ensuring that the benefits of AI are equally distributed at OpenAI? Yeah.
Sam Altman: Two principles here. One, I think that people that are going to be most affected by technology should have the most say in how it's used and what the governance for it is. I I think that this is something that some of the existing big tech tech platforms have have gotten wrong. And I believe most of the time if people understand the technology, they can express But in the moment, people don't know me included, people don't always have like the self discipline to not get led astray. So I can certainly say that I my best life is not like scrolling all night on my phone, like reading Instagram or whatever. That's pretty good. And then this and I think we'll get pretty good answers. And the second is I really believe in some form, and there's a lot of act asterisks to be placed here, but in democratic governance. If we collectively make an AGI, I think Along on the other side, like, sometimes in the heat of the moment, democracy doesn't make very good decisions. So figuring out how to balance And then, you know, my current best idea, and maybe there's a better one, is some form of a universal basic income or basic wealth sharing or or something where people get to sort of we we we share the benefits of this as as we can.
Unknown: Definitely. You And one of the, you know, one of the super interesting things that's, this has really come up in a lot of we should put as a community into the data sets and the code and algorithms, sort of relatively
Sam Altman: some intersection of data and algorithms, which I suspect will be the case, plus sort of real time human correction and user correction,
Unknown: out of open AI, was this amazing paper, Scaling Laws for Large Language Models. And I think that was the understanding, fireside chat was Drago Ongulov, who's the head of research at Waymo. And one the things that he discussed was there's this natural almost misalignment where, you know, neural networks are very good at optimizing this misalignment that's created by how the technology is developed between what you actually want the system to do and what what your loss function tells the system to do?
Sam Altman: example for how really how we how we design these systems and what optimize them for,
Unknown: are really important. Yeah. So, you know, there's been a number of incredible breakthrough results in the AI research community over the past few years. Many of them coming out of OpenAI, like GP three and CLIF and Dolly. One of the of the trends has been that these breakthroughs require more and more data, more and more compute, and more and more concentrated engineering resources. And so it seems to be the case that the the sort of effort required to do great research is increasing by quite a bit, and the resources required is increasing. How do you think about this impacting kind of the landscape of
Sam Altman: the most impressive sort of useful systems, that does require huge amounts of data and compute. So to like make GPT three or four or whatever, that requires a large and complicated and high amount of various types of expertise effort, and there's only a handful of the companies in the world that can do that. But the fundamental research ideas that make those systems possible most impressive results have started that way and only then scaled up. So but the part that is true is to to sort of scale it up into the maximally impressive system that someone else can use.
Unknown: for, you know, as many researchers and prospective researchers in the audience today to believe that you you should you should certainly believe that it is possible to do great research For sure. Independent of of all these resources. with the the highest performance for other organizations, user requires lots of resources. How do you think that maximal benefit of AI technology?
Sam Altman: need to be some, but like collaboration is always tough. Right? It's always like a little bit slower and a little bit more difficult to get to work than it seems like it should be. And so and then we'll work with this government, other governments,
Unknown: to ask regarding OpenAI is, what are the bottlenecks that you all have experienced in scaling up OpenAI? The organization and the research and results altogether.
Sam Altman: and you sort of just have to accept somewhat more process and planning and slowness in exchange for dramatically more firepower.
Unknown: know, my assumption would be that in the past five years, it's accomplished a lot more than than what you had expected. How have, you know, maybe when you started it, what were your expectations for what would be possible within this time frame? And how how have you done with respect to those?
Sam Altman: And I think it's pretty smart, but the reliability is a big problem. And so maybe we get to something that is like subjectively a 100 times smarter than GPT three, but 10,000 times more reliable. And then that's a system where you can start imagining like way more utility. Also something that can sort of like
Unknown: the field of AI research is is this incredible up leveling of of what it means to do AI research. So, you know, originally, if you were to think maybe twenty years ago, What are some of the the short term use cases of AI that that you think are are sort of right around the corner that that you believe are gonna be very impactful for the world on the whole,
Sam Altman: we just need a lot more access to high quality versions of, you know, everybody should have access to incredible educators, Everybody should have access to incredible medical care.
Unknown: ability to distill human knowledge is really Today, ever before. And if you were to give a few words of direction to this community of people who are all coming into machine learning,
Sam Altman: I will pick only one piece of advice for a young researcher, is think for yourself. but something about the way that the academic system works right now, sort of it feels like everyone should be doing this really novel research, but it seems so easy to get sucked into working what everybody else is working on and that's the whole reward that's what the whole reward system optimizes. Right? And the best researchers I know in this field, but really most others too, are the ones that are of willing to kind of trust their own intuition, to follow their own instincts about what's going to work, and do work that may not be popular in the field right now. Just keep grinding at it until they get it to work.
Unknown: with respect to to AI and everything that's that's been happening today. You know, I think there there's kind of as we discussed before, there's very few mental models that people can have to actually understand, One of the things that that one topic that really interests me is how What are this kind of The changes to the physics of economics that AI will will encourage. was an incredible thing. What do you think are some of the the
Sam Altman: Well, like, you know, the cost of replicated goods went to zero with software. I think the cost of labor for like many definitions of the word labor should go to zero and that makes all the models very weird. So if you had to pick one input to the economic models that is not supposed to be zero, kind of my expectation is that's it. There's been this long standing push of too much power, in my opinion, shifting from labor to capital, but my intuition is that should go way further. And I think, like, most of the interesting economic theory to go figure out is how to counteract that.