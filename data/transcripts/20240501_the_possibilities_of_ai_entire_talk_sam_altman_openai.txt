Unknown: ETL is brought to you by STVP, the Stanford Entrepreneurship Engineering Center and BASIS, the Business Association of Stanford Entrepreneurial Students. I'm Ravi Balani, a lecturer in the management science and engineering department and the director of Alchemist, an accelerator for enterprise startups. And today, I have the pleasure of welcoming Sam Altman to ETL. Sam is the cofounder and CEO of OpenAI. Open is not a word I would use to describe the seats in this class, and so I think by virtue of that that everybody already probably knows Open AI. But for those who don't, Open AI is the research and deployment company behind ChatGPT, DALL E, and Sora. Sam's life is a pattern of breaking boundaries and transcending what's possible both for himself and for the world. He grew up in the Midwest, in St. Louis, came to Stanford, took ETL as an undergrad, and we held on to Sam for two years. He studied computer science, and then after his sophomore year he joined the inaugural class of Y Combinator with a social mobile app company called Looped that then went on to go raise money from Sequoia and others. He then dropped out of Stanford, spent seven years on Looped which got acquired, and then he rejoined Y Combinator in an operational role. He became the president of Y Combinator from 2014 to 2019, and then in 2015 he co founded OpenAI as a nonprofit research lab with a mission to build general purpose artificial intelligence that benefits all humanity. OpenAI has set the record for the fastest growing app in history with the launch of ChatGBT which grew to a 100,000,000 active users just two months after launch. Sam was named one of Time's 100 most influential people in the world. He was also named Time's CEO of the year in 2023, and he was also most recently added to Forbes list of the world's billionaires. Sam lives with his husband in San Francisco and splits his time between San Francisco and Napa, and he's also a vegetarian. And so with that, please join me in welcoming Sam Altman to the stage. And in full disclosure, that was a longer introduction than Sam probably would have liked. Brevity is the soul of wit, and so we'll try to make the questions more concise. But this is also Sam's birth week. It was his birthday on Monday, and I mention that just because I think this is an auspicious moment both in terms of time, you're 39 now, and also place, you're at Stanford in ETL, that I would be remiss if this wasn't sort of a moment of just some reflection and I'm curious if you reflect back on when you were half a life younger, when you were 19 in ETL. If there were three words to describe what your felt sense was like as a Stanford undergrad, what would those three words be? And, you know, a lot has changed in the last nineteen years, but that's gonna pale in comparison to what's gonna happen in the next nineteen. Yeah. And so I need to ask you for your advice if you were a Stanford undergrad today. So if you had a freaky Friday moment, tomorrow you wake up and suddenly you're 19 in inside of Stanford undergrad knowing everything you know, what would you do? Would you I drop would like
Sam Altman: I was, like, coming of age at the luckiest time, like, in several centuries probably. I think the degree to which the world is is gonna change and the the opportunity to impact that, starting a company, doing AI research, any number of things is is, like, quite remarkable. I think this is probably the best time to start I think with what you can do with AI is is, like, gonna just get more remarkable every year. And the greatest companies get created at times like this. The most impactful new products get built at times like this. So I would feel incredibly lucky,
Unknown: I And if you did join on the research side, would you join so we had Kazra here last week who was a big advocate of not being a founder, but actually joining an existing company to sort of learn learn the chops. For the for the students that are wrestling with should I start a company now at 19 or 20, or should I go join another entrepreneurial, either research lab or venture, what advice would you give them? Well,
Sam Altman: since he gave the case to join a company, I'll give the other one, which is I think you learn a lot just starting a company. And if that's something you want to do at some point, there's this thing Paul Graham says, but I think it's like very deeply true. There's no pre startup like there is pre med. You kinda just learn how to run a startup by running a startup.
Unknown: near term challenges that you're seeing in AI that are the ripest for a start startup? And just to scope that, what I mean by that are what are the holes that you think are the top priority needs for OpenAI that OpenAI will not solve in the next three years? Yet? So
Sam Altman: I'm not gonna answer it because I think you should never take this kind of advice about what startup to start ever from anyone. I think by the time there's something that is like the kind of thing that's obvious enough that me or somebody else will sit up here and say it, it's probably like not that great of a startup idea. And I totally understand But I I think like one of the most important things I believe about having an impactful career is you have to chart your own course. If if the thing that you're thinking about is something that someone else is gonna do anyway or more likely something that a lot of people are gonna do anyway, You should be like somewhat skeptical of that. And I think a really good muscle to build is coming up with the ideas that are not the obvious ones to say. So I don't know what the really important idea is that I'm not thinking of right now, but I'm very sure someone in this room does it, knows what that answer is. And I think learning to trust yourself and come up with your own ideas and do the very like non consensus things, like when we started OpenAI, that was an extremely nonconsensus thing to do, and now it's, like, the very obvious thing to do. Now I only have the obvious ideas because I'm just, like, stuck in this one frame. But I'm sure you all have the other ones.
Unknown: that no one else is talking about?
Sam Altman: How to build really big computers. I mean, think other people are talking about that, but we're probably, like, looking at it through a lens that no one else is quite imagining yet. I wrestling with how we when we make not just like grade school or middle school level intelligence, but like PhD level intelligence and beyond, the best way to put that into a product, the best way to have a positive impact with that on society
Unknown: can you share I know there's been a lot of speculation and probably a lot of hearsay too about the semiconductor foundry endeavor that you are reportedly embarking on.
Sam Altman: if if you believe, which we increasingly do at this point, that AI infrastructure is gonna be one of the most important inputs to the future, this commodity that everybody's gonna want, and that is energy, data centers, chips, chip design, new kinds of networks, It's it's how we look at that entire and how we make a lot more of that. And I don't think it'll work to just look at one piece or another, but we we gotta do the whole thing. Okay. So there's multiple big problems. I think like just this is the arc of human technological history as we build bigger and more complex systems.
Unknown: parameters. GBT four was cost $400,000,000 with 10 x the parameters. So it almost four x the cost but 10 x the parameters. if you don't want to correct the actual numbers, if that's directionally correct, does the cost, do you think, keep growing with each subsequent yes? that?
Sam Altman: giving people really capable tools and letting them figure out how they're gonna use this to build the future is a super good thing to do and is super valuable. And I am super willing to bet on the ingenuity of you all and everybody else in the world to figure out what to do about this. So there is probably some more business minded person than me OpenAI somewhere that is worried about how much we're spending, but I kinda don't.
Unknown: OpenAI's phenomenal. ChatGPT's phenomenal. Everything else, all the other models are phenomenal. It burned you burned $520,000,000 of cash last year. That doesn't concern you in terms of thinking about the economic model of how do you actually where's gonna be the monetization source? Well, first of all,
Sam Altman: you know, it's like important to ship early and often. And we believe in iterative deployment. Like if we go build AGI in a basement and then, you know, the world is like kind of blindfolded along, I don't think that's like I don't think that makes us like very good neighbors. So I think it's important given what we believe is going to happen to express our view about what we believe is going to happen. But more than that, the way to do it is to put the product in people's hands and let society co evolve with the technology. Let society tell us what it collectively and people individually want from the technology, how to productize this in a way that's going to be useful, where the model works really well, where it doesn't work really well. Give our leaders and institutions time to react. Give people time to figure out how to integrate this into their lives, to learn how to use the tool. I'm sure some of you all like cheat on your homework with it, but some of you all probably do like very amazing, wonderful things with it too. And as each generation goes on, I think that will expand. And that means that we ship imperfect products, but we have a very tight feedback loop And we learn and we get better. And it does kind of suck to ship a product that you're embarrassed about, but it's much better than the alternative. And in this case in particular, That's how we're gonna do it. And there may be there could totally be things in the future that would change where we think iterative deployment isn't such a good strategy. But it does feel like the current best approach that we have and I think we've gained a lot from doing this and hopefully the larger world has gained something too. Whether we burn 500,000,000 a year or we will be back here. There will be like a new set of students. We'll be talking about how startups are really important and technology is really cool. We'll have this new great tool in the world. It'll feel it would feel amazing if we got to teleport forward six years today and have this thing that was like smarter than humans and many subjects and could do these complicated tasks for us and, you know, like we could have these complicated program written or this research done or this business started. And yet, like, the sun keeps rising. Like people keep having their human dramas. Life goes on. So sort of like super different in some sense that we now have like abundant intelligence at our fingertips.
Unknown: And you mentioned artificial general intelligence, AGI, artificial general intelligence. In a previous interview, you defined that as software that could mimic the median competence of a or the competence of a median human for tasks. Yeah.
Sam Altman: too loose of a definition. There's too much room for misinterpretation in there to I think be really useful or get at what people really want. Like I kind of think what people want to know when they say like what's the timeline to AGI is like when is the world going to be super different? When is the rate of change going to get super high? When is the way the economy works going to be really different? Like when does my life change? And that for a bunch of reasons may be very different than we think. Like, I can totally imagine a world where we build PhD level intelligence in any area and, you know, we can make researchers way more productive. Maybe we can even do some autonomous research. And in some sense, like, that sounds like it should change the world a lot. And I can imagine that we do that and then we can detect no change in global GDP growth for years afterwards, something like that, way more capable than we have right now, one year and every year after. And that I think is the important point. So I've given up on trying to give the AGI timeline, but I think every year for the next many we have dramatically more capable systems every year.
Unknown: And Gang, I know there's tons of questions for Sam. In a few moments I'll be turning it up, so start thinking about your questions. A big focus on Stanford right now is ethics. And can we talk about how you perceive the dangers of AGI? And specifically, do you think the biggest danger from AGI is going to come from a cataclysmic event, which makes all the papers? Or is it going to be more subtle and pernicious, sort of like how everybody has ADD right now from using TikTok? Is it are you more concerned about the subtle dangers or the cataclysmic dangers?
Sam Altman: really serious and a real thing. But I think we at least know to look out for that and spend a lot of effort. The example you gave of everybody getting ADD from TikTok or whatever, I don't think we knew to look out for. am worried just about so even though I think in the short term things change less than we think, as with other major technologies, in the long term I think they change more than we think? And I am worried about what rate society can adapt to something so new and how long it'll take us to figure out the new social contract versus how long we get to do it. I'm worried about that. Okay.
Unknown: into the curriculum the cornerstone of resilience is self awareness, and I'm wondering if you feel that you're pretty self aware of your driving motivations as you are embarking on this journey.
Sam Altman: And in the future, I think over the next couple of decades, I think resilience and adaptability will be more important than it has been in a very long time. So I think that's really great. On the self awareness question, I think I have like a general bias to be too pro technology just because I'm curious and I wanna see where it goes and I believe that technology is on the whole a net good thing. But I think that is a worldview that has overall served me and others well and thus gotten like a lot of positive reinforcement I think at various times in my career, all of those, I think there are these like levels that people go through. At this point, I feel driven by like wanting to do something useful and interesting.
Unknown: Okay. And then where were you when you most last felt most like yourself?
Sam Altman: And this sounds like a dodge, but I think that's, like, among the most remarkable facts in human history that we can just do something and we can say right now with a high degree of scientific certainty, GPT five is gonna be smarter than, a lot smarter than GPT four, GPT six is gonna be a lot smarter than GPT five And we are not near the top of this curve and we kind of know what know what to do. And this is not like it's gonna get better in one area. This is not like we're gonna, you know, it's not that it's always gonna get better at this eval or this subject or this modality. It's just gonna be smarter in the general sense. And I think the gravity of that statement is still like underrated. you know, stifling human innovation and and continue to spur that? So I'm actually not worried at all about stifling of human innovation. I I really deeply believe that people will just surprise us on the upside with better tools. I think all of history suggests that if you give people more leverage, they do more amazing things. And that's kinda like we all get to benefit from that. That's just kinda great. I am, though, increasingly worried about how we're gonna do this all responsibly. I think as the models get more capable, we have a higher and higher bar. We do a lot of things like red teaming and external audits, and I think those are all really good. But I think as the models get more capable, we'll have to deploy even more iteratively, have an even tighter feedback loop on looking at how they're used and where they work and where they don't work. And this world that we used to do where we can release a major model update every couple of years, we probably have to find ways to, like, increase the granularity on that and deploy more iteratively than we have in the past. And it's not super obvious to us yet how to do that, but I think that'll be key to responsible deployment. And also the way we kind of have all of the stakeholders negotiate what the rules of AI need to be, that's gonna get more complex over time too. no matter where the computers are built, I think global and equitable access to use the computers for training as well as inference is super important. One of the things that's like very core to our mission is that we make ChatGPT available for free to as many people as want to use it with the exception of certain countries where we either can't or don't for a good reason want to operate. How we think about making training compute more available to the world is gonna become increasingly important. I I do think we get to a world where we sort of think about it as a human right to get access to a certain amount of compute and we gotta figure out how to, like, distribute that to people all around the world. There's a second thing though, which is I think countries are going to increasingly realize the importance of having their own AI infrastructure, and we want to figure out a way, and we're now spending a lot of time traveling around the world, to build them in the many countries that'll want to build these. And I hope we can play some small role there in helping that happen. I I mean, first of all, what you really want is to be right. Being contrarian and wrong is still as wrong. And if you predicted, like, 17 out of the last two recessions, you probably were contrarian for the two you got right. Probably, not even necessarily. But you were wrong 15 other times. And so I think it's easy to get too excited about being contrarian. And again, like, the most important thing is to be right, and the group is usually right. But where the most value is is when you are contrarian and right. And that doesn't always happen in like sort of a zero or one kind of way. Like everybody in the room can agree that AI is the right place to start a company. And if one person in the room figures out the right company to start and then successfully executes on that and everybody else thinks, oh, that wasn't the best thing you could do, That's what matters. So it's okay to kind of like go with conventional wisdom when it's right and then find the area where you have some unique insight. In terms of how to do that, I do think surrounding yourself with the right peer group is really important and finding original thinkers is important. But there is part of this where you kind of have to do it solo or at least part of it solo or with a few other people who are like gonna be your cofounders or whatever. And I think by the time you're too far in the like how can I find the right peer group, you're somehow in the wrong framework already? So learning to trust yourself and your own intuition and your own thought process, which gets much easier over time. No one, no matter what they say, I think is like truly great at this when they're just starting out. Yeah. Because like you kinda just haven't built the muscle and like all of your social pressure and all of like the evolutionary pressure that produced you was against that. So it's it's something that like you get better at over time and and and don't hold yourself to too high of a standard too early on it. I I I forget exactly what the kind of world's electrical generating capacity is right now, but let's say it's like 3,000, 4,000 gigawatts, something like that. Even if we add another 100 gigawatts for AI, it doesn't materially change it that much. But it changes it some and if we start at a thousand gigawatts for AI someday it does. That's a material change. But there are a lot of other things that we want to do and energy does seem to correlate quite a lot with quality of life we can deliver for people. My guess is that fusion eventually dominates electrical generation on earth. I think it should be the cheapest, most abundant, most reliable, densest source. I could could be wrong with that, and it could be solar plus storage. And, you know, my guess most likely is it's gonna be eighty twenty one way or the other, and there'll be some cases where one of those is better than the other. But those kind of seem like the the two bets for, like, really global scale 1Â¢ per kilowatt hour energy. I mean, best lesson I learned was that we had an incredible team that totally could have run the company without me and did did for a couple of days. And you never and also that the team was super resilient. Like, we knew that a crate some crazy things and probably more crazy things will happen to us between here and AGI as stronger and stronger emotional reactions and the stakes keep ratcheting up. And, you know, I thought that the team would do well under a lot of pressure, but you never really know until you get to run the experiment. And we got to run the experiment and I learned that the team was super resilient and like ready to kind of run the company. then I thought about it and I realized just like how much I loved OpenAI.
Unknown: Well, then can we talk about the structure about it? Because this Russian doll structure of the open AI where you have the nonprofit owning the for profit.
Sam Altman: So if if you're gonna start a company, you gotta have like some theory that you're gonna sell a product someday. And we didn't think we were going to. We didn't realize we were gonna need so much money for compute. We didn't realize we were gonna like have this nice business. making money or or power. Cannot overstate how foreign of a concept. Like
Unknown: Okay. Because there's this so and the reason why I'm asking is just, you know, when we're teaching about principle driven entrepreneurship here, you can you can understand principles inferred from organizational structures. When The United States was set up, the architecture of governance is the constitution. It's got three branches of government, all these checks and balances, and you can infer certain principles that there's a skepticism on centralizing power, that things will move slowly, it's hard to get things to change, but it'll be very, very stable. If you you know, not to parrot Billie Eilish, but if you look at the OpenAI structure and you think, what was that made for? It's a you have a you're near a $100,000,000,000 valuation and you've got a very, very limited board that's a nonprofit board which is supposed to look after it's fiduciary duties to
Sam Altman: And you have to just adapt. There's a mission we really cared about. We thought AI was going to be really important. We thought we had an algorithm that learned. We knew it got better with scale. We didn't know how predictably it got better with scale. And we wanted to push on this. We thought this was like going to be a very important thing in human history. And we didn't get everything right but we were right on the big stuff and our mission hasn't changed and we've adapted the structure as we go and we'll adapt it more in the future. You know, don't You don't get to like solve everything really nicely all at once. It doesn't work quite like it works in the classroom as you're doing it. And my advice is just like trust yourself to adapt as you go. It'll be a little bit messy but you can do it. And I just ask this because of the significance of OpenAI.
Unknown: You have a board which is all supposed to be independent financially so that they're making these decisions as a nonprofit, thinking about the stakeholder, their stakeholder that they are fiduciary of, isn't the shareholders. It's humanity. Everybody's
Sam Altman: But, you know, we've put a structure in place that we think is a way to get I don't I I think about that so much, and I have such a hard time saying what it's actually gonna do. I or or maybe more accurately, I have such a hard time saying what it won't do. And we were talking earlier about how it's like not gonna maybe it won't change day to day life that much. But the balance of power in the world, it feels like it does change a lot. But I don't have a deep answer of exactly how. I I always get nervous anthropomorphizing AI too much because I think it, like, can lead to a bunch of weird oversights. But if we say, like, how much can AI recognize its own flaws? I think that's very important to build. And right now and the ability to like recognize an error in reasoning and have some sort of like introspection ability like that, that that seems to me like really important to pursue. I think it is the shared mission. I mean, I think people like like each other and we feel like we've you know, we're in the trenches together doing this really hard thing. But I think it really is like deep sense of purpose and loyalty to the mission. And when you can create that, think it is like the strongest force for success at any start at least that I've seen among startups. And, you know, we try to like select for that in people we hire. But even people who come in not really believing that AGI is gonna be such a big deal and that getting it right is so important tend to believe it after the first three months or whatever. And so that's like that's a very powerful cultural force that we have. Thanks. One thing that I think is important is not to pretend like this technology or any other technology is all good. I believe that AI will be very net good, tremendously net good. But I think like with any other tool, But I do think it's important to realize that But we're not gonna get this perfectly right. Like, we, society, are not gonna get this perfectly right. And And the way that that balance gets negotiated of safety versus freedom and autonomy, think it's like worth studying that with previous technologies
Unknown: Gang, actually, I've got to cut it. Sorry. I know. I just want to be very sensitive to time. I know the interest far exceeds the time and the love for Sam.
Sam Altman: It of course does. And I think it would be like really weird and a bad sign if it didn't scare me. Humans have gotten dramatically smarter and more capable over time. You are dramatically more capable than your great great grandparents and there's almost no biological drift over that period. Like sure you eat a little bit better and you got better healthcare. But that's not the main reason you're more capable. You are more capable because the infrastructure of society is way smarter and way more capable than any human. And through that it made you, society, the people that came before you, made you the internet, the iPhone, a huge amount of knowledge available at your fingertips and you can do things that your predecessors would find absolutely breathtaking. Society is far smarter than you now. Society is an AGI as far as you can tell. And contribute to brick by brick, step by step and then we use to go to far greater heights for the people that come after us. Things that are smarter than us will contribute to that same scaffolding. You will have your children will have tools available that you didn't and that scaffolding will have gotten built up to greater heights. And that's always a little bit scary, but I think it's like more way more good than bad and people will do better things and solve more problems And the people of the future will be able to use these new tools and the new scaffolding that these new tools contribute to. If you think about a world that has AI making a bunch of scientific discovery, what happens to that scientific progress is it just gets added to the scaffolding. And then your kids can do new things with it or you in ten years can do new things with it. But the way it's going to feel to people, I think, is not that there is this much smarter entity because we're much smarter in some sense than the great great great grandparents or more capable at least, but that any individual person can just do more.