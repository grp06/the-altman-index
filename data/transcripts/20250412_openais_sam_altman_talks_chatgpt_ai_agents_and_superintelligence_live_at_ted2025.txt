Unknown: genuinely astounded me when I asked it to come up with a diagram that shows the difference between intelligence and consciousness. Like, how would you do that? This is what it did. that your overall model has.
Sam Altman: like through every other technological revolution in history, okay, now of what we'll have for someone in a particular job increases, but the capabilities will increase so dramatically that I think it'll be easy to rise to that occasion.
Unknown: He came up with this. I thought this was actually rather profound. What do you think?
Sam Altman: if it is I will say that I think the creative spirit of humanity is an incredibly important thing. And we want to build tools that lift that up, that make it so that new people can create better art, better content, write better novels that we all enjoy. I believe very deeply that humans will be at the center of that. I also believe that we we probably do need to figure out some sort of new model around the economics of creative output. I think people have been building on the creativity of others for a long time. People take inspiration for a long time. access to creativity gets incredibly democratized and people are building off of each other's ideas all the time, I think there are incredible new business models that we and others are excited to explore. every time throughout history we have put better and more powerful technology in the hands of creators, I think we collectively get better creative output and people do just more amazing stuff. I mean, an even bigger question is when they haven't consented to it. In our opening session, Carol Cadwalader that line should be and how people say, spend your whole life, I think open source has an important place. We we actually just last night hosted our first community session to kinda decide the parameters of our open source model and how we want to shape it. We're going to do something near the frontier, I think better than any current open source model out there. This will not be all like, there will be people who use this in ways that some people in this room, maybe you or I, don't like. And, you know, I think we were late to act on that, but we're going to do it really well now. I mean, you're spending, it seems like an order or even orders of magnitude more than
Unknown: DeepSeek allegedly spent, although I know there's controversy around that. Are you confident that the actual better model is going to be recognized? Or are you actually like, isn't this, in some ways, life threatening to the notion that, yeah, by going to massive scale, tens of billions of dollars investment, we can we can maintain an incredible lead?
Sam Altman: We are so incredibly constrained. I have never seen growth in any company,
Unknown: So you're confident. You're seeing it grow, take off like a rocket ship. You're racing incredible new models all the time. What
Sam Altman: we have to build a great product, not just a great model. And so there will be a lot of people with great models,
Unknown: continue to focus on building the best defining product in this space. I mean, after I saw your announcement yesterday that you've now ChatGPT will know all of your query history, I entered, tell me about me, ChatGPT, from what you know.
Sam Altman: It's not, you know, it's not that you plug your brain in one day, but it's you will talk to ChatGPT over the course of your life, and someday, maybe if you want, it'll be listening to you throughout the day and sort of observing what you're doing, and it'll get to know you, and it'll become this extension of yourself, this companion, this thing that just tries to, like, help you be the best, do the best you can. In the movie Her,
Unknown: the AI
Sam Altman: AI you don't have to just like go to ChatGPT or whatever and say, have a question, give me an answer, but you're getting like The thing that I'm personally most excited about is AI for science at this point. I think I am a big believer that the most important driver of the world and people's lives getting better and better is new scientific discovery. We can do more things with less. We sort of push back the frontier of of what's possible. We're starting to hear a lot from scientists with our latest models that they're actually just more productive than they were before, that it's actually mattering to what they can discover. What's a plausible near term discovery, like room temperature meaningful progress against disease with AI assisted tools. Know, physics maybe takes a little bit longer, but I hope for it. move that big in the coming months as AgenTex software engineering really starts to happen.
Unknown: you as, you know, you have access to this stuff, and you we hear all these rumors coming out of AI, and it's like, oh my god, they've seen consciousness, or they've seen AGI, or they've seen some kind of apocalypse coming. Have you seen has there been a scary moment when you've seen something internally and thought, uh-oh, we need to pay attention to this?
Sam Altman: There have been moments of awe. And I think with that is always like, how far is this going to go? What is this going to be? But I continue to believe there will come very powerful models that people can misuse in big ways. People talk a lot about the potential for new kinds of bioterror, models that can present like a real cybersecurity challenge, models that are capable of I think many people mean, where people talk about disinformation
Unknown: I mean, you've had some departures from your safety team. How many people have departed? Why have they why have they left?
Sam Altman: there are clearly different views about AI safety systems. I I would really point to our track record. There are people who will say all sorts of things. You know, something like 10% of the world uses our systems now a lot. And You can't You don't wake up one day and say, hey, we didn't have any safety process in place. Now we think the model is really smart, so now we have to care about safety. You have to care about it all along this exponential curve. Of course, the stakes increase and there are big challenges. But of deploying them to the world, getting feedback, while the stakes are relatively low, learning about, like, hey, this is something we have to address. I think as we move into these agentic systems, there's a whole big category of new things we have to learn to address. So let's talk about
Unknown: agentic systems and the relation between that and AGI. I think there's confusion out there. I'm confused. So artificial general intelligence, it feels like chat GBT is already a general intelligence. I can ask it about anything, and it comes back with an intelligent answer. Why isn't that AGI?
Sam Altman: that it's currently weak at. It can't go discover new science and update its understanding and do that. I actually even without this sort of ability to get better at something it doesn't know yet, I might accept that as a definition of AGI. But the current systems,
Unknown: when do you think that we may be there?
Sam Altman: What I think matters though, and what people want to know is not where is this one magic moment of we finished, but given that what looks like is going to happen is that the models are just going to get smarter and more capable and smarter and more capable and smarter and more capable on this long exponential, different people will call it AGI at different points, but we all agree it's going go way, way past that, you know, to whatever you want to call these systems that get much more capable than we are. The thing that matters is how do we talk about a system that is safe through all of these steps and beyond as the system gets more capable than we are, as the system can do things that we don't totally understand? And I think more important than a when is AGI coming and what's the definition of it, it's recognizing that we are in this unbelievable exponential curve. And Someone else can say, superintelligence is out here. But we're going to have to contend and get wonderful benefits from this incredible system. And so I think we should shift the conversation away from what's the AGI moment to a recognition
Unknown: change moment when AI is set free to pursue projects on its own and to put the pieces together. You've actually you've got a thing called operator, which starts to do this, and I tried it out. You know, I wanted to book a restaurant and it's kind of incredible. It kind of can go ahead and do it, but this is what it said. There's a you know, it's an intriguing process and, you know, give me your credit card and everything else and I I I declined on this case to go forward. But I think this is this is the challenge that people are gonna have. It's it's kind of like it's an incredible superpower. It's a little bit scary and Joshua Benji when when he spoke here said that agentic AI is the thing to pay attention to. This is when everything could go wrong as we give power to AI to go out onto the internet to do stuff. I mean, going out onto the internet was always in the sci fi stories, the moment where, you know, escape happened and potential things could go horribly wrong. How do you both release agentic AI and have guardrails in place so that it doesn't go too far?
Sam Altman: I think people are going to be slow to get comfortable with agentic AI in many ways, but I also really agree with what you said, which is that even if some people are comfortable with it and some aren't, we are going have AI systems clicking around the Internet. And this is, I think, the most interesting and consequential safety challenge we have yet faced. Because AI that you give access to your systems, your information, the ability to click around on your computer, now those, when the AI makes a mistake, it's much higher stakes. I I kind of think they're increasingly becoming one dimensional. Like, a good product is a safe product. You will not use our agents if you do not trust that they're not going to empty your bank account or delete your data or who knows what else.
Unknown: In a world where agency is out there and and say that, you know, maybe open models are widely distributed, someone says, okay, AGI, I want you to go out onto the Internet and, you know, spread a meme however you can, what the danger moments are, and that we we cannot
Sam Altman: We And we'll update that over time. But we've tried to outline where we think the most important danger moments are, or what the categories are, how we measure that, and how we would mitigate something before releasing it. What I was gonna say is I totally understand that. I totally understand looking at this and saying, this is an unbelievable change coming to the world. you know, maybe I don't want this. Or maybe I love parts of it. Maybe I love talking to Chad GPT, but I worry about what's going to happen to art, and I worry about the pace of change, and I worry about these agents clicking here, clicking around the internet. And maybe on balance, I wish this weren't happening, or maybe I wish it were happening a little slower, or maybe I wish it were happening in a way where I could pick and choose what parts of progress were And going to I think think the fear is totally rational, the sort of the anxiety is totally rational. We all have a lot of it too. But a, there will be tremendous upside. Obviously, you know, you use it every day, you like it. B, I really believe that society figures out over time, And I, like, I think this conversation is really important. I think talking about these areas of danger are really important, talking about new economic models are really important. But we have to embrace this with, like, caution but not fear, What is the right policy proposal? I I do think the idea that as these systems get more advanced and have legitimate global impact, we need some way, you know, maybe the companies themselves put together the right framework or the right sort of model for this, but we need some way that very advanced models have external safety testing, and we understand when we get close to some of these danger zones. I very much still believe in that. It struck me as ironic that a safety agency we need to define rigorous testing for models, understand what the threats that we collectively, society, most want to focus on, and make sure that as models are getting more capable, we have a system where we all get to understand what's being released in the world. I think this is really important, and I think we're not far away from models that are going to be of great public interest in that sense.
Unknown: visionary who's done the impossible, and you shocked the world with far fewer people than Google. You came out with something that was much more powerful than anything you've done. I mean, it's it's it is amazing what you've built. But the other narrative is that you have shifted ground, that you've shifted from being open AI, this open thing, to the allure of building something super powerful. And, you know, some you've lost some of your key people. There's a narrative out there. Some people believe that you're not to be trusted in this space. I would love to know who you are. How what is your narrative about yourself? What what are your core values, Sam, that that can give us the world confidence that someone with so much power here
Sam Altman: in this one voice in this AI revolution, trying to do the best we can and kind of steward this technology into the world in a responsible way. We've definitely made mistakes, we'll definitely make more in the future. On the whole,
Unknown: There's you posted this well, so okay. So here's here's the ring of power from Lord of the Rings. Your claimed that, you know, he thought that you'd been corrupted by the ring of power, It's in what is in everyone's mind as we see technology CEOs get more powerful, get richer, is can they handle it or does it become irresistible? Does the power and the wealth make it impossible to a for profit model to develop at the right pace. But but you you tell me, I just I'm if you if you don't feel that, like what Your son, I mean, that last thing you said that I've never felt love like this, I think any parent in the room so knows that feeling, that wild biological feeling that humans have and AIs never will of your your holding likely the most unbelievable life, but also you inject a ten percent chance that he gets destroyed. Do you press that button?
Sam Altman: things, and by far, the most amazing thing that has ever happened to me, like, everything everybody says is true.
Unknown: Tristan Harris gave a very powerful talk here this week in which he said that the key problem, in his view, was that you and your peers in these other models all feel basically that the development of advanced AI is inevitable, that the race is on, and that there is no choice but to try and win that race and to do so as responsibly as you can. And maybe there's a scenario where your super intelligent AI can act as a brake on everyone else's or something like that. But that the very fact that everyone believes it is inevitable means that Do you think that you and your peers do feel that it's inevitable, and can you see any pathway out of that where we could collectively agree to just slow things down a bit, have society as a whole weigh in a bit and say, no, let's You know, we don't want this to happen quite as fast. It's too disruptive.
Sam Altman: So I think this happens. And and again, like, this is where I think the track record does matter. If we were rushing things out and there were all sorts of problems, either the product didn't work as people wanted it to or there were real safety issues or other things there. And I will come back to a change we made. I think you could do that. There is communication between most of the efforts. With one exception, I think all of the efforts care a lot about AI safety. deep care to get this right. I think the caricature of this is just like this crazy race or sprint or whatever people feel the impact of this so incredibly a meeting in OpenAI or other companies, you'd be like, oh, these people are really kind of caring about this. Now, we did make a change recently to how we think about one part of what's traditionally been understood as safety, which is with our new image model, we've given users much more freedom on what we would traditionally think about as speech harms. But I think part of model alignment is following what the user of a model wants it to do within the very broad bounds of what society decides. So if you ask the model to depict a bunch of violence or something like that, or to sort of reinforce some stereotype, there's a question of whether or not it should do that. We're taking a much more permissive stance. Now, there's a place where that starts to interact with real world harms that we have to figure out how to draw the line for. But, you know, I think there there will be cases where a company says, okay, we've heard the feedback from society. People really don't want models to censor them in ways that they don't think make sense. That's a fair safety negotiation. But
Unknown: the solution to those kinds of problems is to bring people together and meet at one point and a summit of the best but not too many people, small, but and you and your peers to try to crack what agreed safety lines could be across the world.
Sam Altman: and we can like learn the collective value preference of what everybody wants, rather than have a bunch of people who are blessed by society to sit in the room and make these decisions. I think that's very cool. And I think you will see us do more in that direction. When we have gotten things wrong, because the elites in the room had a different opinion about what people wanted for the guardrails on imagegen than what people actually wanted, and we couldn't point to real world harm, so we made that change. I'm proud of that. can help us be wiser, make better decisions, can talk to us. If we say, hey, I want thing x, you know, rather than, like, the crowd spin that up, AI can say, hey, totally understand that's what you want. If that's what you want at the end of this conversation, you're in control, you pick. But have you considered it from this person's perspective, or the impact it'll have on these people? Like, I think AI can help us be wiser make better collective governance decisions than we could before. Well, we're well out of time. Sam, I'll give you the last word. What kind of world do you believe, all things considered, of, like, a little toddler sitting in, you know, a doctor's office waiting room or something. And there was a magazine, like one of those old, you know, glossy cover magazines, and the toddler had his hand on it and was going like this and kind of angry. They will never grow up in a world where products and services are not incredibly smart, incredibly capable. They will never grow up in a world where computers don't just kind of understand you and It'll be a world of incredible the rate of change is incredibly fast and amazing new things are happening. And it'll be a world where, like, individual ability, impact, whatever, is just so far beyond what a person can do today.
Unknown: some of the biggest opportunities, the biggest moral challenges, the biggest decisions to make of perhaps any any human in history pretty much.