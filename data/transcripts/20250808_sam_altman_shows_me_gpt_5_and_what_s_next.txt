Unknown: And the reason that we do all of that is because we really believe that if people see those better futures, they can then help build them. So my goal here is to try my best to and see what it looks like. Fantastic. Awesome. Starting with what you just announced. You recently said, surprisingly recently, that GPT-four was the dumbest model any of us will ever have to use again. But GPT-four can already perform better than 90% of humans at the SAT and the LSAT and the GRE, and it can pass coding exams and sommelier exams and medical licensing. And now you just launched GPT-five.
Sam Altman: What can GPT five do that GPT four can't? First of all, one important takeaway is you can have an AI system that can do all those amazing things you just said, and it doesn't it clearly does not replicate a lot of what humans are good at doing, which I think says something about the value of SAT tests or whatever else. But I think had you gone back to if we were having this conversation the day of GPT four launch and we told you how GPT four did at those things, you were like, oh, man. This is gonna have huge impacts and some negative impacts on what it means for a bunch of jobs or, you know, what people are gonna do and, you know, this is a bunch of positive impacts that you might have predicted that haven't yet come true. And so there there's something about the way that these models are good that does not capture a lot of other things that we need people to to do or care about people doing. And I suspect that same thing is gonna happen again with GPT five. People are gonna be blown away by what it does. It's really good at a lot of things, and then they will find that People will use it for all sorts of incredible things. It will transform a lot of But we, people, society, will co evolve with it to expect more with, you know, better tools. So, yeah. Like, I think this model is quite remarkable in many ways, quite limited in others. But the fact that for, you know, three minute, five minute, one hour tasks that, like, an expert in a in a field could maybe do or maybe struggle with, that the fact that you have in your pocket one piece of software that can do all of these things is really amazing. I think this is like unprecedented at any point in human history that I've that a technology has improved this much this fast. And and the fact that we have this tool now, you know, we're like living through it and we're kind of adjusting step by step. But if we could go back in time five or ten years and say this thing was coming, we would be like, probably not. Let's
Unknown: assume that people haven't seen the headlines.
Sam Altman: kinda any hard scientific or technical question and When I was in junior high or maybe it was ninth grade, I got a TI 83, this old graphing calculator. And I spent so long making this game called Snake. Yeah. It was a very popular game with kids in my school. game of Snake. And of course, it did that perfectly in, like, seven seconds. And then I was like, okay, am I supposed to be would my, like, 11 year old self think this was cool or, like, you know, miss something from the process? And I had, like, three seconds of wondering, like, oh, is this good or bad? And then I immediately said, you had this experience that reminded me of being, like, 11 and programming again where I was just like, I don't know. Wanna try this. Now I have this idea. Now I have to Yeah. But I could do it so fast and I could, like, express ideas and try things and play with things in such real time. I was like, oh, man. You know, I was worried for a second about kids missing the struggle of learning to program in this sort of Stone Age way, and now I'm just thrilled for them because the the way that people will be able to create with these new tools, the speed with which you can sort of bring ideas to life, you know, in that's that's pretty amazing. So this idea that GPT five can just not only answer all these hard questions for you, but really create on demand, almost instantaneous software, that's I think that's gonna be one of the defining
Unknown: As you're talking about that, I find myself thinking about a concept in weightlifting of time under tension. Yeah. And for those who don't know, it's you can squat a 100 pounds in three seconds or you can squat a 100 pounds in thirty. You gain a lot more by squatting it in 30. And when I think about our creative process and when I've felt most I think that that cognitive time under tension is so important. And it's it's ironic almost because these tools have taken enormous cognitive time under tension to develop. But in some ways, I do think people might say they're you people are using them as a escape hatch for thinking in some ways, maybe. Now you might say, yeah, but we did that with the calculator and we just moved on to harder math problems. Do you feel like there's something
Sam Altman: It's different with mean, there are some people who are clearly using Chachapatim not to think, and there are some people who are using it to think more than they ever have before. stretch their brain with it a little more and be able to do more. I think that like, you know, society is a competitive place. Like, if you give people new tools, in theory, maybe people just work less, but in practice, it seems like people work ever harder and the expectations of people just go up. So my my guess is that like other tools, some people like other pieces of technology, some people will do more and some people will do less. But certainly for the people who want to use ChatGPT to increase their cognitive time under tension, they are really able to. And it is I take a lot of inspiration from what, like, the top 5% of most engaged users do with ChatGPT. Like, it's really amazing how much people are learning and doing and, you know, outputting.
Unknown: So specific tasks that you found most interesting are,
Sam Altman: the AI can write software for anything, and that means that you can express ideas in new ways, that the AI can do very advanced things. It can do, you know, it can, like in in some sense, you could, like, ask GPT four anything, but because GPT five is so good at programming, it feels like it can do anything. Of course, it can't do things in the physical world, but it can get a computer to do very complex things. And software is this super powerful, you know, way to, like, control some stuff and actually do some things. So that that for me has been the most striking. It's gotten it's much better at writing. So this is like nuanced quality they can't quite articulate. But then when they have to go back to GPT four to test something, it feels terrible.
Unknown: It's about the next stage. What what comes after GBT V? In which year do you think a large language model will make a significant scientific discovery? And what's missing such that it hasn't happened yet? He caveated here that we should leave math and special case models like AlphaFold aside. He's specifically asking about fully general purpose models like the GPT series. I would say most people will agree that that happens at some point over the next two years, but the definition of significant matters a lot. And so some people significant might happen,
Sam Altman: you know, in early twenty five. Some people might, maybe not until late twenty twenty six. Sorry, early twenty twenty six, maybe some people not until late twenty twenty seven. But I would I would bet that by late twenty seven, most people agree that there has been an AI driven significant new discovery. And the thing that I think is missing is just the kind of cognitive power of these models. A framework that one of the researchers said to me that I really liked is, you know, a year ago, we could do well on like a high school, like a basic high school math competition, problems that might take a professional mathematician seconds to a few minutes. We very recently got an IMO gold medal. Is a crazy, difficult, a significant new mathematical theorem is like a thousand hours of work for a top person in the world. So we've gotta go from, you know, another significant gain. But if you look at our trajectory, you can say, like, okay, we're getting to that. We have a path to get to that time horizon.
Unknown: The long term feature that you've described is superintelligence.
Sam Altman: If we had a system that could do better research better AI research than, say, the whole OpenAI research team, like, we were willing, if we said, okay, the best way we can use our GPUs is to let this AI decide what experiments we should run,
Unknown: And so one of the steps, it sounds like you're saying, on that path is this moment of scientific discovery, asking better questions, of grappling with things in a in a way that expert level humans do to come up with new discoveries. One of the things that keeps knocking around in my head is if we were in 1899,
Sam Altman: question is did you like, if we think about that forward, like like, if we think of where we are now, should this if if we'd never got another piece of physics data, do we expect that a really good superintelligence could just think super hard about our existing data and maybe say, like, solve high energy physics with no new particle accelerator, or does it need to build a new one and design new experiments? Obviously, we don't know the answer to that. Different people have different speculation. But I suspect we will find that for a lot of science, it's not enough to just think harder about data we have, but we will need to build new instruments, conduct new experiments, and that will take some time. That is the real world is slow and messy and, you know, whatever. So I'm sure we could make some more progress just by thinking harder about the current scientific data we have in the world, But my guess is to make the big progress, we'll also need to build new machines and run new experiments, and there will be
Unknown: some slowdown built into that. Another way of of thinking about this is AI systems now are just incredibly good at answering almost any question. But maybe one of the things we're saying is it's another leap yet, and what what Patrick's question is getting at is to ask the better questions.
Sam Altman: And there's a dimension of human intelligence that seems
Unknown: The next question is from NVIDIA CEO Jensen Huang. I'm gonna read this verbatim. Fact is what is. Truth is what it means. So facts are objective. Truths are personal. They depend on perspective, culture, values, beliefs, context. One AI can learn and know the facts. But how does one AI know the truth for everyone in every country and every background?
Sam Altman: I have been surprised. I think many other people have been surprised too about how fluent AI is at adapting to different cultural contexts and individuals. One of my favorite features that we have ever launched in ChatGPT is the the sort of enhanced memory that came out earlier this year. Like, it really feels like my ChatGPT gets to know me and what I care about and, like, my life experiences and background and the things that have led me to where they are. A friend of mine recently, who's been a huge ChadGBT user, so he's got a lot of a lot of he's put a lot of his life into all these conversations. He gave his ChadGBT a bunch of personality tests and asked them to answer as if they were him, and it got the same scores he actually got even though he'd never really talked about his personality. And my ChatGPT has really learned over the years of me talking to it about my culture, values, my life. And I have used, you know, I sometimes will use it in like I'll use like a free account just to see what it's like without any of my history and it feels really really different. See, I think we've all been surprised on the upside of how good AI is at learning this and adapting.
Unknown: brings me to this seems like a good moment for our first time travel trip. Okay. We're going to 2030. This is a serious question, but I wanna ask it with a lighthearted example. And I think the reason why people reacted so strongly to it, it was maybe the first time people saw a video, enjoyed it, and then later found out that it was completely AI generated. we are teenagers, and we're scrolling whatever teenagers are scrolling in 2030, How do we figure out what's real and what's not real?
Sam Altman: I mean, I can give all sorts of literal answers to that question. We could be cryptographically signing stuff, and we could decide who we trust their signature if they actually filmed something or not. But my sense is what's gonna happen is it's just gonna gradually converge. You know, even like a photo you take out of your iPhone today, it's like mostly processing power between the photons captured by that camera sensor and the image you eventually see. And you've decided it's real enough or most people decided it's real enough, but we'd accepted some gradual move from when it was like photons hitting the film in a camera. And, TikTok, there's probably all sorts of video editing tools being used to make it better than reality. Yeah. Exactly. Or it's just like, you know, whole scenes are completely generated or some of the whole videos are generated like those bunnies on that trampoline. and I think that the the sort of like the threshold for how real does it have to be to consider to be real will just keep moving. So
Unknown: it's sort of an education question that's people will
Sam Altman: real and a little bit not real. Like, you know, we watch a sci fi movie. We know that didn't really happen. You watch someone's beautiful photo of themselves on vacation on Instagram. Like, okay, maybe that photo was literally taken, but you know there's tons of tourists in line for the same photo, and that's left out of it. And I think we just accept that. Now, certainly, a higher percentage of media both will will feel not real, But I think that's been the long term trend anyway.
Unknown: We're graduating from college, you and me. There are some leaders in the AI space that have said that in five years, half of the entry level white collar workforce will be replaced by AI. So let's make it five years. We're still going to 2030. I'm curious what you think the pretty short term impacts of this will be for for young people. I I mean, these, like, half of entry level jobs replaced by AI
Sam Altman: or rescale or whatever the politicians call it that no one actually wants but politicians most of the time. If I were Why? Because there's never been a more amazing time to go create something totally new, to go invent something, to start a company, whatever it is. I think it is probably possible now to start a company that is a one person company that will go on to be worth like more than a billion dollars, and more importantly than that, deliver an amazing product and service to the world. And that that is like a crazy thing. You have access to tools that can let you do what used to take teams of hundreds. And you just have to, like, you know, learn how to use these tools and come up with a great idea.
Unknown: I think the most important thing that this audience could hear from you on this optimistic show is in two parts. First, there's tactically, how are you actually trying to build the world's most powerful intelligence, and what are the rate limiting factors to doing that? And then philosophically, My understanding is that there are three big categories that have been limiting factors for AI. The first is compute, the second is data, and the third is algorithmic design. How do you think about each of those three categories right now? And if you were to help someone understand the next headlines that they might see,
Sam Altman: I would say there's a fourth two, which is figuring out the products to build. Scientific progress on its own, not put into the hands of people, is of limited utility and doesn't sort of co evolve with society in the same way. If I could hit all four of those. Yeah. So on the compute side, yeah, this is like the biggest infrastructure project certainly that I've ever seen. Possibly, it will become the I think it will. Maybe already is the biggest and most expensive one in human history. But the whole supply chain from making the chips and the memory and the networking gear, racking them up in servers, doing, you know, a giant construction project to build like a mega mega data center, putting the, you know, finding a way to get the energy, which is often a limiting factor piece of this and all the other components together. This is hugely complex and expensive and we are we're still doing this in like a sort of bespoke one off way, although it's getting better. Like, eventually, we will just design a whole kind of like mega factory that takes, you know, spiritually it will be melting sand on one end and putting out fully built AI compute on the other. But we are a long way to go from that. And it's an enormously complex and expensive process. We are putting a huge amount of work into building out as much compute as we can and to do it fast. And it's gonna be sad because GPT-five's gonna launch and there's gonna be another big spike in demand, we're not gonna be able to serve it. And it's gonna be like those early GPT-four days, and the world just wants much more AI than we can currently deliver. And building more compute is an important part of doing that. That's actually this is what I expect to turn the majority of my attention to is how we build compute at much greater scales. So how we go from millions to tens of millions and hundreds of millions, and eventually, You know, like if you're gonna run a gigawatt scale data center, it's like, oh, gigawatt, how hard can that be to find? It's really hard to find a gigawatt of power available in the short term. We're also very much limited by the processing chips and the memory chips, how you package these all together, how you build the racks. And then there's like a list of other things that are, you know, there's like permits, there's construction work. But but again, the goal here will be to really automate this. Once we get some of those robots built, they can help us automate it even more. But just, you know, like a world where you can basically pour in money and get out a pre built data center. So that'll be that'll be a huge unlock if we can get it to work. These models have gotten so smart. There was a time when we could just feed it another physics textbook and got a little bit smarter at physics. But now, like, honestly, GPT five understands everything in a physics textbook pretty well. We're excited about synthetic data. We're very excited about our users helping us create harder and harder tasks and environments to go off and have the system solve. But I think we're data will always be important, but we're entering a realm where the thing that OpenAI does best in the world is we have built this culture of repeated and big algorithmic research gains. So we kind of, you know, figured out the what became the GPT paradigm. We figured out what became the reasoning paradigm. We're working on some new ones now. But it is very exciting to me to think that there are still many more orders of magnitudes of algorithmic gains ahead of us. We we just yesterday if you had asked me a few years ago when we'd have a model of that intelligence running on a laptop, I would have said many, many years in the future. But then we we found some algorithmic gains, particularly around reasoning, but also some other things that let us do a a tiny model that can do this amazing thing. And,
Unknown: thinking about this. I'm curious for people who don't quite know what you're talking about, who aren't familiar with how an algorithmic design would lead to a better experience that they actually use. Could you summarize the state of things right now? Like, what what is it that you're thinking about when you're thinking about how fun this problem is?
Sam Altman: by a lot of experts in the field, which was can we train a model to play a little game, which is show it a bunch of words and have it guess the one that comes next in the sequence. That's called unsupervised learning. There's not you're not really saying like this is a cat, this is dog, just saying here's some words, guess the next one. And the fact that that can go learn these very complicated concepts unlikely to work. Like, how was that all gonna get encoded? And yet humans do it. Babies start hearing language and figure out what it means kinda largely, And so we did it. And then we also realized that if we scaled it up, it got better and better. But we had to scale over many, many orders of magnitude, so it wasn't that good in the GPT one day. It wasn't good at all in the GPT one days. And a lot of experts in the field said, oh, this is ridiculous. It's never gonna work. It's not gonna be robust. But we have these things called scaling laws and we said, okay, so this gets predictably better as we increase compute, memory, data, whatever. And we can we can decide we can use those predictions to make decisions about how to scale this up and do it and get great results. And that has worked over, yeah, a crazy number of orders of magnitude. And it was so not obvious at the time. Like, that was that was I think the the reason the world was so surprised is that that seemed like such an unlikely finding. Another one was that we could use these language models with reinforcement learning where we're saying this is good, this is bad, to teach it how to reason. progress. And that that was another thing that felt like, if it works, it's really great, but like, no way this is gonna work. It's too simple. And now we're on to new things. We've figured out how to make much better video models. We are we are discovering new ways to use new kinds of data in an environment to kind of scale that up as well.
Unknown: But it also is true behind the scenes that it's a it's not linear like that. It's messier. Tell us a little bit about the mess before GPT-five. What were the interesting problems that you needed to solve?
Sam Altman: We did a model called Orion that we released as GPT-four 0.5, and we had we did too big of a model. It was just it was it's a very cool model, but it's unwieldy to use and we realized that for kind of some of the research we need to do on top of a model, we need a different shape. So we we followed one scaling law that kept being good without without really internalizing. There was a new even steeper scaling law that we got better returns for compute on which was this reasoning thing. So that was like one alley we went down and turned around, but that's fine. That's part of research. We had some problems with the way we think about our datasets as these models really have to get this big and learn from this much data. the middle of it, in the day to day, you kind of you make a lot of U turns as you try things or you have an architecture idea that doesn't work. But the aggregate, the summation of all the squiggles, has been remarkably smooth on the exponential.
Unknown: I'm sitting here interviewing you about the thing that you just put out, you're thinking about What are the things that you can share that are at least the problems that you're thinking about
Sam Altman: bizarre on the first day, and then we'll get used to them really fast. So we'll be like, oh, it's incredible that this is, like, being used to cure disease. And be like, oh, it's extremely scary that models like this are being used to, like, create new biosecurity threats. And then we'll also be like, man, it's really weird to, like, live through watching the world speed up so much. And, you know, the economy grows so fast and the, it will feel vertigo inducing, sort of the rate of change. And then, like happens with everything else, the remarkable ability of people, of humanity to adapt to any amount of change A kid born today will never be smarter than AI, ever. And a kid born today, by the time that kid, like, kinda understands the way the world works, will just always be used to an incredibly fast rate of things improving and discovering new science. They will just they will never know any other world. It will seem totally natural. It will seem unthinkable and stone age like that we used to use computers or phones or any kind of technology that was not way smarter than we were. nothing different than the way you've been parenting kids for tens of thousands of years. Like, love your kids, show them the world, like, support them in whatever they want to do, and teach them how to be a good person.
Unknown: It sounds a little bit like some of the you know, you've said a couple of things like this that that, you know, And it sounds like what you're saying is there will be more optionality for them in a in a world that you envision. Totally. And therefore, they will have more is a little bit too far in the future to think about. So maybe this this was gonna be a jump to 2040, but maybe it will keep it shorter than that. When I think about the area where AI could have, for both our kids and us, the biggest genuinely positive impact on all of us, it's health. what do you hope that he's telling me AI is doing for our health in 2035?
Sam Altman: Okay. Yeah, please. One of the things we are most proud of with GPT five is how much better it's gotten at health advice. People have used the GPT-four models a lot for health advice. And, you know, I'm sure you've seen some of these things on the Internet where people are like, I had this life threatening disease and no doctor could figure it out, and I put my symptoms and a blood test into ChatGPT. It told me exactly the rare thing I had. I went to a doctor. I took a pill. I'm cured. That's amazing, obviously. And a huge fraction of ChatGPT queries are health related, so we wanted to get really good at this. And we invested a lot, And, g b d five is significantly more likely to tell you what you actually have or what you actually should do. And by 2035, I think we will be able to use these tools to cure a significant number, or at least treat a significant number of diseases that currently plague us. I think that'll be one of the most viscerally felt benefits of AI.
Unknown: People talk a lot about how AI will revolutionize healthcare. But I'm curious to go one turn deeper on specifically what you're imagining. Like, is it that these AI systems could have helped us see GLP-1s earlier, this medication that has been around for a long time but we didn't know about this other effect? Is it that, you know, AlphaFold and protein folding is helping create new medicines? Would like to be able to ask.
Sam Altman: to go off and think and then say, Ah, okay. I read everything I could find. I have these ideas. I need you to go get a lab technician to run these nine experiments and tell me what you find for each of them and, you know, wait two months for the cells to do their thing. Send the results back to GPT-eight and say, tried it. Here you go. Think, think, think. Say, okay, I just need one more experiment. That was a surprise. Run one more experiment. Give it back. GPT, it says, okay, go synthesize this molecule and try, you know, mouse studies or whatever.
Unknown: one of the things that they refer to is the Industrial Revolution. They say, I chose 2050 because I've heard people talk about how by then, the change that we will have gone through will be like the Industrial Revolution, but, quote, 10 times bigger and 10 times faster. The Industrial Revolution gave us modern medicine and sanitation and transportation and mass production and all all of the conveniences that we now take for granted. It also was incredibly if this all goes the way you hope, who still gets hurt in the meantime?
Sam Altman: don't really know what this is going to feel like to live through. I think we're in uncharted waters here. I do believe in human adaptability and infinite creativity and desire for stuff, and I think we always do figure out new things to do. But the transition period, if this happens as fast as it might And I don't think it will happen as fast as some of my colleagues say the technology will, but society has a lot of inertia. Mhmm. People adapt their way of living surprisingly slowly. There are classes of jobs that are gonna totally go away. And there will be many classes of jobs that change significantly, and there'll be the new things in the same way that your job didn't exist some time ago, neither did mine. it's still disruptive to individuals, but society has gotten has proven quite resilient to this. And then in some other sense, we have no idea how far or fast this could go, and thus
Unknown: I'd like to talk about what some of those could be, because I'm not a historian by any means. But the first Industrial Revolution, my understanding is, led to a lot of public health implementations because public health got so bad. Led to modern sanitation because public health got so bad. The Second Industrial Revolution led to workforce protections because labor conditions got so bad. Every big leap specific can we get as early as possible about what that mess can be? What what are the public interventions that we could do ahead of time to reduce the mess that we think that we're headed for?
Sam Altman: much less someone who can see the future. I It seems to me like something fundamental about the social contract may have to change. It may not. It may may be that actually capitalism works as it's been working surprisingly well. And like demand supply balances do their thing and we all just figure out kind of new jobs and new ways to transfer value to each other. But it seems to me likely that we will decide we need to think about how access to this maybe most important resource of the future gets shared. The best thing that it seems to me to do is to make AI compute as abundant and cheap as possible such that we're just like, there's way too much and we run out of good new ideas to really use it for and it's just like anything you want is happening. Without that, I can see quite literal wars being fought over it. But, you know, new ideas about how we distribute access to AGI compute, that seems like a really great direction. Like, a crazy but important thing to think about.
Unknown: we often ascribe almost full responsibility of the AI future that we've been talking about to the companies building AI. This is not a question about specific, you know, federal regulation or anything like that, although if you have an answer there, I'm curious. But what would you ask of the rest of us? What is the shared responsibility here, and how can we act in a way that would help make the optimistic version of this more possible?
Sam Altman: My favorite historical example for the AI revolution is the transistor. It was this amazing piece of science that some brilliant scientists discovered. It scaled incredibly like AI does and it made its way relatively quickly into every many things that we use. Your computer, your phone, that camera, that light, whatever. And it was a it was a real unlock for the tech tree of humanity. And there were a period in time where probably everybody was really obsessed with the transistor companies, the semiconductors of, you know, Silicon Valley back when it was Silicon Valley. But now, you can maybe name a couple of companies that are transistor companies, but mostly you don't think about it. Mostly it's just seeped everywhere. And Silicon Valley is, you know, like, probably You think about what Apple did with the iPhone, and then you think about what TikTok built on top of the iPhone and like, alright, here's this long chain of all these people that nudged society in some way and what our governments did or didn't do and what the people using these technologies did. And I think that's what will happen with AI. And they will think about the companies that built on it, and what they did with it and the kind of, like, political leaders, the decisions they made that maybe they wouldn't have been able to do without AI, but they will still think about, like, what this president or that president did. And, you know, the role of the AI companies is all these companies and people and institutions before us built up this scaffolding. We added our one layer on top, And that is the beauty of our society. We kind of all I love this idea that society is the superintelligence. No one person could do on their own what they're able to do with all of the really hard work that society has done together to give you this amazing set of tools. And
Unknown: build on it well.
Sam Altman: That is what I feel as this
Unknown: cataclysmic change. The one I'm thinking about right now is with CRISPR pioneer Jennifer Doudna, and it did feel like that was also what she was saying in some way. She had discovered something that really might change the way that most people man, I I hope that this and building the AI future that would be best for the most people. And I can imagine that it is easier, maybe more quantifiable sometimes to focus on the next way to win the race. when those two things are at odds, what is an example of a decision that you've had to make
Sam Altman: I think there are a lot. So one of the things that we are most proud of is many people say that ChatGeBT is their favorite piece of technology ever, trying to help you. It's trying to help you accomplish whatever you ask. It's very aligned with you. It's not trying to get you to use it all day. It's not trying to get you to buy something. It's trying to kind of help you accomplish whatever your goals are. And that is that's like a very special relationship we have with our users. We do not take it lightly. There's a lot of things we could do that would grow faster, that would get more time in ChatGPT that we don't do because we know that our long term incentive is to stay as aligned with our users as possible. And
Unknown: I'm gonna ask my next question. it feels like we're in the first inning. Yeah. seems like you're gonna be someone who is leading the next few. What is a way what is a learning from inning one or two, or a mistake that you made that you feel will affect how you play in the next?
Sam Altman: being too flattering to users. And for some users, was most users, was just annoying. But for some users that had fragile mental states, it was encouraging delusions. That was not the top risk we were worried about. It was not the thing we were testing for the most. It was on our list. And I think it was a great reminder of we now have a service that is so broadly used. In some sense, society is coevolving with it. And when we think about these changes and we think about the unknown unknowns, we have to operate in a different way and have a wider aperture to what we think about as our top risks.
Unknown: Most concerned about the creation that you've built?
Sam Altman: It was like, wow, this is really like this is this is an amazing accomplishment of this group of people that have been pouring their life force into this for so long. On a what have we done moment, there was I was talking to a researcher recently. You know, there will probably come a time where our systems are, Already our people are sending billions of messages a day to ChatGPT and getting responses that they rely on for work or their life or whatever. And, you know, one researcher can make some small tweak to how Chad GPT talks to you or talks to everybody. And that's just an enormous amount of power for one individual making a small tweak to the model personality. No person in history has been able to have billions of conversations a day. And so, you know, somebody could do something. But this is like just thinking about that really hit me of like, this is like a crazy amount of power for one piece of technology to have and Yeah. That we gotta, like, think about what it means to make a personality change to the model at this kind of scale. it could have been a very different conversation with somebody else, but in this case, was like, what do a good set of procedures look like? How do we think about how we want to test something? How do we think about we want to communicate it? But with somebody else, it could have gone in a very philosophical direction. It could have gone in a what kind of research do we want to do to go understand what these changes are going make? Do we want to do it differently for different people? So it went that way, but mostly just because of who I was talking to.
Unknown: To combine what you're saying now with your last answer, one of the things that I have heard about GPC five, and I'm still playing with it, is that it is supposed to be less effusively, guide it
Sam Altman: But as we've been making those changes and talking to users about it, not all bad for ChatGPT to, it turns out, be encouraging of you. Now, the way we were doing it was bad, but turning it like something in that direction might have some value in it. How we do it, we overall personality.
Unknown: being much more in my life. Yeah. Being like in my Gmail and my calendar and my like, I've been using gbt4 mostly as an
Sam Altman: Over time, it'll start to feel way more proactive. So maybe you wake up in the morning and it says, Hey, this happened overnight. I noticed this change on your calendar. I was thinking more about this question you asked me. I have this other idea. And then eventually we'll make some consumer devices and it'll sit here during this interview and maybe it'll leave us alone during it, but after it'll say, That was great, but next time you should have asked Sam this, or When you brought this up, he didn't give you a good answer, so you should really drill him on that. And it'll just feel like it becomes more like this entity that is this companion with you throughout your day.
Unknown: They are hopefully feeling like they maybe see visions of moments in the future a little bit better.
Sam Altman: the number of people that I have the the most common question I get asked I am surprised how many people ask that and have never tried using Chattypati for anything other than a better version of a Google search. And so the number one piece of advice that I give is just try to get fluent with the capability of the tools. Figure out how to use this in your life. Figure out what to do with it. And I think that's probably the most important piece of tactical advice. You know, go meditate, learn how to be resilient and deal with a lot of change. There's all that good stuff too, but just using the tools really helps.
Unknown: doing all of this research beforehand, I spoke to and using them. I spoke to a lot of people that were actually in labs and trying to build what we have defined as superintelligence. And it did seem like there were these two camps forming. There's a group of people who are using the tools, like you in this conversation, and building tools for others, saying this is gonna be a really useful future that we're all moving toward. Your life is gonna be full of choice. And we've talked about our my potential kids and Yeah. And their futures. And then there's another camp of people that are building these tools that are saying it's gonna kill us all. And I'm curious how that cultural disconnect has like, what am I missing
Sam Altman: I I can't I can't really put myself in the headspace. If that's what I really truly believed, I assume it's true. I assume it's in good faith. I assume there's just like, there's some psychological issue there I don't understand about how they make it all make sense, but it's very strange to me. Do
Unknown: And when you ask people for specifics on how it's gonna kill us all, I mean, don't think we need to get into this on an optimistic show, but you hear the same kinds of refrains. You think about, you know, something trying to accomplish a task and then over accomplishing that task. AI, and and maybe that is an overreliance that we, you know, would need to think about. And and, you know, you you play out these different scenarios, Or maybe it really is someone who just says, 99%
Sam Altman: gets wiped out, and I really want to work to maximize, to move that 99 to 99.5, why did you pick this one? How did you get started? What did you see about this before sci fi shows growing up, and I always thought it would be really cool if someday somebody built it. I thought it be the most important thing ever. I never thought I was gonna be one to actually work on it. And I feel unbelievably lucky and happy and privileged that I get to do this. I feel like I've come a long way from my childhood. But there was never a question in my mind that this would not be the most exciting, interesting thing. I just didn't think it was gonna be possible. And when I went to college, it really seemed like we were very far from it. And then in 2012, the AlexNet paper came out, done in partnership with my co founder, Ilya. And it seemed to me like there was an approach that might work. And then I kept watching for the next couple of years as scaled up, scaled up, got better, better. And I remember having this thing of like, why is the world not paying attention to this? Mhmm.