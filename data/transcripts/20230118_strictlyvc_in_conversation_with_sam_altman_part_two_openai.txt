Unknown: and we all thought you were being hyperbolic, and DALL E so surprised people?
Sam Altman: ChatGPT and the API for like, I don't know, ten months or something before we made ChatGPT. And I sort of thought someone was gonna just build it or whatever and that enough people had played around with it. Definitely, if you make a really good user experience on top of something, like one thing that I very deeply believed was the way people wanted to interact with these models was via dialogue. And I you know, we kept telling people this, we kept trying to convince people to build it, and people wouldn't quite do it. So we finally said, alright, we're just gonna do it. But, yeah, I think the pieces were there for a while. I think one of the reasons I think Dolly surprised people is if you asked five or seven years ago that kind of ironclad wisdom on AI, First, it comes for physical labor, truck driving, working in a factory, working in a factory, then truck driving. Then the sort of less demanding cognitive labor, then the really demanding cognitive labor like computer programming, and then and then very last of all, or maybe never, because maybe it's like some deep human special sauce was creativity. And of course, we can look now and say, really looks like it's gonna go exactly the opposite direction. But I think that is not super intuitive, and so I can see why DALL E surprised people. But I genuinely felt somewhat confused about why ChaCePity did. And to get people, institutions, policymakers, get them familiar with it, thinking about the implications, feeling the technology, getting a sense for what it can do and can't do very early, rather than drop a super powerful AGI in the world all at once. And so we put GPT-three out almost three years ago. And then we put it into an API like, I think it was maybe like June two and a half years ago. the incremental update from that to ChatGPT, I felt like should have been predictable. And I want to do more introspection on why I was sort of miscalibrated on that.
Unknown: releasing things in a responsible way. I guess what gave you the confidence to
Sam Altman: We have an internal process where we try to break things study impacts. We use external auditors. We have external red teamers. We work with other labs and have safety organizations look at stuff. There are societal changes that CHAT GPT is going to cause or is causing. There's, I think, a big one going now about the impact of this on education, academic integrity, all of that. But starting these now where the stakes are still relatively low, rather than just put out what the whole industry will have in a few years with no time for society to update, I think would be bad. COVID did show us, for better or for worse, or at least me, that society can update to, like, massive changes sort of faster than I would have thought in many ways. But I still think, given the magnitude of the economic impact we expect here, more gradual is better. And so putting out a very weak and imperfect system like CHAT GPT and then making it a little better this year, a little better later this year, little better next year, that seems much better than the alternative.
Unknown: Can you comment on whether GPT-four
Sam Altman: is coming out in the first quarter, first half of the year? It'll come out at some point when we are confident that we can do it safely and responsibly. I think in general, we are going to release technology much more slowly than people would like. We're gonna sit on it for much longer than people would like. And eventually, people will be happy with our approach to this. But at the time, I realized people want the shiny toy, it's frustrating. I totally get that. thing. I don't know where it all comes from. I don't know why people don't have like better things to speculate on. I get a little bit of it, like it's sort of fun, that it's been going for like six months at this volume. People are begging to be disappointed. And they will be. Like, it's, you know, people are gonna, like, the hype is just like, It I mean, in some sense, that's what's happening. Like, we built a thing deeply imperfect as it is. We couldn't figure out how to monetize it. You could talk to it. We put it out into the world via an API. And other people, by playing around with it, figured out all these things to do. So it was not quite the like, ask thing, and it tells you how to monetize. But it hasn't gone
Unknown: So right now, modeling I'm sorry, licensing to startups. So you you are early on, and, you know, people are sort of looking at the the whole of what's happening out there, and they're saying, you know, you've got, like, Google, which could potentially release things this year. You have a lot of, you know, AI upstarts nipping at your heels. Are you worried about what you're building being commoditized and, I guess, driving the
Sam Altman: access to AI is super democratized, where there are several AGIs in the world that can kind of like help allow for multiple viewpoints and not have anyone get too powerful. And that the cost of intelligence and energy, because it gets commoditized, trends down and down and down, and the massive surplus there, access to the systems, eventually governance of the systems, benefits all of us. So, yeah, I sort of hope that happens. I think competition is good. At least, you know, until we get to AGI, I deeply believe in capitalism and competition to offer the best service at the lowest price.
Unknown: differing viewpoints,
Sam Altman: will have to agree and, like, set some laws on what an AGI can never do or what one of these systems should never do. And one of the cool things about the the path of the technology tree that we're on, which is very different like before we came along and it was sort of DeepMind having these games that were like, you know, having agents play each other and try to deceive each other and kill each other and all that, which I think could have gone in a bad direction. understand language. And so we can say, hey, you know, model, here's what we'd like you to do. Here are the values we'd like you to align to. And we don't have it working perfectly yet, but it works a little and it'll get better and better. And the world can say, alright. Here are the rules. Here's here's the very broad bounds of very broad, like, absolute rules of a system. But within that, people should be allowed very different things that they want their AI to do. And so if you want the super, like, never offend, save for work model, you should get that. And if you want an edgier one that is sort of like creative and exploratory but says some stuff you might not be comfortable with or some people might not be comfortable with, you should get that. And I think there will be many systems in the world that have different settings of the values that they enforce. And really what I think, and this will take longer, is that you as a user should be able to write up a few pages of here's what I want, here are my values, here's how I want the AI to behave, and it reads it and thinks about it and acts exactly and pressure pushes the price that people will pay token down, And how they're using your tech? It's great. They're the only tech company out there that I think we I'd be excited to partner with this deeply. I think Satya is an amazing CEO, but more than that human being and understands so so do Kevin Scott and Mikhail, who we work with closely as well, like understand the stakes of what AGI means and why we need to have all the weirdness we do in our structure and our agreement with them. And so I really feel like it's a very values aligned company, and there's some things they're very good at, like building very large supercomputers and the infrastructure we operate on and putting the technology into products. There's things we're very good at, like doing research,
Unknown: and it's been a great partnership. Can you comment on whether the reports are accurate that it's gonna be in Bing, in Office, or maybe it's already in those things?
Sam Altman: You are a very experienced and professional reporter. and products and services are tactics in service of that partnerships too. But important ones and we, like, we really wanna be useful to people.
Unknown: to its employees, we just it's too imperfect. It could harm our reputation. We're not ready. I hope. I hope when they launch something anyway, when they suspended that seven year veteran of their responsible AI organization who thought that the chatbot that he was working on for them had become sentient. I think maybe that's why you rushed out ChatGPT because yours is amazing, and if theirs is also amazing, So we talked earlier on about education. People are scared and to be scared by AI, but you guys are gonna have to sort of develop different skill sets in your lifetime that are valued. The New York public school system just restricted access to JET GPT, which is probably not as big a story as it it sort of seemed from the headline. But what do you tell educators? What are misconceptions about what you're working on? How can you kind of allay their concerns? Look, I I get it.
Sam Altman: I get why educators feel the way they feel about this. And and probably, like, I think this is just the new we're gonna try to, you know, do some things in the short term. And there may be ways we can help teachers be like a little bit more likely to detect output or anyone output of like a GPT like system. But honestly, a determined person is gonna get around them. And I don't think it'll be something society can or should rely on long term. We're just in a new world now. Like, generated text is something we all need to adapt to. And that's fine. We adapted to, you know, calculators and changed what we tested for in math classes, I imagine. This is a more extreme version of that, no doubt. But also, the benefits of it are extreme as well. We hear from teachers who are understandably very nervous about the impact of this on homework. We also hear a lot from teachers who are like, Wow, this is like an unbelievable personal tutor for each kid. And I think that I have used it to learn things myself and found it like much more compelling than other ways I've learned things in the past. Like I would much rather have ChatGPT teach me about something than go read a textbook. So, you know, it's like an evolving world and
Unknown: we'll all adapt and I think be better off for it, and we won't want to go back. Well, my 15 year old son came home one day and was using it to understand some science concepts better, which I thought was really great. Yeah. But the same kid also was like, could I use this to write my papers?
Sam Altman: watermarking technologies and other techniques. So it sounds like you don't think it's No, think we will experiment with this. Other people will, too. I think it is important for the transition. But I would caution policy, national policy, individual schools or whatever, from relying on this because I don't, like, fundamentally, I think it's impossible to make it perfect. You'll think, you know, people will figure out how much of the text they have to change. There will be other things that modify the outputted text. But I think what's important to realize is, like, the playing field has shifted. And that's fine. There's good and bad. And we just figure out, like, rather than try to go back, we figure out the way forward. So even if you develop technologies, could be sort of rendered irrelevant
Unknown: I also wanted to ask Anthropic. Arrival,
Sam Altman: Arrival in some sense, I think super highly of those people, like very, very talented and
Unknown: multiple AGIs in the world, think, is better than one. Sure. Well, what I was gonna ask, just for some background, it was founded by a former OpenAI VP of Research who you, I think, met when he was at Google. But it is stressing an ethical layer as a kind of distinction from other players. And I just wondered
Sam Altman: what the kind of the wide bounds are. But then I think individual users should have a huge amount of liberty to decide how they want their experience and their interaction to go. So I think it is like a combination of society you you know, like, we have there are a few asterisks on the free speech rules, and society has decided, like, free speech not quite absolute. I think society will decide also decide, like, language models not quite absolute. Mhmm. But there are a lot of there's a lot of speech that is legal that you find distasteful, that I find distasteful, he finds distasteful. And we all probably have somewhat different definitions of that. And I think it is very important that that is left to the responsibility of individual users and groups, not not one company, and that the government there govern and not dictate all of the rules. like, people are interested in it. We'll try to do it. Other people will try to do it. It could be, like, pretty soon. It's it's a it's a legitimate research project, so it could be pretty soon. It could take a while. Okay. I think by, like, 2028, pending, you know, good fortune with regulators, we could be plugging them into the grid. And I think we'll do a, you know, a a really great demo well before that, like, hopefully pretty soon. I mean, I I think the best case is like so unbelievably good that it's like hard to I I I think it's like hard for me to even imagine. Like I can sort of I can sort of think about what it's like when we make more progress of discovering new new knowledge with these systems than humanity has done so far, but, like, in a year instead of 70,000. I can sort of imagine what it's like when we kind of, like, launch probes out to the whole universe and find out really, you know, everything going on out there. I can sort of imagine what it's like when we have just like unbelievable abundance and systems that can sort of, you know, help us resolve deadlocks and improve all aspects of reality and kind of like let us all live our best lives. But I can't quite like I think the the the good case is just so unbelievably good that you sound like a really crazy person to start talking about it. And the bad case, and I think this is, like, important to say, is, like, lights out for all of us. I'm more worried about, like, an accidental misuse case in the short term where, you know, someone gets a super powerful like, it's not like the AI wakes up and decides to be evil. And I think all of the sort of traditional AI safety thinkers reveal a lot about more about themselves than they mean to when they talk about what they think the AGI is gonna be like. But but I can see the accidental misuse case clearly, and that's that's super bad. So I think, like, yeah, I think it's like impossible to overstate the importance of AI safety and alignment work. I would like to see much, much more happening. But I think it's more subtle than most people think and that, you know, you hear a lot of people talk about AI capabilities and AI alignment as, like, orthogonal vectors. And, you know, you're bad if you're a capabilities researcher and you're good if you're an alignment researcher. It actually sounds very reasonable, but they're almost the same thing. Like, deep learning is just gonna solve all of these problems, and so far that's what the progress has been. And progress on capabilities is also what has let us make the systems safer and vice versa, surprisingly. And so I think and The closer we get, the harder time I have answering because I think that it's gonna be much blurrier and much more of a gradual transition than than people think. If you if you imagine, like, a two by two matrix of sort of short timelines until the AGI takeoff era begins and long timelines until it begins and then a slow takeoff or a fast takeoff, the world I think we're heading to and the safest world, the one I most hope for, is the short timeline, slow takeoff. But I think people are going to have hugely different opinions about when and there you declare victory on the AGI thing. Oh. yeah. Like, when you walk down Market Street at night or, like, if I try to walk home and walk through the Tenderloin, like, late, it's not great. And I think it's, like, a real shame that we put up with treating people like this, and we continue to elect leaders who sort of don't think this is okay, but also don't fix the problem. I totally get how hard this is. I totally get how complicated this is. I also I think, unlike other tech people, will say that tech has some responsibility for it, but other cities manage to do better than this. Like, it is a solvable problem. And to entirely blame tech companies who don't get to run the city, that doesn't feel good either. And I wish there could be a more collaborative partnership instead of all of the finger pointing. I am super long in person work. I am super long the Bay Area. I'm super long California. I think we are probably going through some trying times, but I am hopeful we, like, come out of the fire better for it. maybe like one order of magnitude less of everything. Like one order of magnitude less of Yeah. I would have expected sort of one order of magnitude less on everything. And I think less hype is probably better just as like a general rule. But One of the sort of strange things about these technologies is they are impressive but not robust. And so you use them in a first demo, you kind of have this like very impressive like, wow, this is like incredible and ready to go. You use them a 100 times, you see the weaknesses. And so I think people can get a it's sort of a false impression of how good they are. However, that's all gonna get better. The critics who point these problems out and say, well, this is why it's like, you know, all like a like, you know, fake news or whatever are are equally wrong. And so, I I think it's good in the sense that people are updating to this, thinking hard about it and all of that. have occasionally used it to summarize super long emails, I've never used it to write one. I actually, summarizing documents is something I use it for a lot. It's super good at that. I use it for translation. Yeah. I think whenever someone like talks about a technology being the end of some other giant company, it's usually wrong. Like, I think people forget they get to make a counter move here and they're like pretty smart, pretty competent. But I do think it mean there is change for search that will probably come at some point, but not as dramatically as people think in the short term. My guess is that people are going to be using Google the same way people are using Google now for quite some time. And also Google for whatever this whole Code Red thing is, is probably not gonna change that dramatic, would be my guess. UBI, I think UBI is good and important, but very far from sufficient. I think it is like a little part of the solution. I think it's great. Like, think we should as wealth and resources much more than we have. And that'll be important over time. But I don't think that's gonna solve the problem. I don't think that's gonna give people meaning. Don't think it means people are gonna entirely stop trying to create and do new things and whatever else. So I sort of would consider it like an enabling technology, but not like a plan for society.
Unknown: Is that why your company, though, is a capped profit company? I mean, are you planning to take the proceeds that you're
Sam Altman: presuming you're going to make someday, and you're going to give them back to society? I mean, is that the role of Yeah. Whether we do that just by, like, saying, here's cash for everyone, totally possible, or whether we do that by saying, like, here is you know, we're gonna, like, invest all of this in a nonprofit that does a bunch of science because scientific progress is how we all make progress. Unsure, but yeah, we would like to operate for the good of society. And I think I'm a big believer in sort of design a custom structure for whatever you're trying to do. And I think AGI is just really different. And so the cap will turn out to be super important.
Unknown: Can I ask selfishly? So if UBI is only part of the solution, and I've got teenagers and we all have jobs, what should we be preparing for? As I said, my son's teacher was trying to prepare them, but, of course, you would
Sam Altman: Resilience, adaptability, ability to, like, learn new things quickly, And that changed. And now, I think learning will change again. And we'll probably adapt faster than we think.
Unknown: more questions? Thank you. You so The
Sam Altman: look, I think people are gonna do different things. I don't think there'll be one answer. And I think people will sort. The people who want fully in person will do that. People who want fully remote will do that. I think a lot of people will do, like, hybrid. I have always been a fan of going to the office a few days a week and work at home a day or two a week. YC was, like, very much like, being a YC partner was very much that way. OpenAI was that way before the pandemic. OpenAI is that way now. I personally am skeptical that fully remote is gonna be the thing that everyone does. And I think even the people who thought it was a really good idea are now sort of saying like, the next like forty years sitting in my bedroom looking at a computer screen on Zoom, do I really want that? Am I really sure? With some skepticism. There are some people who do. What I think has been the hardest is companies who are the wrong kind of hybrid where it's not like, you know, these four days everyone's in, these two days everyone's home, whatever. But it's But it's all gonna, like, continue to evolve and people will sort into what they want. I I would bet that many of the most important companies of this decade are still pretty heavily in person. past few decades of or more than a few, past like seven or eight decades of technological progress about how to do really good safety engineering and safety systems management. And a lot of that about how we like learn how to build safe systems and safe processes will translate. Imperfect, there will be mistakes, but we know how to do that. I think the AGI safety stuff is really different personally and worthy of study as its own category. And they're they're because the stakes are so high and the and the irreversible situations are so easy to imagine, we do need to somehow treat that differently and figure out a new set of safety processes and standards. The only thing that I think is like easy in a mega bubble is capital. So it was a great time to raise capital for a startup from say 2015 to '20 when did that go wrong? '20 2021. But everything else was pretty hard. It was like pretty hard to hire people. It was like pretty hard to like rise above the noise. It was pretty hard to do something that mattered without having like thousands of competitors right away. And a lot of those startups that looked like they were doing well because of the same reason capital was cheap, found that actually they were not able to like build as much enduring value as they hoped. Now, raising capital is like tough. It's still sort of reasonable I think at like seed stages but it certainly seems much tougher at later stages. But all the other stuff is much easier. You actually can concentrate talent, people are not constantly poached. You can rise above the noise threshold whether that's with like customers, the press, you know, users, whatever. I would much rather have a hard time raising capital but an easier time doing everything else than the other way around. So that's why I think it's a better time. In terms of what I would do now, I would probably go do AI for some vertical. Well,
Unknown: brought to mind this information story about Jasper that I thought was interesting. It's a customer of yours, a copywriting company relying on your AI language models. And now ChatGPT is so good that it's got to kind of find a new reason for being, I think. Is that a danger for many startups?
Sam Altman: But if I understand that it was basically like the company was saying, we had built this thing on GPT-three, and now ChatGPT is available for free, and that's causing us problems. That right? well, let me say, I think the best thing you can do to make an AI startup is the same way that a lot of other companies differentiate, which is to build deep relationships with customers, a product they love, and some sort of moat that doesn't have to be technology and network effect or whatever. And I think a lot of companies in the AI space are doing exactly that. And you've got to plan that OpenAI's models are going to get better and better. We view ourselves more as a platform company, but we will do some, you know, like a business strategy I've always really respected is like the platform plus killer app together. And so we will probably do something to help show people what we think is possible. But I think you want to build a startup, and I think Jasper is going to do this, or already is doing this, that has deep value on top of the fundamental language model. And we are a piece of enabling technology.
Unknown: Is there anybody knowing what you know or you think you see coming that should basically drop what they're doing right now because they're cooked?
Sam Altman: sure if I had more time to think about it, I could come up with an answer. But in general, I think there's gonna be way, way more new value created. Like, this is gonna be a golden few years than people who are should just, like, stop what they're doing. I would not ignore it. I think you gotta, like, embrace it big time. But I think the amount of value that's about to get created, we have not seen since