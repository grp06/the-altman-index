Unknown: Is on the oversight of our artificial intelligence, the first in a series of hearings intended to write the rules of AI. Our goal is to demystify and hold accountable those new technologies to avoid some of the mistakes of the past. If you were listening from home, you might have thought that voice was mine and the words from me. But in fact, that voice was not mine. The words were not mine. And the audio was an AI voice cloning software trained on my floor speeches. The remarks were written by Chad GBT when it was asked how I would open this hearing. And you heard just now the result. I asked ChatGPT, why did you pick those themes and that content? And it answered, and I'm quoting, Blumenthal has a strong record in advocating for consumer protection and civil rights. He has been vocal about issues such as data privacy. And the potential for discrimination in algorithmic decision making. Therefore, the statement emphasizes these aspects. Mister Altman, I appreciate Chad GPT's endorsement. In all seriousness, this apparent reasoning is pretty impressive. I am sure that we'll look back in a decade and view chat GPT and GPT four like we do the first cell phone, those big clunky things that we used to carry around. But we recognize that we are on the verge really of a new era. The audio and my playing it may strike you as curious or humorous, but what reverberated in my mind was what if I had asked it and what if it had provided an endorsement of Ukraine surrendering or Vladimir Putin's leadership? That would have been really frightening, and the prospect is more than a little scary to use the word, mister Altman, you have used yourself. And I think you have been very constructive in calling attention to the pitfalls as well as the promise, and that's the reason why we wanted you to be here today and we thank you and our other witnesses for joining us. For several months now, the public has been fascinated with GPT, GALI, and other AI tools. These examples like the homework done by chat GPT or the articles and op eds that it can write feel like novelties. But the underlying advancement of this era are more than just research experiments. They are no longer fantasies of science fiction. They are real and present. The promises of curing cancer or developing new understandings of physics and biology or modeling climate and weather, all very encouraging and hopeful, but we also know the potential harms. And we've seen them already, weaponized disinformation, housing discrimination, harassment of women and impersonation fraud, voice cloning, deep fakes. These are the potential risks despite the other rewards. And for me, perhaps the biggest nightmare is the looming new industrial revolution, the displacement of millions of workers, the loss of huge numbers of jobs, the need to prepare for this new industrial revolution in skill training and relocation that may be required. And already, industry leaders are calling attention to those challenges. To quote chat chat GPT, this is not necessarily the future that we want. We need to maximize the good over the bad. Congress has a choice now. We had the same choice when we faced social media. We failed to seize that moment. The result is predators on the Internet, toxic content, exploiting children, creating dangers for them, and senator Blackburn and I and others like senator Durbin on the judiciary committee are trying to deal with it, Kids Online Safety Act, but congress failed to meet the moment on social media. Now we have the obligation to do it on AI before the threats and the risks become real. Sensible safeguards are not in opposition to innovation. Accountability is not a burden, far from it. They are the foundation of how we can move ahead while protecting public trust. They are how we can lead the world in technology and science, but also in promoting our democratic values. Otherwise, in the absence of that trust, I think we may well lose both. These are sophisticated technology, but there are basic expectations common in our law. We can start with transparency. AI companies ought to be required to test their systems, disclose known risks, and allow independent researcher access. We can establish scorecards and nutrition labels to encourage competition based on safety and trustworthiness. Limitations on use, there are places where the risk of AI is so extreme that we ought to impose restriction or even ban their use, especially when it comes to commercial invasions of privacy for profit and decisions that affect people's livelihoods. And of course, accountability or liability. When AI companies and their clients cause harm, they should be held liable. We should not repeat our past mistakes. For example, section two thirty, Forcing companies to think ahead and be responsible for the ramifications of their business decisions can be the most powerful tool of all. Garbage in, garbage out, the principle still applies. We ought to beware of the garbage, whether it's going into the ideas that we develop in this hearing, I think, will provide a solid path forward. I look forward to discussing them with you today, and I will just finish on this note. The AI industry doesn't have to wait for congress. I hope their ideas and feedback from this discussion and from the industry and voluntary action such as we've seen lacking in many social media platforms and the consequences have been huge. So, I'm hoping that we will elevate rather than have a race to the bottom, and I think these hearings will be an important part of this conversation. This one is only the first. The ranking member and I have agreed there should be more, and we're going to invite other industry leaders. Some have committed to come, experts, academics, and the public, we hope will participate. And with that, I will turn to the ranking member, senator Hawley. the chairman of the judiciary committee and the ranking member, Thanks. Thanks, senator Ervin. It is very much a bipartisan approach, very deeply and broadly. Bipartisan and in that spirit, I'm gonna turn to my friend, senator Graham. Thank you. That was not written by AI for sure. Let me introduce now the witnesses. We're very grateful to you for being here. Sam Altman is the cofounder and CEO of OpenAI, the AI research and deployment company behind ChatGPT and DALI. Mister Altman was president of the Early Stage Startup Accelerator, Y Combinator from 1914 I'm sorry, 2014 to 2019. OpenAI was founded in 2015. Christina Montgomery, is IBM's vice president and chief privacy and trust officer overseeing the company's global privacy program policies compliance and strategy. She also chairs IBM's AI Ethics Board, a multidisciplinary team responsible for the governance of AI and emerging technologies. Christina has served in various roles at IBM, including corporate secretary to the company's board of directors. She is a global leader in AI ethics and governments, and miss Montgomery also is a member of the United States Chamber of Commerce AI Commission and the United States National AI Advisory Committee, which was established in 2022 to advise the president and the National AI Initiative Office on a range of topics related to AI. Gary Marcus is a leading voice in artificial intelligence. He's a scientist, best selling author, and entrepreneur, founder of the robust AI and geometric AI acquired by Uber, if I'm not mistaken. An emeritus professor of psychology and neuroscience at NYU, mister Marcus is well known for his challenges to contemporary AI, anticipating many of the current limitations decades in advance and for his research in human language development and cognitive neuroscience. Thank you for being here. And, as you may know, our custom on the judiciary committee is to swear in our witnesses before they testify. So if you would all please rise and raise your right hand.
Sam Altman: OpenAI was founded on the belief that artificial intelligence has the potential to improve nearly every aspect of our lives, but also that it creates serious risks we have to work together to manage. We're here because people love this technology. We think it can be a printing press moment. We have to work together to make it so. OpenAI is an unusual company, and we set it up that way because AI is an unusual technology. We are governed by a nonprofit, and our activities are driven by our mission and our charter, which commit us to working to ensure that the broad distribution of the benefits of AI and to maximizing the safety of AI systems. We are working to build tools that one day can help us make new discoveries and address some of humanity's biggest challenges, like climate change and curing cancer. Our current systems aren't yet capable of doing these things, but it has been immensely gratifying to watch many people around the world get so much value from what these systems can already do today. We love seeing people use our tools to create, to learn, to be more productive. We're very optimistic that there are going to be fantastic jobs in the future, and that current jobs can get much better. We also have seen what developers are doing to improve lives. For example, Be My Eyes used our new multimodal technology in GPT four to help visually impaired individuals navigate their environment. We believe that the benefits of the tools we have deployed so far vastly outweigh the risks, but ensuring their safety is vital to our work, and we make significant efforts to ensure that safety is built into our systems at all levels. Before releasing any new system, OpenAI conducts extensive testing, engages external experts for detailed reviews and independent audits, improves the model's behavior, and implements robust safety and monitoring systems. Before we released g p t four, our latest model, we spent over six months conducting extensive evaluations, external red teaming, and dangerous capability testing. We are proud of the progress that we made. G p t four is more likely to respond helpfully and truthfully and refuse harmful requests than any other widely deployed model of similar capability. However, we think that regulatory intervention by governments will be critical to mitigate the risks of increasingly powerful models. For example, the US government might consider a combination of licensing and testing requirements for development and release of AI models above a threshold of capabilities. There are several other areas I mentioned in my written testimony where I believe that companies like ours can partner with governments, including ensuring that the most powerful AI models adhere to a set of safety requirements, facilitating processes to develop and update safety measures, and examining opportunities for global coordination. And as you mentioned, I it's important that companies have their own responsibility here no matter what congress does. This is a remarkable time to be working on artificial intelligence. But as this technology advances, we understand that people are anxious about how it could change the way we live. We are too. But we believe that we can and must work together to identify and manage the potential downsides so that we can all enjoy the tremendous upsides. It is essential that powerful AI is developed with democratic values in mind, and this means that US leadership is critical. I believe that we will be able to mitigate the risks in front of us and really capitalize on this technology's potential to grow The US economy and the world's, and I look forward to working with you all to meet this moment, and I look forward to answering your questions. Thank you.
Unknown: Today's meeting is historic. I'm profoundly grateful to be here. I come as a scientist, someone who's founded AI companies, and as someone who genuinely loves AI, but who is increasingly worried. They can and will create persuasive lies at a scale humanity has never seen before. Outsiders will use them to affect our elections. Insiders to manipulate our markets and our political systems. Democracy itself is threatened. Chatbots will also clandestinely shape our opinions, potentially exceeding what social media can do. Choices about data sets that AI companies use will have enormous unseen influence. Those who choose the data will make the rules, shaping society in subtle but powerful ways. There are other risks too, many stemming from the in from the inherent unreliability of current systems. A law professor, for example, was accused by a chatbot of, sexual harassment, untrue, As one prominent lawyer told me on Friday, defendants are starting to claim that plaintiffs are making up legitimate evidence. Poor medical advice could have serious consequences too. An open source large language model recently seems to have played a role in a person's decision to take their own life. The large language model asked the human, if you wanted to die, why didn't you do it earlier? And then followed up with, were you thinking of me when you overdosed? Without ever referring the patient to the human help that was obviously needed. Another system rushed out and made available to millions of children told a person posing as a 13 year old how to lie to her parents about a trip with a 31 year old man. Further threats continue to emerge regularly. A month after GPT four was released, OpenAI released chat GPT plug ins which quickly led others to develop something called auto GPT with direct access to the internet, the ability to write source code, and increased powers of automation. This may well have drastic and difficult to predict security consequences. We all more or less agree on the values we would like for our AI systems to honor. We want, for example, for our systems to be transparent, to protect our privacy, to be free of bias, and above all else, to be safe. But current systems are not in line with these values. Current systems are not transparent. They do not adequately protect our privacy, and they continue to perpetuate bias. And even their makers don't entirely understand how they work. Most of all, we cannot remotely guarantee that they're safe, and hope here is not enough. Seven years later, they're largely beholden to Microsoft, embroiled in part an epic battle of search engines that routinely make things up, and that's forced Alphabet to rush out products and deemphasize safety. Humanity has taken a back seat. AI is moving incredibly fast with lots of potential but also lots of risks. We obviously need government involved, and we need the tech companies involved, both big and small. Allowing independent access to these independent scientists allowing independent scientists access to these systems before they are widely released as part of a clinical trial like safety evaluation is a vital first step. Ultimately, we may need something like CERN, global, international, and neutral, but focused on AI safety rather than high energy physics. We have unprecedented opportunities here, but we are also facing a perfect storm of corporate irresponsibility, widespread deployment, lack of adequate regulation, and inherent unreliability. AI is among the most world changing technologies ever, already changing things more rapidly than almost any technology in history. We acted too slowly with social media. Many unfortunate decisions got locked in with lasting consequence. The choices we make now will have lasting effects for decades, maybe even centuries. The very fact that we are here today in bipartisan fashion to discuss these matters gives me some hope. Thank you, mister chairman. Thanks very much, professor Marcus.
Unknown: We're gonna have seven minute rounds of questioning, and I will begin. First of all, professor Marcus, we are here today because we do face that perfect storm. Some of us might characterize it more like a bomb in a China shop, not a bull. And as Senator Hawley indicated, there are warfare era, but also the genome project, the research on genetics, where there was international cooperation as a result. And we want to avoid those past mistakes, as I indicated in my opening statement, that we're committed on social media. That is precisely the reason we are here today. ChatGPT makes mistakes, all AI does, and it can be a convincing liar, what people call hallucinations. That might be an innocent problem in the opening of a judiciary subcommittee hearing where a voice is impersonated, mine in this instance, or quotes from research papers that don't exist, but CHAT, GPT, and Bard are willing to answer questions about life or death matters, for example, drug interactions. And those kinds of mistakes can be deeply damaging. I'm interested in how we can have reliable information about the accuracy and trustworthiness of these models and how we can create competition and consumer disclosures that reward greater accuracy. The National Institutes of Standards and Technology actually already has an AI accuracy test, the face recognition vendor test. It doesn't solve for all the issues with facial recognition, but the scorecard does provide useful information about the capabilities and flaws of these systems. So there's work on models to assure accuracy and integrity. My question, let me begin with you, Mr. Altman, is should we consider independent testing labs to provide scorecards and nutrition labels or the equivalent of nutrition labels, packaging that indicates to people whether or not the content can be trusted, what the ingredients are, and what the garbage going in may be because it could result in garbage going out.
Sam Altman: that's a great idea. I think that companies should put their own sort of, you know, here are the results of our test of our model before we release it. Here's here's where it has weaknesses. Here's where it has strengths. You know, this is this is as we have, I think, said as loudly as anyone, this technology is in its early stages. It definitely still makes mistakes. We find that people, that users are are pretty sophisticated and understand where the mistakes are that they need or likely to be, that they need to be responsible for verifying what the models say, that they go off and check it. I I worry that as the models get better and better, the users can have sort of less and less of their own discriminating thought process around it, but I think users are more capable than we often give them credit for in conversations like this. A lot of disclosures, which if you've used chat GBT, you'll see about the inaccuracies of the model, are also important. And I'm excited for a world where companies publish with the models information about how they behave, where the inaccuracies are, and independent agencies or companies provide that as well. I think it's a great idea.
Unknown: I alluded in my, opening remarks to the the jobs issue, the economic effects on employment. I think you have said, in fact, and I'm going to quote, development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity, end quote. You may have had in mind the effect on on jobs, which is Let me ask you what your biggest nightmare is and whether you share that concern.
Sam Altman: Like with all technological revolutions, I expect there to be significant impact on jobs, but exactly what that impact looks like is very difficult to predict. If we went back to the the other side of a previous technological revolution, talking about the jobs that exist on the other side, you know, you can go back and read books of this. It's, what people said at the time. It's difficult. I believe that there will be far greater jobs on the other side of this, and that the jobs of today will get better. I think it's important first of all, I think it's important to understand and think about g p d four as a tool, not a creature, which is easy to get confused, and it's a tool that people have a great deal of control over in how they use it. And second, GPT four and other systems like it are good at doing tasks, not jobs. So you see already people that are using GPT four to do their job much more efficiently by helping them with tasks. Now, GPT four will, I think, entirely automate away some jobs, and it will create new ones that we believe will be much better. This happens, again, my my understanding of the history of technology is one long technological revolution, not a bunch of different ones put together, but this has been continually happening. We as our quality of life raises and as machines and tools that we create can help us live better lives, the bar raises for what we do, and and our human ability and what we spend our time going after, goes after more ambitious, more satisfying projects. So there there will be an impact on jobs. We try to be very clear about that, and I think it will require partnership between the industry and government, but mostly action by government to figure out how we want to mitigate that. But
Unknown: I'm very optimistic about how great the jobs of the future will be. Thank you. Let me ask miss Montgomery and professor Marcus for your reaction to those questions as well.
Unknown: On the subject of nutrition labels, I think we absolutely need to do that. I think that there's some technical challenges in that building proper nutrition labels goes hand in hand with transparency. The biggest scientific challenge in understanding these models is how they generalize. What do they memorize, and what new things do they do? The more that there's in the data set, for example, the thing that you want to test accuracy on, the less you can get a proper read on that. So it's important, first of all, that scientists be part of that process, and second, that we have much greater transparency about what actually goes into these systems. If we don't know what's in them, then we don't know exactly how well they're doing when we give something new, and we don't know how good a benchmark that will be for something that's entirely novel. So, I could go into that more, but I want to flag that. Second is on jobs. Past performance history is not a guarantee of the future. It has always been the case in the past that we have had more jobs, that new jobs, new professions come in as new technologies come in. I think this one's gonna be different, and the real question is over what time scale. Is it gonna be ten years? Is it gonna be a hundred years? And I don't think anybody knows the answer to that question. I think in the long run, so called artificial general intelligence really will replace a large fraction of human jobs. We're not that close to artificial general intelligence. Despite all of the media hype and so forth, I would say that what we have right now is just small sampling of the AI that we will build. In twenty years, people will laugh at this, as I think it was senator Hawley made the but maybe senator Durbin made the example about this. It was senator Durbin made the example about cell phones. When we look back at the AI of today, twenty years ago, we'll be like, wow, that stuff was really unreliable. It couldn't really do planning, which is an important technical aspect. Its reasoning was ability reasoning abilities were limited. But when we get to AGI, artificial general intelligence, maybe let's say it's fifty years, that really is going to have, I think, profound effects on labor.
Unknown: Thank you. I'm gonna ask mister Altman if he cares to respond.
Sam Altman: I I think jobs and employment and what we're all gonna do with our time really matters. I agree that when we get to very powerful systems, the landscape will change. I think I'm just more optimistic that we are incredibly creative and we find new things to do with better tools, that will keep happening. My worst fears are that we cause significant we, the field, the technology, the industry cause significant harm to the world. I think that could happen in lot of different ways. It's why we started the company. It's a big part of why I'm here today, and why we've been here in the past and we've been able to spend some time with you. I think if this technology goes wrong, it can go quite wrong, and we want to be vocal about that. We wanna work with the government to prevent that from happening, but we we try to be very clear eyed about what the downside case is and the work that we have to do to mitigate that.
Unknown: Thank you. And and our hope is that the rest of the industry will follow the example that you and IBM, miss Montgomery, have set by coming today and meeting with us as you have done privately in helping to guide what we're going to do so that we can target the harms and avoid unintended consequences to the good.
Sam Altman: survey opinion and then can help organizations into these fine tune strategies to elicit behaviors from voters? Should we be worried about this for our elections? Yeah. Thank you, senator Hawley, for the question. It's it's one of my areas of greatest concern. The the the the more general ability of these models to manipulate, to persuade, I think that's like a broader version of what you're talking about, but given that we're gonna face an election next year and these models are getting better, I think this is a significant area of concern. there's a lot of policies that companies can voluntarily adopt, and I'm happy to talk about what we do there. I do think some regulation would be quite wise on this topic. Someone mentioned earlier, it's something we really agree with people need to know if they're talking to an AI, if if content that they're looking at might be generated or might not. I think it's a a great thing to do is to make that clear. I think we also will need rules, guidelines, about what what's expected in terms of disclosure, that could have these sorts of, abilities that you talk about. So I'm nervous about it. I think people are able to adapt quite quickly. When Photoshop came onto the scene a long time ago, you know, for a while, people were really quite fooled by Photoshopped images, and then pretty quickly developed an understanding that images might be photoshopped. This will be like that, but on steroids. And the interactivity, the ability to really model, predict humans well, as you talked about, I think is going to require
Unknown: the scenario you raised was that we might basically observe people and use surveys to figure out what they're saying. But as Sam just acknowledged, the risk is actually worse, that the systems will directly, maybe not even intentionally, manipulate people. And that was the thrust of the Wall Street Journal article. And it links to an article that I've also linked to called interacting and it's not yet published, not yet peer reviewed interacting with opinionated language models changes users' views. And this comes back ultimately to data. One of the things that I'm most concerned about with GPT-four is that we don't know what it's trained on. I guess Sam knows, but the rest of us do not. And what it is trained on has consequences for essentially the biases of the system. We could talk about that in technical terms. But how these systems might lead people about depends very heavily on what data is trained on them. And so we need transparency about that, and we probably need scientists in there doing analysis in order to understand what the political influences of, for example, of these systems might be. And it's not just about politics. It can be about health. It could be about anything. These systems absorb a lot of data, and then what they say reflects that data. And they're gonna do it differently depending on what what's in that data. So it makes a difference if they're trained on the Wall Street Journal as opposed to the New York Times or or Reddit. I mean, actually, they're largely trained on all of this stuff, but we don't really understand the composition of that. And so we have this issue of potential manipulation, and it's even more complex than that because it's subtle manipulation. People may not be aware of what's going on. That was the point of both the Wall Street Journal article and the other article that I I called your attention to. Let me ask you about
Sam Altman: Yes. We should be concerned about that. To be clear, OpenAI does not But I think other companies are already, and certainly will in the future, use AI models to create, you know, very good ad predictions of what a user will like. I think that's already happening in in many ways.
Unknown: thoughts. But we will definitely see it. Maybe it will be with open source language models. I don't know. But the technology there is, let's say, partway there to being able to do that, and we'll certainly get there.
Sam Altman: Thank you for the question, senator. I I don't know yet exactly what the right answer here is. I'd love to collaborate with you to figure it out. I do think for a very new technology, we need a new framework. Certainly companies like ours bear a lot of responsibility for the tools that we put out in the world, but tool users do as well. And how we want and and also people that will build on top of it between them and the the end consumer. And how we wanna come up with a lie a liability framework there is a super important question, and we'd we'd love to work together.
Unknown: We have many agencies that can respond in some ways. For example, the FTC, the FCC. There are agencies that can. But my view is that we probably need a cabinet level organization within The United States in order to address this. And my reasoning for that is that the number of risks is large. The amount of information to keep up on is so much. I think we need a lot of technical expertise. I think we need a lot of coordination of these efforts. So there is one model here where we stick to only existing law and try to shape all of what we need to do and each agency does their own thing. But I think that AI is going be such a large part of our future and is so complicated and moving so fast. This does not fully solve your problem about a dynamic world, But it's a step in that direction to have an agency that's full time job is to do this. I personally have suggested, in fact, that we should want to do this at a global way. I wrote an article in The Economist. I have a link in here, an invited essay for The Economist behind it are obviously complicated. I'm really heartened by the degree to which this room is bipartisan and and supporting the same things. And that makes me feel like it might be possible. I I would like to see The United States take leadership in such organization. It has to involve the whole world and not just The US to work properly. I think even from the perspective of the companies, it would be a good thing. So the companies themselves do not want a situation where you take these models, which are expensive to train, and you have to have a 190 some of them, you know, one for every country. That that wouldn't be a good way of operating. When you think about the energy costs alone just for training these systems, it would not be a good model if every country has its own policies and each for each jurisdiction, every company has to train another model and maybe, you know, different states are different. So Missouri and California have different rules. And so then that requires even more training of these expensive models with huge climate impact.
Sam Altman: we do need something global. As you mentioned, this can happen everywhere. There is precedent. I know it sounds naive to call for something like this, and it sounds really hard. There is precedent. We've done it before with the IAEA. We've talked about doing it for other technologies. Given what it takes to make these models, the chip supply chain, the limited number of competitive GPUs, the power The US has over these companies, I think there are paths to The US setting some international standards that other countries would need to collaborate with and be part of that are actually workable, even though it sounds on its face like a
Unknown: parliament already is acting on an AI act. your your point is very well taken. Let me turn to senator Graham.
Sam Altman: I would say, first of all, we think that creators deserve control over how their creations are used and what happens sort of beyond the point of of them releasing it into the world. Second, I think that we need to figure out new ways with this new technology that creators can win, succeed, have a vibrant life. And I'm optimistic We'd like to we're working with artists now, visual artists, musicians, to figure out what people want. There there's a lot of different opinions, unfortunately, and at some point, we'll have Let me ask you this. Do you favor something like sound exchange I don't I don't know the numbers on Jukebox on top of my head as a research release. I can I can follow-up with your office, but it's not Jukebox is not something that gets much attention or usage? It was put out to to show that something's possible. Well, senator Durbin just said, you know, and I think it's a fair warning to you all. If we're not involved in this from the get go, and you all already are a long way down the path on this, but if we don't step in, in generative AI? Yes. We are absolutely engaged on that. Again, to reiterate my earlier point, we think that content creators, content owners need to benefit from this technology. Exactly what the economic model is, we're still talking to artists and content owners about what they want. I think there's a lot of ways this can happen. But very clearly, no matter what the law is, the right thing to do is to make sure people get significant upside benefit from this new technology, And we believe that it's really going to deliver that, but that content owners, likenesses, people totally deserve control over how that's used and to benefit from it. Okay. So on privacy we we talked about this a little bit earlier. We are quite concerned about the impact this can have on elections. I think this is an area where, hopefully, the entire industry and the government can work together quickly. There's there's many approaches, and I'll talk about some of the things we do. But before that, response that we need is different. You know, this is a tool that a user is using to help generate content more efficiently than before. They can change it. They can test the accuracy of it. If they don't like it, they can get another version. But it still then spreads through social media or other ways. Like ChatGPT is a single player experience where you're just using this. And so I think as we think about what to do, that's important to understand. There's a lot that we can and do do there. There's things that the model refuses to generate. We have policies. We also importantly have monitoring. So at scale, again, our model, the current version of GPT-four ended training in 2021. It's not a good way to find recent news, and I don't think it's a service that can do a great job of linking out, although maybe with our plug ins, it's it's possible.
Unknown: Transparency is absolutely critical here to understand the political ramifications, the bias ramifications, generated by bots. If I may speak for one second. There's a fundamental distinction between reproducing content and generating content.
Sam Altman: Yeah. We're claiming we need to work together to find a totally new approach. I don't think section two thirty is even the right framework. Yeah. We've been calling for this. We think any That's the simplest way. You get a license, I like to frame it by talking about why we deploy at all, like, why we put these systems out into the world. There's the obvious answer about there's benefits, people are using it for all sorts of wonderful things and getting great value, and that makes us happy. But a big part of why we do it is that we believe that iterative deployment and giving people and our institutions and you all time to come to grips with this technology, to understand it, to find its limitations and benefits, the the regulations we need around it, what it takes to make it safe, that's really important. Going off to build a super powerful AI system in secret and then dropping it on the world all at once, I think would not go well. So a big part of our strategy is while these systems are still relatively weak and deeply imperfect, to find ways to get people to have experience with them, to have contact with reality, and to figure out what we need to do to make it safer and better. And that is the only way that I've seen in the history of new technology and products of this magnitude to get to a very good outcome. And so that that interaction with the world is very important. Now, of course, before we put something out, it needs to meet a bar of safety. And and again, we spent well over six months with g p t four after we finished training it, going through all of these different things and deciding also what the standards were going to be before we put something out there, trying to find the harms that we knew about, put it and and and how to address those. One of the things that's been gratifying to us is even some of our biggest critics have looked at g p t four and said, wow. OpenAI made huge progress on Could focus briefly on whether or not a constitutional model that gives values would be worth it. I was just about to get there. Alright. Sorry about that. Yeah. I think giving the models values up front is an extremely important set. RLHF is another way of doing that same thing, but somehow or other, are with synthetic data or human generated data, you're saying here are the values, here's what I want you to reflect, or here are the wide bounds of everything that society will allow. Then within there, you pick as the user, you know, if you want value system over here or value system over there. We think that's very important. There's multiple technical approaches, but we need to give policymakers and the world as a whole the tools to say, here's the values and implement them. Thank you, miss Montgomery.
Unknown: I'm still feeling my way on that issue. Think global politics is not my specialty. I'm I'm an AI researcher. But I I have moved towards policy in in recent months, really, because of my great concern about all of these risks. I think certainly the UN, UNESCO, and has its guidelines Number one, a safety review like we use with the FDA prior to widespread deployment. You're going to introduce a nimble monitoring agency to follow what's going on, not just pre review, but also post as things are out there in the world with authority to call things back, which we've discussed today. And number three would be funding geared towards things like AI constitution, AI that can reason about what it's doing. I would not leave things entirely to current technology, which I think is poor at behaving in ethical fashion and behaving in honest fashion. And so, I would have funding to try to basically focus on AI safety research. That term has a lot of complications in my field. There's both safety, let's say, term and long term, and I think we need to look at both. Rather than just funding models to be bigger, which is the popular thing to do, we need to fund models to be more trustworthy. Because I wanna hear from mister Altman.
Sam Altman: and can take that license away and ensure compliance with safety standards. Number two, I would create a set of safety standards focused on what you said in your third hypothesis as the dangerous capability evaluations. One example that we've used in the past is looking to see if a model can self replicate and self exfiltrate into the wild. We can give your office a long other list of the things that we think are important there, but specific tests that a model has to pass before it can be deployed into the world. And then third, I would require independent audits, so not just from the company or the agency, but experts who can say the model is or isn't in compliance with these stated safety thresholds and these percentages of performance on question x or y.
Unknown: Thanks, senator Kennedy. Senator Corona.
Sam Altman: I'll give a few examples. One would be about violent content. Okay. Another would be about content that's encouraging self harm. Another is adult content. Not that we think adult content is inherently harmful, that we need to protect our our literally our country from harmful content? To to touch on the first part of what what you said, there are things besides, you know, should this content be generated or not that I think are also important. So that image that you mentioned was generated. I think it'd be a great policy to say generated images need to be made clear in all contexts that they were generated. And, you know, then we still have the image out there, but it's we're at least requiring people to say this was a generated image. make that reality. Where I think the licensing scheme comes in is not with not for what these models are capable of today. Because as you pointed out, you don't need a a new licensing agency to do that. But as we as we head, and, you know, this may take a long time, I'm not sure, as we head towards artificial general intelligence Mhmm. And the impact that will have, and the power of that technology, I think we need to treat that as seriously as we treat other very powerful technologies, and that's where I personally think we need such a such a scheme. I agree. And that is why, by the time we're talking about AGI,
Unknown: kind of a scheme would you contemplate? Contemplate? Well, first if I can rewind just a moment. I think you really put your finger on the central scientific issue in terms of the challenges in building artificial intelligence. We don't know how to build a system that understands harm in the full breadth of its meaning. So what we do right now is we gather examples and we say, is this like the examples that we have labeled before? But that's not broad enough. And so I thought your questioning beautifully outlined the challenge that AI itself has to face in order to to really deal with this. We want A. I. Itself to understand harm, and that may require new technology. So, I think that's very important. On this second part of your question, the model that I tend to gravitate towards, but I am not an expert here, is the FDA, at least as part of it, in terms of you have to make a safety case and say why the benefits outweigh the harms in order to get that license. Probably we need elements of multiple agencies. I'm not an expert there. But I think that the safety case part of it is incredibly important. You have to be able to have external reviewers that are scientifically qualified look at this and say, have you addressed enough? So I'll just give one specific example. Auto GPT frightens me. That's not something that OpenAI made, but something that OpenAI did make call called Chat GPT plug ins led a few weeks later to some building open source software called auto GPT. And what auto GPT does is it allows systems to access source code, access the Internet, and so forth. And there are a lot of potential, let's say, cybersecurity risks there. There should be an external agency that says, well, we need to be reassured if you're going to release this product that there aren't gonna be cyber security problems or there are ways of addressing it. So, professor, I I am running out of time. There there's you know, I just wanted to mention, miss
Sam Altman: is focused on and available in many languages. Thank you. Miss Schrupp? We think this is really important. One example is that we worked with the government of Iceland, which is a language of fewer speakers than many of the languages that are well represented on the Internet, to ensure that their language was included in our model. And we've had many similar conversations, and I look forward to many similar partnerships with lower resource languages to get them into our models. GPT-four is, unlike previous models of ours, were good at English and not very good at other languages, Now pretty good at a large number of languages. You can go pretty far down the list, ranked by number of speakers, and still get good performance. But for these very small languages, we're excited about custom partnerships to include that language into our model run. And the part of the question you asked about values and making sure that cultures are included, we're equally focused on that, excited to work with people who have particular data sets, and to work to collect a representative set of values from around the world to draw these wide bounds of what the system can do. I also appreciate what you said about the benefits of these systems and wanting to make sure we get those to as wide of a group as possible. I think these systems will have lots of positive impact on a lot of people, but in particular, underrepresented historically underrepresented groups in technology, people who have not had as much access to technology around the world, this technology seems like it can be a big lift up. Just take a stab at it. Yeah, thanks for asking, Senator Osloff. Think it's super important. I think there are very different levels here, and I think it's important that any new approach, any new law, does not stop the innovation from happening with smaller companies, open source models, researchers that are doing work at a smaller scale. That's a wonderful part of this ecosystem and of America, we don't want to slow that down. There still may need to be some rules there, but I think we could draw a line at systems that need to be licensed in a very intense way. The easiest way to do it, I'm not sure if it's the best, but the easiest would be to talk about the amount of compute that goes into such a model. So we could define that says above this amount of compute, you are in this regime. What I would prefer, it's harder to do but I think more accurate, is to define some capability thresholds I think a model that can persuade, manipulate, influence a person's behavior, or a person's beliefs, that would be a good threshold. I think a model that could help create novel biological agents would be a great threshold. Yeah. I think it's very important that we continue to understand that these are tools that humans use to make human judgments, and that we don't take away human judgment. I don't think that people should be prosecuted based off of the output of an AI system, for example. We have no I mean, think a minimum is that users should be able to to sort of opt out from having their data used by companies like ours or the social media companies. It should be easy to delete your data. I think those are it should but the thing that I think is important from my perspective running an AI company is that if you don't want your data used for training these systems, So I was speaking about something a little bit different, is the data that someone generates, the questions they ask our system, things that they input there, training on that. Data that's on the public web that's accessible, even if we don't train on that, the models can certainly link out to it. We well, you have to be I mean, you have to be 18 or up, or have your parents' permission at 13 and up to use the product, but we understand that people get around those safeguards all the time, so what we try to do is just design a safe product, and there are decisions that we make that we would allow if we knew only adults were using it, that we just don't allow in the product because we know children will use it some way or other too. In particular, given how much these systems are being used in education, we want First of all, I think we try to design systems that do not maximize for engagement. In fact, we're so short on GPUs, the less people use our products, the better. But we're not an advertising based model, we're not trying to get people to use it more and more. And I think that's a different shape than ad supported social media. Second, these systems do have the capability to influence in obvious and in very nuanced ways, and I think that's particularly important for the safety of children, but that will that will impact all of us. One of the things that we'll do ourselves, regulation or not, but I think a regulatory approach would be good for also, is requirements about how the values of these systems are set and how these systems respond to questions that can cause influence.
Unknown: what kind of encouragement do you have as specifically as possible to forming an agency, to using current rules and regulations? Can you just put some clarity on what you've already stated? Let me just insert. There are more genies yet to come from more bottles. Some genies are already out, but we don't have machines that can really, for example, self improve themselves. We don't really have machines that have self awareness, and we might not ever want to go there. So, are other genies to be concerned about. On to the main part of your question, think that we need to have some international meetings very quickly with people who have expertise in how you grow agencies, I'll just emphasize one thing I haven't as much as I would like to, which is that I think science has to be a really important part of it. And I'll give an example. We've talked about misinformation. We don't really have the tools right now to detect and label misinformation with nutrition labels that we would like to. We have to build new technologies for that. We don't really have tools yet to detect a wide uptick in cybercrime, probably. We probably need new tools there. We need science to probably help us to figure out what we need to build and also what it is that we need to have transparency around and Understood. So
Sam Altman: that. We started as a nonprofit, really focused on how this technology was going to be built. At the time, it was very outside the Overton window that something like AGI was even possible. That's that's shifted a lot. We didn't know at the time how important scale was going to be, but we did know that we wanted to build this with humanity's best interest at heart, and a belief that this technology could, if it goes the way we want, if we can do some of those things professor Marcus mentioned, What's happening on the open source community is amazing, but there will be a relatively small number of providers that can make models at the true Is there danger in that? I think there is benefits and danger to that. Like, as we're talking about all of the dangers with AI, the fewer of us that you really have to keep a careful eye on on the absolute, like, bleeding edge of capabilities,
Unknown: combined with oligarchy, where a small number of companies influence people's beliefs through the nature of these systems. Again, I put something in the walls in the record about the Wall Street Journal about how these systems can subtly shape our beliefs and that has enormous influence on how we live our lives. And having a small number of players do that with data that we don't even know about, that scares me. Sam, I'm sorry. One more thing I wanted to add. One thing that I think is very important is that this what these systems get aligned to,
Sam Altman: whose values, what those bounds are, that that is somehow set by society as a whole, by governments as a whole. And so creating that dataset, the align that our alignment dataset, it could be, you know, an AI constitution, whatever it is, that has got to come very broadly from society. One, I think America has got to continue to lead. will impact Americans and all of us wherever it's developed. But I think we want America to lead. We want the regulatory pressure should be on us, it should be on Google, it should be on the other small set of people in the lead the most. We don't want to slow down smaller startups. We don't want to slow down open source efforts. We still need them to comply with things. They can still you can still cause great harm with a smaller model. But leaving the room and the space for new ideas and new companies and independent researchers to do their work, and not putting a regulatory burden that, say, a company like us could handle but a smaller one couldn't.
Unknown: a few more questions. You've all been very, very patient and the turnout today, which is beyond our subcommittee, I think reflects both your value in what you're contributing as well as the interest in this topic, there are a number of subjects that we haven't covered Professor Marcus, which is the monopolization danger, the dominance of markets that excludes new competition and thereby inhibits or prevents innovation and invention, which we have seen in social media, as well as some of the old industries, airlines, automobiles, and others where consolidation has narrowed competition. And so I think we need to to focus on kind of an old area of antitrust, which dates more than a century, still inadequate to deal with the challenges we have right now in our economy. And certainly, we need to be mindful of the way that rules can enable the big guys to get bigger and exclude innovation and competition and responsible good guys such as are represented in this industry right now. We haven't dealt with national security. There are huge implications for national security. I will tell you, as a member of the Armed Services Committee, classified briefings on this issue have abounded and the threats that are posed by some of our adversaries. China has been mentioned here, but the sources of threats to this nation in this space are very real and urgent. in this committee. And then, on the issue of a new agency, you know, I've been doing this stuff for a while. I was attorney general of Connecticut for twenty years. I was a federal prosecutor of the US attorney. Most of my career has been in enforcement. and I'm talking not just about dollars, I'm talking about scientific expertise, you guys will run circles around. And it isn't just the the models or the generative AI that will run models around run circles around them, but it is the scientists in your company. For every success story in government regulation, you can think of five failures. It's true of the SEC. It's true of the whole alphabet list of government agencies. And I hope our experience here will be different. But the Pandora's box requires more than just the words or the concepts licensing new agency. There's some real hard decision making as miss Montgomery has alluded to about how to frame the rules to fit the risk. First, do no harm. Make it effective. Make it enforceable. Make it real. I think we need to grapple with the the hard questions here that, you know, frankly, this initial hearing, I think, has raised very successfully but not answered. And I I thank our colleagues who have participated and and made these very creative suggestions. I'm very interested in You know, literally fifteen years ago, think, advocated abolishing section two thirty. What's old is new again. You know, now people are talking about abolishing section two thirty. Back then, it was considered completely unrealistic, but enforcement really does matter. I wanna ask mister Altman, because of the the privacy issue, and you've suggested that you have an interest in protecting the privacy of the data that may come to you or be available. How do you what specific steps do you take, to protect privacy?
Sam Altman: but that's different than training on it. If you use ChatGPT, you can opt out of us training on your data. You can also delete your conversation history or your whole account.
Unknown: Professor Marcus, you made reference to self awareness, self learning. Already we're talking about the potential for jail breaks. How soon do you think that new kind of generative AI will be
Unknown: New AI that is self aware and so forth? Where? Yes. I mean, I have no idea on that one. I think we don't really understand what self awareness is. And so, it's hard to put a date on it. In terms of self improvement, there's some modest self improvement in current systems. But one could imagine a lot more and that could happen in two years, it could happen in twenty years. There are basic But it's bit hard to put timelines on them. And just going back to enforcement for one second, one thing that is absolutely paramount, I think, is far greater transparency about what the models are and what the data are. That doesn't necessarily mean everybody in the general public has to know exactly what's in one of these systems. But I think it means that there needs to be some enforcement arm that can look at these systems, can look at the data, can perform tests, and so forth. Let
Unknown: me ask you, all of you, I think there has been a reference to elections and banning outputs involving elections. Are there other areas where you think what are the other high risk or highest risk areas where you would either ban or establish especially strict rules? Ms. Montgomery.
Unknown: advice, people using these things as kind of Erzat's therapists. I think we need to be very concerned about that. I think we need to be concerned about Internet access for these tools when they can start making requests, both of people and and Internet things. It's probably okay if they just do search, but as they do more intrusive things on the Internet, like do we want them to be able to order equipment or order chemistry and so forth. So as they, as we empower these systems more by giving them internet access, I think we need to be concerned about that. And then we've hardly talked at all about long term risks. Sam alluded to it briefly. I don't think that's where we are right now. But as we start to approach machines that have a larger footprint on the world beyond just having a conversation, we need to worry about that and think about how we're going to regulate that and monitor it and so forth. In a sense, we've been talking about
Unknown: bad guys or certain bad actors manipulating AI to do harm. Manipulating people. And manipulating people, but also generative AI can manipulate the manipulators.
Unknown: And it's a wonderful metaphor. These systems are almost like counterfeit people. And we don't really honestly understand what the consequence of that is. They're not perfectly human like yet, but they're good enough to fool a lot of the people a lot of the time. And that introduces lots of problems, for example, cybercrime and how people might try to manipulate markets and so forth. So it's a serious concern. In my opening,
Unknown: I suggested three principles. accountability, and limits on use.
Sam Altman: Mister Altman? I I certainly agree that those are important points. I would add that and professor Marcus touched on this. I would add that as we we spend most of the time today on current risks, and I think that's appropriate, I'm very glad we have done it, as these systems do become more capable, and I'm not sure how far away that is, but maybe not not super far, I think it's important that we also spend time talking about how we're going to confront those challenges.
Unknown: that prospect of increased danger or risk resulting from even more complex and capable AI mechanisms certainly
Unknown: may be closer than a lot of people appreciate. Let let me just add for the record that I'm sitting next to Sam, closer than I've ever sat to him except once before in my life. people signed it. It did not call for a ban on all AI research. It only called in nor on all AI, but only on a very specific thing, which would be systems like GPT five. Every other piece of research that's ever been done, it was actually supportive or neutral about. And it specifically called for more AI re sorry, specifically called for more research on trustworthy and safe AI. So you think just so you think that we should take a moratorium, endorse My this opinion is for that the moratorium that we should focus on is actually deployment until we have good safety cases. I don't know that we need to pause that particular project, but I do think its emphasis on
Sam Altman: But I think the frame of the letter is wrong. What matters is audits, red teaming, safety standards that a model needs to pass before training. If we pause for six months, then I'm not sure what we do then. Do we pause for another six? Do we kind of come up with some rules then? we want to build on those, but we think that's the right direction, not a calendar clock pause. harms you, there's standards that we could be sued under, unless I'm really misunderstanding how things work. If the question is, are more are clearer laws about the specifics of this technology
Unknown: we there are areas like copyright where we don't really have laws. We don't really have a way of thinking about wholesale misinformation as opposed to individual pieces of it, where, say, a foreign actor might make billions of pieces of misinformation or a local actor. We have some laws around market manipulation we could apply, but we get in a lot of situations where we don't really know which laws apply. There would be loopholes. The system is really not, thought through. In fact, we don't even know that two thirty does or does not apply here, as far as I know. I think that that's something a lot of people speculated about this afternoon, but it's not solid. Well, we could fix that. Well,
Unknown: want to make a quick point here on the the issue of the moratorium. I think we need to be careful. The world won't wait. The rest of the global scientific community isn't going to pause. We have adversaries that are moving ahead and sticking our head in the sand is not the answer. Safeguards and protections, yes, but a flat stop sign sticking our head in the sand,
Unknown: for any sort of pause, I would just again emphasize there is a difference between research, I wrote an essay, which I have in the appendix, What to Expect When You're Expecting GPT-four. And I said that it would still be a good tool for misinformation, that it would still have trouble with physical reasoning, psychological reasoning, that it would hallucinate. And then along came Sydney and the initial press reports were quite favorable. And then there was the famous article by Kevin Roos in which it recommended he get a divorce. And I had seen Tay and I had seen Galactica from Meta and those had been pulled after they had problems. And Sydney clearly had problems. What I would have done had I run Microsoft, which clearly I do not, would have been to temporarily withdraw it from the market. And they didn't. And that was a wake up call to me and a reminder that even if you have a company like OpenAI that is a nonprofit and Sam's values, I think, have come clear today other people can buy those companies and do what they like with them. And, you know, maybe we have a stable set of actors now, but the amount of power that these systems have to shape our views and our lives is really, really significant. And that doesn't even get into the risks that someone might repurpose them deliberately for all kinds of bad purposes. And so, in the February,
Sam Altman: Yeah. I mean, again, that's that's so much of why we started OpenAI. We have huge concerns about that. I think it's important to democratize the inputs to these systems, the values that we're going to align to. And I think it's also important to give people wide use of these tools. When we started the API strategy, which is a big part of how we make our systems available for anyone to use, there was a huge amount of skepticism over that, and it does come with challenges, that's for sure. But we think putting this in the hands of a lot of people and not in the hands of a few companies is really quite important. And we are seeing the resultant innovation boom from that. But it is absolutely true that the number of companies that can train the true frontier models is going to be small, just because of the resources required, and so I think there needs to be incredible scrutiny on us and our competitors. I think there is a rich and exciting industry happening of incredibly good research and new startups that are not just using our models but creating their own. And I I think it's important to make sure that whatever regulatory stuff happens, whatever new agencies may or may not happen, we we preserve that fire because that's critical. I was I'm a big believer in the democratizing potential of technology, So this point that I made about use of use of the model and building on top of it, as it this is really a new platform. Right? It is definitely important to talk about who's gonna create the models. I wanna do that. I also think it's really important to decide to whose values we're going to align these models. But in terms of using the models, the people that build on top of the OpenAI API do incredible things. And it's, you know, people frequently comment, like, I can't believe you get this much technology for this little money. And so what people are the companies people are building, putting AI everywhere, using our API, which does let us put safeguards in place, I think that's quite exciting, and I think that is how it is being not how it's going to be, but how it is being democratized right now. There is a whole new Cambrian explosion of new businesses, new products, new services happening
Unknown: couldn't agree more that participation by the industry is tremendously important and not just rhetorically, but in real terms because we have a lot of industries that come before us and say, oh, we're all in favor of rules, but not those rules. Those rules we don't like. And it's every rule in fact that they don't like and I sense that there is a willingness to participate here that is genuine and authentic. I thought about asking CHAT GPT to do a new version of Don't Stop Thinking About Tomorrow because that's what we need to be doing here and as Senator Hawley has pointed out, Congress doesn't always move at the pace of technology and that may be a reason why we need a new agency, but we also need to recognize the rest of the world is going to be moving as well. And you've been enormously helpful in focusing us and illuminating some of these questions and performed a great service by being here today. So thank you to every one of our witnesses. And I'm going to close the hearing, leave the record open for one week in case anyone wants to submit anything. Encourage any of you who have either manuscripts observations from your companies to submit them to us and we look forward to our next hearing. This one is closed.