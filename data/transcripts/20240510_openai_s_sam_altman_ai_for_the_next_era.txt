Sam Altman: So I think so far we've been in the realm where it's, you know, you can do like an incredible copywriting business or you can do like a sort of like education service or whatever. I think that like a human level chat bot interface that actually works this time around, like I think like many of these trends that we all made fun of were just too early. Like the chatbot thing was good. Was just too early. services that are done through that where you get great advice or new education services like this, these are going to be very large companies. I think we'll get multimodal models in not that much longer and that'll open up new things. I think people are doing amazing work with sort of agents that can use computers to do things for you, use programs. And this idea of like a language interface very large businesses will get built with this as the interface and more generally that like these very powerful models will be one of the genuine new technological platforms which we haven't really had since mobile. large models out there that other people build on. But right now what happens is And then those those companies will create a lot of enduring value because they will have like a special version of they won't have to have created the base model but they will have created something they can use just for themselves or share with others that has this unique data flywheel going that sort of improves over time and all of that. So I think there will be a lot of value created in that middle layer. I think the biggest systemic mistaken thinking people are making right now is they're like, alright, maybe I was skeptical but this language model thing is really going to work and sure, like images, video too. It's not going to be generating net new knowledge for humanity. It's just going to like do what other people have done and, you know, that's still great. That still like whatever, like AlphaFold and those are adding huge amounts of value and you're gonna see in this like you know, had time to do something else I would be so excited to go after a bio company right now. I think you can just do amazing things there. But there's like another thing that's happening which is like tools that just make us all much more productive, that help us think of new research directions, that sort of write a bunch of our code so we can be twice as productive. And that impact on the net output of one engineer or scientist I think will be the surprising way that AI contributes to science that is outside of the obvious models. But even just seeing now what I think these tools are capable of doing, Copilot as an example, know, would be much cooler stuff than that. That will be a significant change to the way that technological development, scientific development happens. lead to like just an acceleration of progress. I hesitate to use this word because I think there's one way it's used which is fine and one that is more scary. But AI that can start to And so can we automate our own jobs as AI developers very first? The very first thing we do. Can that help us solve the really hard alignment problems that we don't know how to solve? That honestly I think is how it's going to happen. you know, editing your own code and changing your optimization algorithm and whatever else. But there's a less scary version of self improvement which is like kind of what humans do, which is if we try to go off and like process we do that is special to humans, teaching AI to do that, I'm very excited to see what that does for the total I'm a big believer that the only real driver of human progress and economic growth over the long term is the the structure, the societal structure that enables scientific progress and then scientific progress itself. And I think we're gonna make a lot more of that. either in conflict with ours, many sci fi movies about what happens there, or goals where it just doesn't care about us that much. And so the alignment problem is how do we build AGI that does future of humanity? The way that I think the self improving systems help us is not necessarily by the nature of self improving but we have some ideas about how to solve the alignment problem at small scale. And we've been able to align OpenAI's biggest models better than we thought we would at this point. So that's good. I'll start with like the higher certainty things. I think language models are going to go just much, much further than people think. And we're very excited to see what happens there. I think it's like what a lot of people say about running out of compute, running out of data, That's all true but I think there's so much algorithmic progress to come that Another thing is I think we will get true multimodal models working. And so not just text and images but every modality you'd like in one model able to easily fluidly move between things. I think we will have models that continuously learn. So like right now if you use GPT, whatever, it's sort of stuck in time that it was trained and the more you use it, it doesn't get any better and all of that I think will get that changed. So very excited about all of that. And if you just think about what that alone is going to unlock and the sort of applications people will be able to build with that, would be like a huge victory for all of us and just like But I think we're likely to keep making research progress into new paradigms as well. We've been pleasantly surprised on the upside about what seems to be happening. And I think But historically that's a very bad sign for new startup creation or whatever if everybody is like, I'm this with AI. And that's definitely happening now. So a lot of the we were talking about there are all these people saying I'm doing these RL models for fusion or whatever and as far as we can tell they're all much worse than what smart physicists have figured out. I think it is just an area where people are going to say everything is now this plus AI. Many things will be true. I do think this will be like the biggest technological platform of the generation. But I think it's like we like to make predictions where we can be on the frontier, understand predictably what the scaling laws look like or already have done the research where we can say, all right, this new thing is going to work and make predictions out from that way. And that's sort of like how we try to run OpenAI, which is do the next thing in front of us when we have high confidence and take 10% of the company to just totally go off and explore which has led to huge wins and there will be like, But I think that's the way to make predictions. Don't pay attention to the AI for everything. Can I see something working and can I see how it predictably gets better? My basic model of the next decade is that the cost of intelligence, And those I think are two of the major inputs into the cost of everything else except the cost of things we want to be expensive, the status goods, whatever. And because these seismic shifts that happen when the whole cost structure of society changes, happened many times before, temptation is always to underestimate those. prediction about anything that doesn't change a lot or that doesn't get to be applied. But one of the things that is important is it's not like the thing amount of Like, what happens then? Yep. a new container for software and a new computer interaction thing. And AI turns out to be something on the order of a legitimate technological revolution. So I think the currently available models life sciences researchers have told me. They've all looked at it and they're like, it's a little helpful in some cases. There's been some promising work in genomics but stuff on a benchtop hasn't really impacted it. I think that's going to change and I think this is one of these areas where there will be these new 100,000,000,000 to trillion dollar companies started. Those areas are rare. But when you can really change the way that if you can really make a future of pharma company that is just hundreds of times better than what's out there today, that's going be really different. As you mentioned, there still will be the rate limit of bio has to run at its own thing and human trials take however long they take. So I think an interesting cut of this is where can you avoid that? Where are the synthetic bio companies that I've seen that have been most interesting are the ones that find a way to make the cycle time super fast and that benefits an AI that's giving you a lot of good ideas but you've still got to test them which is where things are right now. I'm a huge believer for startups that the thing you want is low costs and fast cycle times. And if you have those, you can then compete as a startup against the big incumbents. And so I wouldn't go pick cardiac disease as my first thing to go after right now with this new kind of company. But you know, using bio to manufacture something, that sounds great. I think the other thing is the simulators are still so bad. And if I were a bio meets AI startup, would certainly try to work on that somehow. Sort of all of the deep biological things. I think we will still really care about interaction with other people. We'll still have fun and the reward drives to create new things and compete for silly status and like, you know, form families and whatever. It's not this is not a utopian one. Maybe. I I think the last question is like an incredible short story. This is like a question that comes up at OpenAI a lot. Like how do I think about how should one think about having kids? There's I think no consensus answer to this. And I think that's what will bring me fulfillment. I think as always it is a personal decision. I get very depressed when people are like, I'm not having kids because of AGI. I want total freedom. But I think all of those I find quite depressing. I think having a lot of kids is great. But you will just like you know, either with text or voice depending on the context, you will just like interface in language and get the computer to do whatever you want. And that will about going that extra mile. A 100%. I just hope it's not like figuring out how to like hack the prompt by adding one magic word to the end that changes everything else. What will matter is the quality of ideas and the understanding of what you want. So the artist will still do the best with image generation AGI is basically the equivalent of a median human that you could like hire as a coworker. So and then they could like say do anything that you'd happy with a remote coworker doing, like just behind a computer, learning how to go be a doctor, learning how to go be a very competent coder. There's a lot of stuff that a median human is capable of getting good at. And I think one of the skills of an AGI is not any particular milestone but the meta skill of learning to figure things out and that it can go decide to get good at whatever you need. kind of like AGI. And then superintelligence is one that's smarter than all of humanity put together. divergent as I think it could be for some people doing incredibly well and others not, I think society just won't tolerate it this time. And so figuring out when we're going to disrupt so much of economic activity. What is the new social contract? My guess is that the things that we'll have to figure out are how we think about fairly distributing wealth, access to AGI systems which will be like kind of the commodity of the realm, and governance like how we collectively decide what they can do, what they don't do, things like that. I'm optimistic that people will figure out how to spend their time and be very fulfilled. Think people worry about that in a little bit of a silly way. I'm sure what people do will be very different but we always solve this problem. But I do think the concept of wealth and access and governance, you know, I think we should have like 10 more things like that that we try. We've explored more recently how this technology can be used for reskilling people that are going to be impacted early. I I think and I think we're seeing this now that tools for creatives are that that is gonna be like the great application of AI in the short term. It is mostly enhancing. It's replacing in some cases but for the majority of the kind of work that people in these fields want to be doing, it's enhancing. And I think we'll see that trend continue for a long time. Eventually, yeah, it probably is just like we look at 100. Okay. It can do the whole creative job. I think it's interesting that if you asked people ten years ago about how AI was going to have an impact with a lot of confidence from almost most people, you would have heard, you know, first it's going come for the blue collar jobs, Then the very high skill, like really high IQ white collar jobs like a programmer or whatever. And then very last of all, and maybe never, it's going to take the creative jobs. And it's really gone exactly and is going exactly the other direction. And I think this there's an interesting reminder in here generally about how hard predictions are but more specifically about, you know, we're not always very aware, maybe even ourselves, of what skills are hard and easy. Like what uses most of our brain and what doesn't, or how difficult bodies are to control or make or whatever. you know, base models that are hugely trained with a gigantic amount of compute and data and then they will train on top of those to create the model for each vertical. And those startups so in some sense they are training their own models, just not from scratch. But they're doing the 1% of training that really matters for whatever this use case is going to be. Those startups I think, they will be hugely successful and very differentiated startups there. But that'll be about the kind of data flywheel that the startup is able to do, the kind of all of the pieces on top and below. This could include prompt engineering for a while or whatever, the sort of the kind of like core I think that's just gonna get too complex and too expensive and the world also just doesn't make enough chips. You may as well you gotta plan for the worst. You certainly it's not a strategy to say it's all gonna be okay. But you may as well