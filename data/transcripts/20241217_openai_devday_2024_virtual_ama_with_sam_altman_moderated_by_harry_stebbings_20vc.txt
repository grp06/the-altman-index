Unknown: Hello everyone. Welcome to OpenAI Dev Day. I am Harry Stebbings of 20VC and I am very very excited to interview Sam Altman. Welcome Sam. Sam, thank you for letting me do this today with you. We have many many questions from the audience and so I wanted to start with one which I don't think people ask you actually very often in interviews which is firstly, how are you? You are one of the busiest people on the planet. You also always look remarkably fresh. Okay. Listen. I wanna start by kind of diving in. We had a lot of fantastic questions from the audience across a number of different kind of areas, And I want to start with actually the question of when we look forward, is the future of OpenAI more models like o one or is it more larger models that we would maybe have expected of old? How do we think about that?
Sam Altman: for example, contribute to new science, help write a lot more very difficult code, that I think can drive things forward to a significant degree. So you should expect rapid improvement in the o series of models, and it's of great strategic importance to us. But eventually, I think we can offer really high quality no code tools. And already, there's some out there that make sense. But you can't you can't sort of in a no code way say, I have, like, a full startup I wanna build. That's going to take a while. The the the general answer we try to give is and you have to assume that we're biased here and talking our book and may be wrong. But the general answer we try to give is we are gonna try our hardest and believe we will succeed at making our models better and better and better. And if you are building a business that patches some current small shortcomings, if we do our job right, then that will not be as important in the future. If on the other hand, you build a company that benefits from the model getting better and better, if, you know, an oracle told you today that o four was gonna be just absolutely incredible and do all of these things that right now feel impossible and you were happy about that, then, you know, maybe we're wrong but at least that's what we're going for. And if instead you say, okay, there's this area where and someone attaches and just barely get it to work, And that is the general philosophical message we try to get out to startups. Like, we we believe that we are on a pretty a quite steep trajectory of improvement, and that the current shortcomings of the models today will just be taken care of by future generations. And you know, I would encourage people to be aligned with that.
Unknown: If you're thinking as a founder today building, where is OpenAI gonna potentially come and steamroll versus where they're not? Also for me as an investor, trying to invest in opportunities that aren't going to get damaged.
Sam Altman: market cap that gets created, new market cap that gets created by using AI to build products and services that were either impossible or quite impractical before. And the There is this one set of areas where we're gonna try to make it relevant, which is, you know, we just want the models to be really, really good such that you don't have to, like, fight so hard to get them to do what you wanna do. But all of this other stuff, which is building these incredible products and services on top of this new technology, we think that just gets better and better. of startups, something like that, wanted to bet against the models getting way better. And so and they were doing these things where we could already see GPT four coming and we're like, man, it's gonna be so good. It's not gonna have these problems. If you're building a tool just to get around this one shortcoming of the model, that's gonna become less and less relevant. And we forget how to like to plug a hole rather than to build something to go deliver like the great AI tutor or the great AI medical advisor or whatever. And and so I felt like 95% of people that were were like betting against the models getting better, 5% of people were betting for the models getting better. I think that's now reversed. I think people have like
Unknown: But there will be $9,000,000,000,000 of value created every single year which will offset the $9,000,000,000,000 CapEx that he thought would be needed. I'm just intrigued. How did you think about that when you saw that? How do you reflect on that?
Sam Altman: This happens with every other mega technological revolution of which this is clearly one. But, You talked about when there could be like a no code software agent. I don't know how long that's gonna take, but if we use that as an example and imagine forward towards it, think about what think about how much economic value gets unlocked for the world. If anybody can just describe like a whole company's worth of software that they want. This is a ways away, obviously. But when we get there and have it happen, think about how difficult and how expensive that is now. Think about how much value it creates if you keep the same amount of value but make it wildly more accessible and less expensive. That that's really powerful. And I think we'll see many other examples like that. We I mentioned earlier, like, health care and education, but those are two that are both, like trillions of dollars of value to the world to get right if you and if AI can really really truly enable this to happen in a different way than it has before. don't think big numbers are the point, and they're also the debate about whether it's 9,000,000,000,000 or 1,000,000,000,000 or whatever like, you know, I don't smarter people than me it takes to figure that out. But but the value creation does seem just unbelievable here.
Unknown: for which it's valued, open source is an incredibly prominent method through which it could be. How do you think about the role of open source in the future of AI and how does internal discussions look like for you when the question comes, should we open source any models
Sam Altman: There's clearly a really important place in the ecosystem for open source models. There's also really good open source models that now exist. I think there's also a place for, like, nicely offered, well integrated services and APIs. And, you know, I think it's
Unknown: As a delivery mechanism, we have the open source as a kind of end prop to customers and a way to deliver that, we can have agents. I think there's a lot of
Sam Altman: This is like my off the cuff answer. It's not well considered, but something that I can give a long duration task to and provide minimal supervision during execution for. Maybe I can give the following example. When people talk about an AI agent acting on their behalf, the the main example they seem to give fairly consistently is, oh, you can, like, And either it can like use OpenTable or it can like call the restaurant or or whatever. And, you know, it's like okay, sure, that's that's like a mildly annoying thing to have to do and it maybe like saves you some work. you can just do things that you wouldn't or couldn't do as a human. So what if what if instead of calling one restaurant to make a reservation, my agent would call me like 300 and figure out which one had the best food for me or some special thing available or whatever. And then you would say, well, that's like really annoying if your agent is calling 300 restaurants. But if if it's an agent answering each of those three hundred three hundred places, then no problem. And it can be this like massively parallel thing that a human can't do. So that's like a trivial example, but there are these like limitations to human bandwidth that maybe these agents won't have. The category I think that was more interesting is not the one that people normally talk about where you have this thing calling restaurants for you. But something that's more like a really smart senior coworker where you can, like, collaborate on a project with and the agent can go do, like, a two day task or two week task really well and you know, ping you when it has questions but come back to you with like a great work product.
Unknown: extraction of value bluntly and normally it's on a per seat basis but now you're actually kind of replacing labor so to speak. How do you think about the future of pricing with that in mind when you are such a core part of an enterprise workforce?
Sam Altman: I mean, I could imagine a world where you can say like, I want one GPU or 10 GPUs or a 100 GPUs to just be like churning on my problems all the time. And it's not like you're not like paying per seat or even per There's a huge amount of infrastructure and scaffolding to build for sure, but I think o one points the way to a model that is capable of doing great agentic tasks.
Unknown: On the model side, Sam, everyone says that models are depreciating assets. The commoditization of models is so rife. How do you respond and think about that? And when you think about the increasing capital intensity to train models, are we actually seeing the reversion of that where it requires so much money that actually very few people can do it?
Sam Altman: It's definitely true that they're depreciating assets. This thing that they're not though worth as much as they cost to train, that seems totally wrong. To say nothing of the fact that there's like a there's a positive compounding effect as you learn to train these models, you get better at training the next one. But the actual, like, revenue we can make from a model, I think, justifies the investment. I to be fair, I don't think that's true for everyone. And there's a lot of there are probably too many people training very similar models. And if you're a little behind or if you don't have a product with the sort of normal rules of business that make that product sticky and valuable, We're very fortunate to have ChatGPT and hundreds of millions of people that use our models. And so even if it costs a lot, we get to like amortize that cost across a lot of people.
Unknown: How do you think about how OpenAI models continue to differentiate over time and where you most wanna focus to expand that differentiation?
Sam Altman: Reasoning is our current most important area of focus. I think this is what unlocks the next, like, massive leap forward in in value created. So that's we'll improve them in lots of ways. We will do multimodal work. We will do other features in the models that we think are super important to
Unknown: love to understand that. Reasoning in multimodality specific?
Sam Altman: you know, like, people, like, when they're babies and toddlers before they're good at language can still do quite complex visual reasoning. So clearly, this is possible.
Unknown: Totally is. How will vision capabilities scale with new inference time paradigm set by o one?
Sam Altman: Without spoiling anything, I would expect rapid progress in image based models.
Unknown: with models, different cultures, different languages, and how important that is?
Sam Altman: use British English. I haven't tried, but I would have guessed that it's really good at doing British English. Is it not?
Unknown: How does OpenAI make breakthroughs in terms of like core reasoning? Do we need to start pushing into reinforcement learning as a pathway or other new techniques aside from the transformer?
Sam Altman: The how we do it is like our special sauce. It's easy. It's really easy to copy something you know works. And one of the reasons that people don't talk about about why it's so easy is you have the conviction to know it's possible. And so after after a research lab does something, even if you don't know exactly how they did it, it's I won't say easy, but it's doable to go off and copy it. And you can see this in the replications of GPT four, and I'm sure you'll see this in replications of o one. What is really hard and the thing that I'm most proud of about our culture is the repeated ability to go off and do something new and totally unproven. And a lot of organizations no, I'm not talking about AI research, just generally. A lot of organizations talk about the ability to do this. There are very few that do across any field. And in some sense, I think this is one of the most important inputs to human progress. So one of the, like, retirement things I fantasize about doing is writing a book of everything I've learned about how to build an organization and a culture that does this thing, not the organization that just copies what everybody else has done. Because I think this is something that the world could have a lot more of. It's limited by human talent, but there's a huge amount of wasted human talent because this is not an organization style or culture, whatever you wanna call it that we are all good at building. So I'd love way more of that and that is I think the thing most special about us. Sam, how is human talent wasted? Oh, there's just a lot of really talented people in the world that are not working to their full potential because they work at a bad company or they live in a country that doesn't support any good companies or a long list of other things. I mean, the the one of the things I'm most excited about with AI is I hope it'll get us much better than we are now at helping get everyone to their max potential,
Unknown: Sam, you've had an incredible journey again, sorry for the off script. You've had an incredible journey over the last few years through, unbelievable hyper growth. You say about writing a book there in retirement. If you reflect back on the ten years of leadership change that you've undergone, how have you changed your leadership most significantly?
Sam Altman: is just the rate at which things have changed. At a normal company, one of the things that just came to mind out of like a rolling list of a 100 is how hard it is how much active work it takes to get the company to focus not on how you grow the next 10%, but the next 10 x. And growing the next 10%, it's the same things that worked before will work again. But to go from a company doing, say, like, a billion to $10,000,000,000 in revenue requires a whole lot of change, and it is not the sort of, like, let's do last week what we did this week mindset. And in a world where people don't get time to even get caught up on the basics because growth is just so rapid, I I badly underappreciated the amount of work it took to be able to, like, keep charging at the next big step forward while still not neglecting everything else that we have to do. how you build the structures to, like, get the company to get good at thinking about 10 x more stuff or bigger stuff or more complex stuff every eight months, twelve months, whatever. There's a big piece in there about planning, about how you balance what has to happen today and next month with the the long lead pieces you need in place for even, you know, things that are more normal, like planning ahead enough for, like, office space in a city like San Francisco is surprisingly hard at this kind of rate. there was either no playbook for this or someone had a secret playbook they didn't give me,
Unknown: Keith Raboit did a talk and he said about you should hire incredibly young people 30 and that is what Peter Thiel taught him and that is the secret to building great companies. And it got a little bit of resistance to say the least. I'm intrigued when you think about this book that you write in retirement and that advice, you build great companies by building incredibly young, this like Trojan horse of youth energy ambition but less experience or the much more experienced, I know how to do this, I've done it before?
Sam Altman: I mean the obvious answer is you can succeed with hiring both classes of people. Like, we have I was just like, right before this, I was sending someone a Slack message about there was a guy that we recently hired on one of the teams. I don't know how old he is, but low twenties probably, doing just insanely amazing work. And I was like, can we find a lot more people like this? This is just, like, off the charts, brilliant. I don't get how these people can be so good, so young. But it clearly happens. And when you can find those people, they bring amazing fresh perspective energy, whatever else. On the other hand, when you're, like, designing some of the most complex and massively expensive computer systems that humanity has ever built, actually like pieces of infrastructure of any sort, then I would not be comfortable taking a bet on someone who is just sort of like starting out where the stakes are higher. So you want you want both. And I think what you really want is just, like, an extremely high talent bar of people at any age and a strategy that said, I'm only gonna hire younger people or I'm only gonna hire older people, I believe would be misguided. I I think it's, like, somehow just it's not quite the framing that resonates with me. But the part of it that does is and one of the things that I feel most grateful about Y Combinator for is inexperience does not inherently mean not valuable. high potential people at the very beginning of their career that can create huge amounts of value. And we as a society should bet on those people and it's a great thing.
Unknown: I am gonna return to some semblance of the schedule as I'm I'm really gonna get told off. But anthropics models have been sometimes cited as being better for coding tasks. Why is that? Do you think that's fair and how should developers think about when to pick OpenAI versus a different provider?
Sam Altman: Yeah. They have a model that is great at coding for sure and it's impressive work. I think developers use multiple models most of the time, agentified world. But and something about the way that we currently talk about it or think about it feels wrong. May maybe if I had to describe it, we will shift from talking about models to talking about systems, but that'll take a while. Without going into detail about how it's going to happen, the the the core of the question that you're getting at is, is the trajectory of model capability improvement going to keep going like it has been going? And the answer that I believe is yes for a long time. Well, when we started working on GPT four, there were some issues that caused us a lot of consternation that we really didn't know how to solve. We figured it out, but there was there was definitely a time period where we just didn't know how we were gonna do that model. And then in this shift to o one and the idea of reasoning models, that was something we had been excited about for a long time. But it was like a long and winding road of research to get here.
Unknown: Is it difficult to maintain morale
Sam Altman: and that that's a very motivating thing. And no one expects that to be easy and a straight line to success, but it does feel like There's a famous quote from history. It's something like I I'm gonna get this totally wrong. But the spirit of it is like, I never pray and ask for God to be on my side. You know, I pray and hope to be on God's side. And there is something about betting on deep learning that feels like being on the side of the angels, and
Unknown: What unmade decision weighs on your mind most?
Sam Altman: bet on this next product or that next product? Or are we gonna like build our next computer this way or that way? They they are kind of like really high stakes one way door ish that like everybody else I probably delay for too long. But but mostly the hard part is every day, it feels like there are a few new fifty one forty nine decisions that come up that kinda make it to me because they were fifty one forty nine in the first place. And then I don't feel like particularly likely I can do better than
Unknown: Is there a commonality in the person that you call when it's fifty one forty nine?
Sam Altman: I think the wrong way to do that is to have one person you lean on for everything. And the right way to at least for me, the right way to do it is to have, like, 15 or 20 people, each of which you have come to believe has good instincts and good context in a particular way. And you get to like phone a friend to the best expert rather than try to have just one across the board.
Unknown: How worried are you about semiconductor supply chains and international tensions today?
Sam Altman: I don't know how to quantify that. Worried, of course, is the answer. It's probably not it's well, I guess I could quantify it this way. It is not my top worry, but it is in, like, the top 10% of all worries. the the sort of generalized complexity of all we, as a whole field, are trying to do. and it feels like a think it's all gonna work out fine, but it feels like a very complex system. Now this kind of, like, works fractally at every level. So you can say that's also true, like, inside of OpenAI itself. That's also true inside of any one team. But, you know, an example of this since you were just talking about semiconductors is you gotta balance the power availability with the right networking decisions, with being able to, like, get enough chips in time and whatever risk there is going to be there with the ability to have the research ready to intersect that so you don't either, like, be caught totally flat footed or have a system that you can't utilize with the right product that is going to use that research to be able to pay the eye watering cost of that system. So the it's supply chain makes it sign sound too much like a pipeline, but but, yeah, the overall ecosystem complexity at every level of, like, the fractal scam is unlike anything I have seen in any industry before. And there's an interesting point here which is everybody likes to use previous examples of a technology revolution to talk about to put a new one into more familiar context. And, a, I think that's a bad habit on the whole. And but I understand why people do it. And b, I think the ones people pick for analogizing to AI are particularly bad. So the Internet was obviously quite different than AI, and you brought up this one thing about cost. And whether it costs like 10,000,000,000 or a 100,000,000,000 or whatever to be competitive, it was very like, one of the defining things about the Internet revolution was it was actually really easy to get started. Now, another thing that cuts more towards the Internet is for many companies, this will just be like a continuation of the Internet. It's just like someone else makes these AI models and you get to use them to build all sorts of great stuff and it's like a new primitive for building technology. But if you're trying to build the AI itself, that's pretty different. Another example, people use electricity, which I think doesn't make sense for a ton of reasons. The one I like the most, caveated by my earlier comment that I don't think people should be doing this, or trying to like use these analogies too seriously is the transistor. It was a new discovery of physics. It had incredible scaling properties. It seeped everywhere pretty quickly. You know, we had things like Moore's Law in a way that we could now imagine like a bunch of laws for AI that tell us something about how quickly it's going to get better. And everyone kind of bent like the whole tech industry kind of benefited from it. And there's a lot of transistors involved in the products and delivery of services that you use, but you don't really think of them as transistor companies. It's there's a very complex, very expensive industrial process around it with a massive supply chain. And, you know, the the incredible progress based off of this very simple discovery of physics led to this gigantic uplift of the whole economy for a long time, even though most of the time you're think about it and you don't say, oh, this is a transistor product. It's just like, oh, alright. This thing can like process information for me.
Unknown: building today as a whatever twenty three, twenty four year old with the infrastructure that we have today.
Sam Altman: Some AI enabled vertical. I'll I'll I'll use tutors as an example, but like the the the best AI tutoring product or the, you know, that I could possibly imagine to teach people to learn. Any category like that. Could be the AI lawyer. Could be the sort of like AI CAD Doesn't have to like literally be infinite context, but some way that you can have an AI agent that like knows everything there is to know about you, has access to all of your data, I mean, I kinda respect everybody in the space right now. I think there's, like, really amazing work coming from the whole field. I and incredibly talented, incredibly hardworking people. I don't mean this to be a question, Dodge. It's like I can point to super talented people doing super great work everywhere in the field. Like ever or doing current work? Doing current work. Well, I feel like I I feel like I can't just go name a bunch of OpenAI people because that would be unfair. And it would sound like I'm just biased. So let me try to think of a non OpenAI person. do what they've done and built in terms of using AI to deliver a really magical change between them, like, in the same way that but if you were like, hey, Sam, I want you to go, like, make a new important discovery in physics, you'd probably be happy to wait a couple of years. And the answer is it should be user controllable.
Unknown: When you think about maybe an insecurity in leadership, an area of your leadership that you'd like to improve, where would you most like to improve as a leader and a CEO today?
Sam Altman: The thing I'm struggling with most this week is I feel more uncertain than I have in the past about what our like, the details of what our product strategy should be. I think that product is a weakness of mine in general, and it's something that right now the company like needs stronger and clearer vision on from me. Like we have a wonderful head of product and a great product team, but it's an area that I wish I were a lot stronger on and acutely feeling the the miss right now. You hired Kevin. what we're gonna say no to, like, really trying to speak on behalf of the user about why we would do something or not do something, like, really trying to be rigorous about not not having like fantastical dreams.
Unknown: massive. Okay. I'll change it then. Have a five year horizon for OpenAI and a ten year. If you have a magic wand and can paint that scenario for the five year and the ten year, can you paint that canvas for me for the five and ten year?
Sam Altman: two years, but if we are right and we start to make systems that are so good at, you know, for example, helping us with scientific advancement. Actually, I I will just say it. I think in five years, it looks like we an unbelievably rapid rate of improvement in technology itself. You know, people are like, man, the AGI moment came and went, whatever. The, like, the pace of progress is, like, totally crazy, and we're discovering all this new stuff both about AI research and also about all of the rest of science. like if we could sit here now and look at it, it would seem like it should be very crazy. And then the second part of the prediction is that society itself actually changes surprisingly little. An example of this would be that I think if you asked people five years ago if computers were gonna pass the Turing test, they would say no. And then if you said, well, what if an oracle told you it was going to? They would say, well, it would somehow be like just this crazy breathtaking change for society. And we did kinda satisfy the Turing test, roughly speaking, of course. And society didn't change that much. It just sort of went whooshing by. And that's kind of a example of what I expect to keep happening, which is progress scientific progress keeps going, outperforming all expectations. And society in a way that I think is good and healthy changes not that much.