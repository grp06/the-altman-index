Sam Altman: a significant thing that happened was the shift with these large unsupervised models. And then once they had that represented, you could, like, use them to, like, do these more sophisticated tasks. But this idea that you were just gonna go, like, read all of the text ever written by humans on the Internet with no particular supervision signal, just trying to predict the next word one at a time, was, like, a very laughable idea when we started doing that. And it's gone, like, further certainly than I thought it would go and I think most people in the field. But but, really, that one, like, fundamental idea that we can, you know, scale up these large unsupervised models and get these, like, quite surprising results, get the zero shot learning working, which I still think is, like, somewhat miraculous. You know, Copilot has been great for coding, but we're we're pretty optimistic that there can be, like, Copilot for x for many values of x. And the amount of knowledge work augmentation that that can do, even if we never figure out another big research idea in all of the OpenAI with Microsoft. And I I think without doing that, people wouldn't quite appreciate that. I too small or too distracted to do. One one particular category is powerful small models. So models that are meant to do one thing really well, but not the big general purpose stuff we do or meant to run on, like, one piece of consumer hardware.
Unknown: there tends to be waves where all the value accrues to startups and waves where all the value accrues to incumbents. So if you look at the first Internet wave, for example, a huge chunk of the market cap went to startups. It went to Google. It went to Amazon. And then a chunk of it went to incumbents like Microsoft or Apple. And then you fast forward and you go to mobile. And with mobile, it was again kind of a fifty fifty split. Right? Like, there was Instagram and WhatsApp, And then there's waves where all the value goes to to startups like crypto. Right? There's really no incumbent value that's really been captured by crypto until now. It seems like all the value roughly went to incumbents. Right? And so when I think of all the various AI startups that I know got founded or funded or people invested in or worked on, very few of them actually created any real breakouts. Right? The biggest things were TikTok, which is really a subsidiary of ByteDance, Google, Netflix with this recommendation engine,
Sam Altman: you can kinda squint and tell yourself the story either way of the incumbents get more powerful or the upstarts always win is, like, the the default. they've been more powerful for longer and and less beatable by startups than they're supposed to be. But I I do wonder if we're just, like, heading into a new world where this is the shape of things to come and the value of investors making ton of money because we're in this, like, up elevator and valuations went nuts. But a a relatively small number of companies, particularly in the later last three quarters of the decade, it felt like they're gonna be the, like, future trillion dollar platform companies. called joint stock corporations, and and that led to this, like, enabled this just unbelievable I I mean, I think we can say all of the sort of trite things at this point. Like, I I do believe that natural language is this new interface. That's real, and that's gonna happen. And and, you know, we will control lots of things. Not everything, because sometimes, like, GUI is better. But we will control lots of things by, like, telling a computer what we want. And But, also, an alien intelligence that can run at, like, just this unbelievable speed and unbelievable cost effectiveness. We hope cost effectiveness. you know, full computer using agent, like, just a chatbot, whatever you want. But it'll be, like, given the the limitations of our current tools, does it fit? I think if you have, like, just a rote task that is, like, super valuable and you want done again and again and again with, you know, some variation, these small models will be great. And if you need the kind of, like, reasoning or thinking that that we like to think of as, like, special in human,
Unknown: one other thing that this kinda touches on is the shifts towards an increasing number of open source models and then AI safety and AI alignment.
Sam Altman: I think there is, like, a a capability level of systems where there is genuine danger. Like, not just the risk to offend, but the risk to actually cause, like, irreparable harm to the world that we should not open source. You know, we don't open source nuclear weapons really either. We decided we're gonna, like, have some controls on that as a society. And
Unknown: machines? And then there's safety. And safety encompasses alignment.
Sam Altman: models shouldn't offend or say something that they perceive as toxic. And but if you're talking about AGI safety at all, you're awful and you're, you know, distracted from other issues. And then you have people who say, well, all that matters is AGI safety. And if you're trying to distract with, model not saying stuff that is likely to offend unless you want it to. models sub AGI models doing what a user intends within some very broad bounds, I think, is a reasonable thing for us all to aspire to. And by seeing how people try to misuse them, we we have been able to actually learn pretty quickly. And a lot of the lessons have not been the obvious thing. So the people who kind of, like, say, oh, you should never deploy. You can't guarantee all of these things. Like, you know, Google is a famous example of this. I think that's just wrong. And I also think in the other direction, this idea that, like, you know, we're gonna go work in secret in a lab and build AGI and then drop a super powerful AGI in the world all at once is also wrong. time to adapt and learn and think. One of the things I think OpenAI has done well, although I realize some people disagree with us, is push the over to the window on AGI. You know, talking about AGI five years ago was a very crazy thing. People really didn't take you seriously. Talking about AGI safety, crazier. And so if you're just, like, On the AGI safety front, I do think the the issue of how we align a powerful AGI is really important. But I also think that the people who sat in a room and tried to do this in, like, Microsoft Word instead of the s code got it, And so this is another reason why I think you have to you you have to sort of, like, work on safety capabilities and alignment together. And then they're not orthogonal vectors at all. They're not quite the same vector either. They're pretty closely aligned.
Unknown: in performance for AI models, irrespective
Sam Altman: physical jobs automated, the truck drivers, the factory workers, whatever. Then we have the, like, easy white collar jobs, then the really hard high cognitive labor white collar jobs like mathematician or whatever. And then finally, if ever because maybe creativity is like magic in human and somehow, like, outside the realm of science, Maybe eventually we get the creative jobs. copilot style pretty fast and all these things or we really want a human involved.
Unknown: And so I think there's almost, like, three ages to to mankind or to compute. Right? Up until recently, almost all compute was human. And then now we're in kind of this mixed era. And then we're probably gonna move into an era where most of the compute in terms of the actual impact of the world is gonna be completely driven by machines. We're kind of in this transition phase. Do you think AGI eventually subsumes us? Are we is is biology just a bootloader for a future machine intelligence?
Sam Altman: the future will be like and and what humans do and how we spend our time and, you know, what is made available to us will be really quite different. But that's been the case for a kind of, like, life before and after every technological revolution history. I do think this one is going to be the biggest one ever, but I suspect that's a mistake. some version of emerge, at least for some of us. strategy. But if it's but, like, any of the kind of, like, us versus them competition, I think the whole, like, survival of the fittest version of evolution is over, thankfully, because that that wouldn't go well for us. But I I think it gets to be much more directed here now, and, you know, we get to think and reflect and
Unknown: that gets into sort of this fine line between these two forms of intelligence in some form.
Sam Altman: think it's more likely much more likely you are, but, like, I I don't So I think we have, like, promising ideas but no certainty. this thing is just sitting there asleep. Someone passes a prompt to it. The energy flows through forward once. set of weights. I guess I could more likely believe that it was, like, somehow conscious during the training process. Even that feels One of the someone proposed this idea that the the human experience of consciousness is that we can't predict that they themselves can't predict the outputs either and so have the same subjective experience. in AI infrastructure or, like, how we rack up servers or the whole stack that will, in a few years, be, like, fully automated. And the things right now, teams of people, there will be, like, great frameworks and tech stacks and, like, equivalent to AWS is for you. Just push a button, it'll happen. But we are very much still in the, like, painful early part of the tech rollout. because we did not want the incentive to make unlimited money. This was another thing that at the time people are like, these people are like, you know, incredibly self indulgent idiots that now I think actually, like, looks fairly prescient. And you see around the office people making different decisions than they would as well meaning as as anyone is if they have the incentives for us to just, grow infinitely. So I think we got the financial incentives right. that could go wrong is we become, like, way more successful. We are we we are confident that we have, like, the right financial incentives in place, the right governance incentive mechanisms in place. But somehow, like, we accumulate too much power before we figure out how to, like, dissolve it, and I think we're already seeing this. Like, the, you know, the video games where, like, people are making their own art. They're designing their own persistent characters. You're seeing the very beginning of, like, very short, And I think all of this is, like, you know,
Unknown: If you look at technology waves, they tend to accrue first to technology companies,
Sam Altman: about AI is The more you know about the AI about AI, the field, the worse you are at getting value out of the current models. take one of these models and think about what they they they they bring all of the baggage and the stuff that's wrong with it. If you just go take it to, like, you know, random person in a nontechnological enterprise technology, and it won't reward deep tech technological expertise. It'll reward, like, knowing your problem space really well or being really creative or coming at something from a fresh angle. quite miraculous things that come out of it. Like, the the brain does do amazing stuff and amazingly power efficiently in the outer loop of evolution also because this was, like, quite a wide search space. But but I would bet you can simulate all of the stuff you need in silicon is that's been really healthy is that you can treat scaling laws as a scientific prediction. You can do this for compute. You can do this for data. But you can measure at small scale and you can predict quite accurately I I think that that is going to be a combination of training models to act in specific ways to be aware of that to sort of, like, make it clear when that's happening that like, that's not the world we should be in anyway. And so you you are seeing some some things now where to, like, debias a model or whatever, people are, like, silently packing their prompt at the beginning. There will be, like, other cases where I think even in a world where we're not dependent on that prompt injection still we won't have one very soon because even if we never figure out another research idea, the economic value from the current paradigm and how much further that can be pushed is gonna carry us for many years to come. But it is possible, however unlikely,
Unknown: you know, 90% of the audience or people who are working actively in this field is is founders or engineers or product people or or founders. So